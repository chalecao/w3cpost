<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>计算框架 on</title><link>/bigdata/framework/</link><description>Recent content in 计算框架 on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 06 Oct 2020 08:49:15 +0000</lastBuildDate><atom:link href="/bigdata/framework/index.xml" rel="self" type="application/rss+xml"/><item><title>阿里巴巴的大数据进化之路</title><link>/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/</guid><description>阿里云大数据计算服务概述
阿里巴巴大数据计算服务MaxCompute的前身叫做ODPS，是阿里巴巴内部统一的大数据平台，其实从ODPS到MaxCompute的转变就是整个阿里巴巴大数据平台的演化过程。所以在本次会着重分享阿里巴巴大数据在过去七八年的时间所走过的路以及后续技术发展大方向。
首先做一个基本的定位，大家可以看到下面这张图是一个航空母舰战队。如果把阿里巴巴整体数据体系比作这个战队，那么MaxCompute就是中间的那艘航空母舰，几乎阿里巴巴99%的数据存储以及95%的计算能力都在这个平台上产生。
每天有大概超过一万四千名阿里巴巴内部的开发者会在这个平台上进行开发，也就是每四个阿里员工中就有一个在使用这个平台。每天有超过三百万个作业在这个平台上运行，几乎涵盖了阿里内部所有的数据体系，包括支付宝的芝麻信用分，淘宝商家的每日商铺账单以及“双11”的大流量处理都是在这个平台上进行的。MaxCompute平台有上万台服务器分布在多个不同地域的集群中，具备多集群的容灾能力。在公共云上，MaxCompute每年以250%的用户量和计算量在增长。此外MaxCompute对接到专有云平台上提供了几十套的部署，这里包括了大安全、水利等所有政府业务，也包括城市大脑项目，几乎所有城市大脑项目的底层都是使用这套系统做存储和大数据计算服务，以上就是对于MaxCompute平台的整体定位。
如下图所示的是MaxCompute平台技术全景图。其中最底层是计算平台，最下面是数据流入流出的数据总线，称为DataHub，它现在也为公有云提供服务。数据会通过DataHub流入到MaxCompute大数据计算平台上来，在MaxCompute平台上会与包括人工智能平台在内的所有平台进行互动构成完整数据平台的计算体系。在这之上是开发套件，如Dataworks、MaxCompute Studio，其包括最基本的对数据的管理和认知、对于数据的开发以及对作业的开发和管理。针对于这样的开发和基础平台，向上提供的计算服务包括语音转文本、光学文字识别、机器翻译以及智能大脑这些业务类的产品。在应用层就包括了向淘宝、天猫等比较老牌的淘系产品以及比较新的高德、菜鸟网络以及合一集团等提供所有的技术服务，以上就是MaxCompute平台对内和对外的整体布局。
二、阿里巴巴数据平台进化之路****
接下来分享MaxCompute平台在过去的七八年时间里是如何演化的。在淘系建立之初，在2009年之前使用基本都是IOE的系统，当时阿里更加偏重电商系的系统，属于垂直线的。当时每个BU都有自己的一套从上到下、从业务到平台的产品。2009年的时候，使用的数据库基本都是Oracle，当时阿里巴巴拥有亚洲最大的Oracle集群，所以在那个时候戏称为Oracle之巅，当时的计算规模已经到达百TB的级别了。然后发现随着淘宝运算量的发展，也随着用户量每年以百分之几百甚至上千的增长速率不断增加，Oracle集群无法承接所有业务的发展，所以当时思考的第二个项目就是Greemplum。因为Greenplum与Oracle的兼容度比较好，所以当时想到在Oracle遇到瓶颈的时候使用Greenplum做第二条的基础发展路线。
在阿里巴巴发展之初，各BU都以各自为战的状态发展，其实这也是各个公司在创立之初的普遍状态。大约经过了一年多的时间，阿里巴巴又遇到了Greenplum的天花板，此时的数据量大概比Oracle扩展了10倍。但是此时发现Greenplum在百台机器之后就很难再扩展上去了，但是即便是百台机器的规模对于阿里这样蓬勃发展的企业而言是远远不够的。2009年9月阿里云启动，当时给出的愿景是要做一整套计算平台，其包括三大部分：底层的分布式存储系统——盘古、分布式调度系统——伏羲、分布式大数据存储服务——ODPS，也就是现在的MaxCompute。
大概花了一年的时间，第一个平台开始运行了，当时的ODPS就作为核心的计算引擎在其中发挥作用。到了2012年，这个平台基本上稳定了，这时候开始做到数据统一存储、数据统一的标准化和安全统一管理。当做实现了上述目标之后，在2013年的时候开始大规模的商业化。当时做了一个“5K”项目，也就是单集群突破5千台，同时具备多集群的能力，这种二级扩展能力基本上就标志着阿里内部的数据平台的奠基基本完成。与此同时，因为在做这套产品时候，由于各个BU之间之前是各自为战的，有很多的BU采用了开源的Hadoop体系，所以在当时有两套体系同时存在，一套称为云梯1也就是基于开源体系的，另外一套叫做云梯2就是阿里内部自研体系的。那时候阿里巴巴的Hadoop集群做到了亚洲最大规模，达到了5000台，能够提供PB级别的数据处理能力。在2014年到2015年，因为有两套技术体系并立，所以阿里内部做了一个决定就是将整个技术体系进行统一，所以启动了“登月”计划。而在“登月”的过程中必须要考虑几个需求，第一个就是多集群的能力，第二个就是良好的安全性，第三个就是要有海量的数据处理能力并且需要具备金融级的稳定性。基于上述需求，阿里巴巴当时选择了云梯2系统，也就是今天大家看到的MaxCompute。在2016年到2017年，MaxCompute开始对内支撑所有的业务，并且也开始对外提供服务。多集群扩展到超过万台，并且开始全球化的部署，现在MaxCompute在美东、美西、新加坡、日本、澳大利亚、香港、德国以及俄罗斯都部署了集群。
**登月计划 ****– **一个统一的过程
接下来分享“登月”计划和为什么选择这样的一条技术路线。正如上述所提到的在执行“登月”计划之前，各个BU之间存在着大大小小数十个计算平台，这是业务初期的必然属性。而在技术上最终出现了两套体系，一套是基于开源的体系，另外一套则是基于自研的体系。其实这两套体系在技术和架构上来讲或多或少都有相互的借鉴，但是在技术发展线路上又各不相同，在数据存储格式、调度方法以及对外运算接口上也是各不相同的。当时遇到了以下几个问题：
扩展性差，在两三年前的那个时候，Hadoop体系的NameNode，JobTracker，HiveServer等都还是单点系统，在稳定性层面上存在一定的问题。 性能低，在5K及以上的规模上引擎性能的提升有限，也就是在5K以下基本上可以做到线性扩展，但是超过5千台之后可能就会有问题。 安全性不够高，这一点是非常值得关注的问题。因为整个阿里巴巴在万人级别的规模上是一套标准的多租户体系。所谓多租户就是阿里有很多个BU，每个BU之下有很多个部门，每个部门之下还有组和员工，那么每个BU以及每个部门之间获取的权限应该是不同的，对于如何在数据安全的前提下进行共享的要求非常高，对此基于文件的授权体系不能满足灵活要求。 稳定性比较差，不能支持多个集群和跨集群容灾。 并且当时代码虽然开源但反馈回社区的周期很长，很多集群变成事实上的“自研”系统；这又进一步导致的版本不统一，各个集群无法互联互通！当时出现的问题就是淘宝的数据天猫都无法使用，小微金融的数据其他的BU也无法使用，互相申请权限非常困难，整个体系无法打通。但是大家都知道阿里巴巴不是依靠实体资产，阿里巴巴没有商品和仓储，内部最为核心的就是数据资产。如果在平台性的体系中，数据无法做到互联互通和高效运转，那么就会对公司发展造成很大的危害。
所以阿里巴巴就经历了这样的一个“漫长”和“昂贵”的登月过程。在登月计划中，阿里巴巴集团层面牵头，其中有名有姓的项目大概有24个，当时的登月1号是阿里金融，登月2号是淘宝，这24个项目的“登月”总共历时了一年半的时间，将整个数据统一到了一起。
为了保障“登月计划”的顺利实施，当时MaxCompute平台做了这样的几件事情：
保证能够满足当时Hadoop集群所能够提供的功能，在性能方面至少不会比其他平台差。 在编程接口层面，需要让编程模型等多个方面兼容。 提供完善的上云工具和数据迁移/对比工具，使得可以方便地从Hadoop体系中迁移到MaxCompute上来。 由于不得不在业务进行中升级，和业务方一起做无缝升级方案，“在行驶的飞机上换引擎”。 在实现了统一之后大致有这样三点好处：
打造了集团统一的大数据平台。“登月计划”将阿里巴巴内部所有的机器资源、数据资源统一到了一起。因为数据具备“1+1&amp;gt;2”的特性，所有的数据贯通之后，集群整体的利用效率、员工的工作效率以及数据流转等方面就变得非常高效的。到目前阿里集团内部计算业务运行于MaxCompute集群上，总存储能力达到EB级别，每天运行ODPS_TASK超过几百万。 新平台是安全的，同时可管理、能开放。因为阿里巴巴内部存储的数据和其他的厂商并不一样，阿里巴巴内部很多数据都是交易或者金融数据，所以对于数据的安全性要求非常高，比如同一张表中不同的字段对于不同的用户而言权限应该是不同的，MaxCompute平台提供了这种细粒度的安全性。在登月的过程中，不仅将数据统一到了一起，还实现了数据分级打标、数据脱敏、ODPS授权流程、虚拟域接入在云端查询版等工作。 新平台具备高性能和全面的数据统一。随着把数据统一到一起，阿里巴巴在管理平台上也做了统一化，比如统一的调度中心、同步工具和数据地图等，通过这些将阿里的数据体系进行全面的统一。而且新平台因为经过了很多的业务锤炼和梳理以及人员的整合，整个团队在一个比较大的规模上可以投入到一个平台上做更好的性能优化和功能调优，所以在2014年存储资源优化节约几百PB，通过梳理，各业务团队的作业数/计算量分别有30%-50%的下降，一些历史遗留问题得到全面的清理。 **三、**MaxCompute 2.0 Now and moving forward
接下来分享当阿里巴巴具备了内部的统一的大数据平台之后，未来在基础和业务上应该如何做。
**MaxCompute 2.0 **架构持续升级
在2016年杭州云栖大会上，阿里巴巴发布了MaxCompute 2.0，那个时候推出了全新的SQL引擎并且提供非结构化处理能力，在2017年MaxCompute做了持续的创新和优化。如下图所示，MaxCompute 2.0实现了很多的技术创新，最上面MaxCompute提供了DataWorks开发套件以及MaxCompute Studio；在运算模式上可以支持多种，比如批处理、交互式、内存以及迭代等。再往下在接口层面，今年会推出一个新的查询语言叫做NewSQL，它是阿里巴巴定义的一套新的大数据语言，这套语言兼容传统SQL特性，同时又提供imperative与declarative优势。
在引擎层面，优化器除了可以基于代价还可以基于历史运行信息进行优化。在运行时方面，将IO做成了全异步化。在元数据管理、资源调度和任务调度方面主要做了两件事，一个是做到了Bubble Based Scheduling，也就是当将所有作业数据连接到一起进行Bubble Shuffle的时候，要求上下游是完全拉起的，这对于资源的消耗是非常高的，而Bubble是通过做一个合理的failover 的Group在资源和效率上找到一个平衡点；另外一点是今年着眼于生态和开放性，可以和Hadoop以及Spark等集群做灵活的互动，这是今年在生态层面的发力。在底层，MaxCompute今年除了提供原始的文件格式之外还提供了Index的支持，提供了AliORC，它与社区原生ORC兼容，性能却更高。此外，MaxCompute今年还开始做分级存储，除了内存和缓存以外，还会在SSD、SSD的HDD以及冷备压缩存储上做分层存储，今年其实在内部已经提供了超密存储的机型，未来也会逐步地转移到公有云上来。
**大数据计算 **典型场景分析（从开发到上线）
下图所示的是大数据计算的典型场景分析，这也是阿里内部大多数员工以及云上的经常会接触的事情。通常情况下，一套大数据体系的建立需要分成这样几个过程，需要从数据源到开发阶段再到生产阶段。首先，数据源可以是应用，也可以是应用的服务，也可能来自应用的log日志。一方面可以将应用的信息通过log或者message的方式上传上来，另一方面很多数据信息其实落在DB中，DB的binlog其实可以被采集下来同步到数据平台中。另外一部分数据源就是已有的存量数据。当拥有了这些数据之后可以通过主动拉取、手动上传以及同步中心的方式将数据上传到集群中来。之后就可以进入开发阶段，开发阶段又分为三个部分，第一个部分是数据发现，也就是究竟有什么样的数据可以用，通过IDE的方式做作业的编写或者做数据的编写。在开发阶段提供了通用计算、机器学习、图计算以及流计算等。
在开发完成之后进入到生产阶段，在生产阶段的Workload就分成3部分、一类叫做Workflow，每个月生成一份账单报表就是一个典型的Workflow任务，其特点就是具备周期性，比如每天、每小时或者每个月，这种类型的作业通常情况下作业量比较大，但是周期性却是可以预测的。再往下就是Interactive Analysis，也就是交互式查询，大家可能某一天希望看到数据上的某些统计信息，然后基于这些信息做商业决策，这也许会写到明天的某一份报告里，这种是与开发者做交互的，写一个作业上去发现数据有问题再调整回来，然后来回做这样的交互。第三点是基于时序或者流式的数据处理，这种处理比较典型的就是“双11”数据大屏，它就是滚动的流式计算的典型特点，基本的生产场景就分为以上三大类。
这三大类场景的要求是各不相同的，在数据源层面，对于数据的上传，当数据量比较大的时候，隔离流控是一个技术要点。同时当进入到生产阶段，数据的上传上载需要具备完整性的检查，包括需要进行规则检查的补充。当数据上传变成常规形态的时候，每天都会进行数据上传的时候就有可能因为系统、应用或者数据源的问题导致数据断裂，这种情况发生之后就需要系统具备补数据的能力。而系统也需要对于开发阶段提供必要的支持，因为开发阶段通常是小数据量的，代码和脚本的更新速度比较快，可能经常处于试错的过程，所以需要系统具备准实时的能力、开发效率和Debug效率，这实际上是对人提出的要求。当进入到生产阶段的时候，通常情况下作业相对比较固定，资源和数据量消耗大，对于稳定性的要求就比较高，系统需要提供系统级别的优化能力以及运算能力。以上就是站在阿里巴巴的角度看的从开发到上线的大数据典型场景的分析。
**大数据计算 **典型场景分析（从计算量和延迟的角度）
下图所示的是一个从计算量和延迟的角度看的数据轴，从数据量上看，从100GB到10TB再往上，最高可以到PB级别，在“双11”当天，MaxCompute平台处理了上百PB的数据。在延迟的角度，会达到非常低的延迟状态。可以看到图中的橘色斜线，其含义是当对于数据量以及实时性的要求越高成本就会越高，所以大数据计算的要求就是将这个轴一直向上移动，也就是能够在更短的时间内处理更大量的数据的时候成本越来越低。
在作业分析来看，主要分成三块，其中最典型的数据清洗、数仓建立以及报表类的作业等通常情况下是以小时和天为单位运行的，按照阿里巴巴的数据统计基本上20%这样类型的任务会消耗掉80%的计算资源，这样任务的特定就是基本上以定时任务为主，query是固定的，所以通常情况下运行效率比较高。而实时监控类型的作业就是典型的流计算业务，比如像监控报警、大屏广播等。而交互式作业大致分成两部分，一类是分析类的另外一类是BI类，BI类的意思是说大多数的人可能看不到Query和中间系统，只能看到BI环境比如像阿里云对外推出的QuickBI，大家可以通过配置和拖拽的形式访问系统，这种用户通常是非技术人员，这对于系统的交互性要求比较高，因为其是在UI上进行工作的，同时对于这样的工作一般有比较强的延时要求，一般是在秒级或者几十秒之内完成这样的作业，所以通常情况下数据量比较小，要求数据提前整理好。而交互分析的数据量处于中小级别，有一定的延迟要求。所以这样不同任务对应的不同的技术优化方案，Data Workflow就偏向pipeline型的作业，提升运行性能和效率是关键，对于以开发类和BI类的作业为主的，作业量比较大占大头，但是整体资源占用率比较低，对于这种类型开发效率和时序化是关键。今年在MaxCompute大数据平台的发展上，除了继续提升整体系统效率以外，时序化和开发效率也是今年的重点。
大数据计算 交互式BI类场景分析
在这三种Workload中重点分享一下其中比较基本的BI类的作业。为了实现这样的业务所以对于实时性有更高的要求，比如onlineJob的优化、热表Cache、Index Support等，还要有更优的查询计划、运行时的优化、生态连接、存储格式的进一步提升，需要在数据上支持Index使得在进行运算的时候可以将数据聚集到更小的规模上去，以上这些都是相关的优化。
如下图所示的是OnlineJob的基本设计思想，OnlineJob主要是针对中等规模、低延迟的交互式场景，并且提供了可靠的服务，目前在阿里巴巴内部60%的作业都以这种方式来运行。这个模式主要使用了这样的几点技术：
进程常住（以服务的形式Stand by），进程随着作业完成之后不销毁，一直处于等待状态。 进程可以做到作业间复用。 网络直连，避免落盘。 事件驱动的调度方式。 基于统计和历史信息的自动切换，用户不感知。 下图所展示的是交互式BI类场景下一个优化的例子。传统基于MapReduce的方式拉起多个Mapper做Shuffle的时候数据会落到磁盘里，然后再由下面的Join去读取，中间是割裂的，需要进行一次磁盘的数据交换，而MaxCompute的方式是做网络直连，这样的好处是不用等到第一个Session做完，第二个Session就可以启动，而这样同时也会带来一个坏处就是当failover的时候Group就会变得很大，所以需要做的额外工作就是在内存中及时地进行Checkpoint，这个Checkpoint也可以做到SSD上或者另外一台机器上，这样的方式既提高了效率也降低了延迟并且能够保证failover Group不失效。</description></item><item><title>大数据数据库MPP-MapReduce</title><link>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/</guid><description>这些年大数据概念已经成为IT界的热门，我们经常也会在新闻和报纸中看到。大数据概念中最为关键的技术就是数据库管理系统，伴随着hadoop和MapReduce技术的流行，大数据的数据库中Hive和Spark等新型数据库脱颖而出；而另一个技术流派是基于传统的并行数据库技术演化而来的大规模并行处理（MPP）数据库比如GreenPlum和HAWQ也在最近几年突飞猛进，这两种流派都有对应的比较知名的产品，他们都已得到了市场的认可。
然而对于一个不是搞大数据的门外汉，如何理解大数据概念，如何根据自身的需求来选择对应的数据库管理系统，就成了很多用户很关心的话题。本文将采用比较通俗易懂的介绍，让大家从本质上认识这些大数据库管理系统的技术实现和应用场景。
Map Reduce技术 # 大数据概念，顾名思义就是要解决数据量很大的情况下的数据库处理问题，所以在数据量增长很快的情况下，如何让处理的时间不能增长太多就成了关键。学习过算法的同学们很快能够举出例子，比如对半查找法，在一个已排序好的数据集中如何找到和指定数相等的位置？
最笨的方法是从头到尾查找一遍，这个时间复杂度跟数据量是1:1的关系； 比较好的方法是，讲这个数据分成几段，每段由单独的计算机去算，这样效率能提高n倍（n为分为的段数），时间复杂度跟数据量是1：n的关系； 而对半查找法则是根据已知数据集是排好序的，所以我们只要在数据集的中间位置比较一下，就能知道我们要找的数据是在前半段还是后半段，然后选取有效的半段递归下去，直到有效半段只含一个数值就找到值相等的位置了，这个时间复杂度是1：2^m（m为递归循环的次数）。 假设我们有1024个数，那么用这三个方法的处理时间比值为：1024：1024/n：10。从这个比值上很容易看出，随着数据量的增大，第三种方法的优势越发明显。
如何让普通的数据处理也能像对半查找法一样高效，Google发表的论文提出了Map Reduce编程模型。该模型比较抽象复杂，接下来我用生活中的例子来说明该模型设计的原理。比如在一个大学里的大课堂里，老师想要知道上课的学生总数（实际上就是数据库的count操作），于是他让学生做如下事情：
所有的学生都站起来，同时每个学生记住自己的人数为1 每个同学都各自寻找站着的学生，找到后进行如下操作（假设学生A找到学生B）：
1) A的最新人数=A的旧人数+B的人数
2) B坐下，A站着，继续步骤2.直到最后站着的人只剩下一个，那么该人持有的人数就是这堂课学生的总数。 通过上面这个方式来算总人数，实际上是上面介绍的对半查找法的逆操作：
当只有一个学生站着的时候，改学生拿到的数就是总人数（计算结果）；对应到对半查找法输入待查找的数据集（算法初始状态） 当最后只剩下最后2名学生站着的时候，它正要把两个学生各自持有的总人数相加，它就像是对半查找法的第一次，找到中间的位置，判断相等的数应该是在前半段还是在后半段里。 当最后剩下4名学生站着的时候，他们会分成两组来处理，每组算出各自组的总人数，并每组只留一个同学站着；
而对半查找法里则是在步骤2里选择出来的半段里，再坐对半查找。 …… 当最后剩下2^n名学生站着的时候（假设此时每个学生都站着），各自记住人数为1；对应到对半查找法里就是在n-1步骤处理完后选择的半段数据集个数只为1，找到要查找的数据。 通过上面的对比我们不难发现，无论是从计算方式来看，还是从数据处理/搜索空间来看，这两个算法是互逆的。唯一的不同是对半查找法不需要再对已经判断舍弃的半段不用在运行，比如3）步骤中，对边查找继续搜索前半段或者后半段，但是学生点人数确实两组学生都要进行报数计算。
学生点人数的方法看起来真的非常完美，可是这里面忽略了一个问题，那就是计算资源的问题，上面的每个学生都可以作为一个计算资源。而在现实中计算资源不会像这个例子一样那么多。所以还需要考虑如何讲这些步骤放到有限的计算资源上运行的问题。Map Reduce编程模型就是为了实现将很多复杂运算，以上面学生算总人数的方式去执行的一种编程模型。学生点人数中步骤1抽象为Map（每个学生都map成一个人数1），步骤2中的1）和2）就是Reduce操作，（将学生A和B两个学生站着处理完变成只有一个A站着）。同时考虑到在计算资源有限的情况下如何进行性能优化的问题，该编程模型还会将很多人的map操作，变为一个集合的map操作，讲多次Reduce操作变为一次集合的reduce操作。这样每个map或reduce就可以很方便地在一个计算资源（比如一个计算机）上进行运算了。
大规模并行处理（MPP）技术 # Mpp技术是从原来的并行数据库发展而来的，基于关系数据库的成熟技术，伴随着分布式与并行数据库技术的发展而来的。其中最为关键的技术就是它能够判断出数据之间的相互依赖关系，将可以并行的部分分发到各个节点上并行运行，针对关系数据库中最为常用的等值比较和等值联接（Join）等操作做出特别的优化，将待比较的列按照某种规律进行hash，根据不同的hash值分发到不同的节点上去进行比较处理（它可以被看做是Hash Join的分布式版本）。将查询中能并行的操作和操作产生的中间结果，通过这样的方式分发到不同的节点上去运算，从而最大程度地并行处理，来达到提高性能的方法。
回到前面讲的学生点人数的例子，MPP的思路就是，根据现有的计算资源，将全班学生先按照简单规则分组排队，比如现有n台计算节点，我们就可以把全班学生分成n队，然后每队放到一个计算节点上去计算，计算完讲每队的计算结果再进行相加得到最后全班的总人数。
数据库管理系统中两种技术的优劣分析 # 性能比较 # 可能细心的读者已经发现，MPP的学生点人数的处理方式不就是我在前面介绍的查找指定数例子中的第二个方法嘛；而Map Reduce对应的第三种方法看起来更高效啊。这句话在理想状态下是成立的，不过回到数据库管理系统里来看，采用这两种方式的性能比较就得另说了。
根据前面论述的理论处理时间比值：“假设我们有1024个数，那么用这三个方法的处理时间比值为：1024：1024/n：10”，似乎很难让我们觉得这样的方法针对大数据处理有何优势。但是也正如我前面所说的，如果考虑计算资源的个数是有限的情况下，这个理想的比值又得重新改写了。
试想一下，采用Map Reduce方式处理的学生点人数例子，在有限的计算资源下的模型应该添加一条这样的流程：
1.每两个站着的学生要相加持有的人数时，需要到一个专门的地方去排队认证（每个认证的地方就是一个计算资源）。
在这样的规则下，我们再来讨论Map Reduce和MPP的性能对比。比如我们就只有3个计算资源，那么Map Reduce这种修改后的点人数流程能够发挥的最大性能，跟MPP先分3队，再点人数的性能已经没有区别了。更何况如何协调两个站着的学生去认证处排队，也比MPP现通过简单方法分队再处理更耗时间，这样会导致每次数据库查询的初始化等准备时间增加很多。
而且目前所有的数据库管理系统一般都是部署到特定的节点上的，所以能利用计算资源都是一定的。所以Map Reduce在这种情况下很难发挥优势。更为糟糕的是，因为一般的数据库查询，可能会涉及到很多操作及其他们之间的依赖关系（在关系数据库中，查询一般都会被转化为查询树，用来表示操作节点之间的先后顺序），这样很多情况是无法做到并行处理的。而MPP数据库能够利用传统的关系数据库技术，更容易根据这些依赖关系来规则执行计划，达到最大程度的并行处理。
其他方面的因素 # 由于Map Reduce模型与传统的数据库处理技术相去甚远，很难将传统数据库支持的所有操作都毫无差异地用它重新实现一遍，所以通过它实现的数据库管理系统在支持传统的数据库复杂查询时就显得力不从心了。另外在语言数据库的接口，SQL标准的支持，性能调优配置等方面也因为不能继承关系数据库的成熟技术，而导致学习门槛增高，易用性难于保证。
真实的TPC-DS测试比较 # 根据上面的分析，我们不难看出MPP数据库的优势，下面我们选取同样都是底层文件系统采用Hadoop的HDFS分布式文件系统作为数据存储，上层采用MPP技术的HAWQ与采用Map Reduce的Hive在TPC-DS基准测试中的对比结果吧（数据来自：1)：
性能：简单查询性能相当；HAWQ在处理复杂语句的性能是Hive的三四倍左右。 对复杂查询的支持：Hive只支持基准测试99条语句中的66条，而HAWQ支持全部。 总结 # Map Reduce计算模型在计算资源无限、数据无相关性的情况下很容易具有良好的扩展性，特别适用于计算网格等领域或者简单数据库查询的处理上。但是就目前而言，在实现数据库管理系统领域，它仍然受限与资源分配、数据相关性等因素的制约，很难达到MPP发展的高度。不过技术发展日新月异，也许不出时日，它就能突破这些障碍，或者与MPP技术结合，或许有新技术助力，追平甚至超越MPP数据库也是很有可能的。</description></item><item><title>MPP架构</title><link>/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/</guid><description>面试官：说下你知道的 MPP 架构的计算引擎？ 这个问题不少小伙伴在面试时都遇到过，因为对 MPP 这个概念了解较少，不少人都卡壳了，但是我们常用的大数据计算引擎有很多都是 MPP 架构的，像我们熟悉的 Impala、ClickHouse、Druid、Doris 等都是 MPP 架构。 采用 MPP 架构的很多 OLAP 引擎号称：亿级秒开。 本文分为三部分讲解，第一部分详解 MPP 架构，第二部分剖析 MPP 架构与批处理架构的异同点，第三部分是采用 MPP 架构的 OLAP 引擎介绍。
一、MPP 架构 # MPP 是系统架构角度的一种服务器分类方法。 目前商用的服务器分类大体有三种：
SMP（对称多处理器结构） NUMA（非一致存储访问结构） MPP（大规模并行处理结构） 我们今天的主角是 MPP，因为随着分布式、并行化技术成熟应用，MPP 引擎逐渐表现出强大的高吞吐、低时延计算能力，有很多采用 MPP 架构的引擎都能达到“亿级秒开”。 先了解下这三种结构： 1. SMP # 即对称多处理器结构，就是指服务器的多个 CPU 对称工作，无主次或从属关系。SMP 服务器的主要特征是共享，系统中的所有资源（如 CPU、内存、I/O 等）都是共享的。也正是由于这种特征，导致了 SMP 服务器的主要问题，即扩展能力非常有限。
2. NUMA # 即非一致存储访问结构。这种结构就是为了解决 SMP 扩展能力不足的问题，利用 NUMA 技术，可以把几十个 CPU 组合在一台服务器内。NUMA 的基本特征是拥有多个 CPU 模块，节点之间可以通过互联模块进行连接和信息交互，所以，每个 CPU 可以访问整个系统的内存（这是与 MPP 系统的重要区别）。但是访问的速度是不一样的，因为 CPU 访问本地内存的速度远远高于系统内其他节点的内存速度，这也是非一致存储访问 NUMA 的由来。</description></item><item><title>大数据与流计算概览</title><link>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/</guid><description>大数据与流计算 # 数据时代，从数据中获取业务需要的信息才能创造价值，这类工作就需要计算框架来完成。传统的数据处理流程中，总是先收集数据，然后将数据放到DB中。当人们需要的时候通过DB对数据做query，得到答案或进行相关的处理。这样看起来虽然非常合理，但是结果却非常紧凑，尤其是在一些实时搜索应用环境中的某些具体问题，类似于MapReduce方式的离线处理并不能很好地解决。
基于此，一种新的数据计算结构—流计算方式出现了，它可以很好地对大规模流动数据在不断变化的运动过程中实时地进行分析，捕捉到可能有用的信息，并把结果发送到下一计算节点。 什么是流计算？ 时下，对信息高时效性、可操作性的需求不断增长，这要求软件系统在更少的时间内能处理更多的数据。传统的大数据处理模型将在线事务处理和离线分析从时序上将两者完全分割开来，但显然该架构目前已经越来越落后于人们对于大数据实时处理的需求。 流计算的产生即来源于对于上述数据加工时效性的严苛需求:数据的业务价值随着时间的流失而迅速降低，因此在数据发生后必须尽快对其进行计算和处理。而传统的大数据处理模式对于数据加工均遵循传统日清日毕模式，即以小时甚至以天为计算周期对当前数据进行累计并处理，显然这类处理方式无法满足数据实时计算的需求。在诸如实时大数据分析、风控预警、实时预测、金融交易等诸多业务场景领域，批量(或者说离线)处理对于上述对于数据处理时延要求苛刻的应用领域而言是完全无法胜任其业务需求的。而流计算作为一类针对流数据的实时计算模型，可有效地缩短全链路数据流时延、实时化计算逻辑、平摊计算成本，最终有效满足实时处理大数据的业务需求。 通常而言，流计算具备三大类特点：
实时(realtime)且无界(unbounded)的数据流。流计算面对计算的 是实时且流式的，流数据是按照时间发生顺序地被流计算订阅和消费。且由于数据发生的持续性，数据流将长久且持续地集成进入流计算系统。例如，对于网站的访问点击日志流，只要网站不关闭其点击日志流将一直不停产生并进入流计算系统。因此，对于流系统而言，数据是实时且不终止(无界)的。 持续(continuos)且高效的计算。流计算是一种”事件触发”的计算模式，触发源就是上述的无界流式数据。一旦有新的流数据进入流计算，流计算立刻发起并进行一次计算任务，因此整个流计算是持续进行的计算。 流式(streaming)且实时的数据集成。流数据触发一次流计算的计算结果，可以被直接写入目的数据存储，例如将计算后的报表数据直接写入RDS进行报表展示。因此流数据的计算结果可以类似流式数据一样持续写入目的数据存储。
概念区分 # 明确了流处理概念的同时，一些其他的概念也需要了解：离线计算、批处理计算、实时计算。 离线计算 正如前文所述，离线计算就是在计算开始前已知所有输入数据，输入数据不会产生变化，且在解决一个问题后就要立即得出结果的前提下进行的计算。在大数据中属于数据的计算部分，在该部分中与离线计算对应的则是实时计算。一般来说，离线计算具有数据量巨大且保存时间长；在大量数据上进行复杂的批量运算；数据在计算之前已经完全到位，不会发生变化；能够方便的查询批量计算的结果等特点。 常用的离线计算框架包括有：
Hadoop，适用于离线大批量数据处理，不需要多次迭代。 Spark，适用于离线快速的处理，不能用于处理需要长期保存的数据；适用于多次迭代的计算模型。 MapReduce，Hadoop框架最核心的设计就是HDFS和MapReduce。HDFS为海量的数据提供了存储，MapReduce则为海量的数据提供了计算，它适用于大规模数据集的并行运算。 HDFS，这个Hadoop分布式文件系统能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。
批量计算 # 批量计算是一种批量、高时延、主动发起的计算。目前绝大部分传统数据计算和数据分析服务均是基于批量数据处理模型: 使用ETL系统或者OLTP系统进行构造数据存储，在线的数据服务(包括Ad-Hoc查询、DashBoard等服务)通过构造SQL语言访问上述数据存储并取得分析结果。这套数据处理的方法论伴随着关系型数据库在工业界的演进而被广泛采用。传统的批量数据处理模型传统的批量数据处理通常基于如下处理模型：
使用ETL系统或者OLTP系统构造原始的数据存储，以提供给后续的数据服务进行数据分析和数据计算。 用户/系统主动发起一个计算作业(例如=Hive的SQL作业)并向上述数据系统进行请求。 计算结果返回，计算作业完成后将数据以结果集形式返回用户。
实时计算 # 实时计算一般都是针对海量数据进行的，一般要求为秒级。实时计算主要分为两块:数据的实时入库、数据的实时计算。主要应用的场景有：
数据源是实时的不间断的，要求用户的响应时间也是实时的（比如对于大型网站的流式数据：网站的访问PV/UV、用户访问了什么内容、搜索了什么内容等，实时的数据计算和分析可以动态实时地刷新用户访问数据，展示网站实时流量的变化情况，分析每天各小时的流量和用户分布情况）。 数据量大且无法或没必要预算，但要求对用户的响应时间是实时的。比如说：昨天来自每个省份不同性别的访问量分布，昨天来自每个省份不同性别不同年龄不同职业不同名族的访问量分布。
对于实时计算来说。首先需要解决数据的就是实时接收的问题，在网络带宽、接收性能、安全防控等情况下，如何实现海量并发数据平稳接收具有很大挑战。 离线=批量？实时=流式？ 习惯上我们认为离线和批量等价；实时和流式等价，但其实这种观点并不完全正确。假设一种情况：当我们拥有一个非常强大的硬件系统，可以毫秒级的处理Gb级别的数据，那么批量计算也可以毫秒级得到统计结果（当然这种情况非常极端，目前不可能），那我们还能说它是离线计算吗？ 所以说离线和实时应该指的是：数据处理的延迟；批量和流式指的是：数据处理的方式。两者并没有必然的关系。事实上Spark streaming就是采用小批量（batch）的方式来实现实时计算。
可以参考链接：https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101。作者是Google实时计算的负责人，里面阐述了他对批量和实时的理解，并且作者认为批量计算只是流式计算的子集，一个设计良好的流式系统完全可以替代批量系统。
流计算 # 目前流式计算是业界研究的一个热点。早期的代表系统有IBM的System S，它是一个完整的计算架构，通过“stream computing”技术，可以对stream形式的数据进行real-time的分析。“最初的系统拥有大约800个微处理器，但IBM称，根据需求，这个数字也有可能上万。研究者讲到，其中最关键的部分是System S软件，它可以将任务分开，比如分为图像识别和文本识别，然后将处理后的结果碎片组成完整的答案。IBM实验室高性能流运算项目的负责人Nagui Halim谈到：System S是一个全新的运算模式，它的灵活性和速度颇具优势。而与传统系统相比，它的方式更加智能化，可以适当转变，以适用其需要解决的问题。 近年来Twitter、LinkedIn等公司相继开源了流式计算系统Storm、Kafka等，加上Yahoo!之前开源的S4，流式计算研究在互联网领域持续升温。下面来盘点一些业界常见的流计算产品。 Storm Storm是一个分布式的、容错的实时计算系统，做作为最早的一个实时计算框架，早期应用于各大互联网公司。在Storm出现之前，进行实时处理是非常痛苦的事情，我们主要的时间都花在关注往哪里发消息，从哪里接收消息，消息如何序列化，真正的业务逻辑只占了源代码的一小部分。一个应用程序的逻辑运行在很多worker上，但这些worker需要各自单独部署，还需要部署消息队列。最大问题是系统很脆弱，而且不是容错的：需要自己保证消息队列和worker进程工作正常。Storm具有编程简单、高性能，低延迟、分布式、可扩展、容错、消息不丢失等特点。
但是，Storm没有提供exactly once的功能，并且开启ack功能后又会严重影响吞吐，所以会给大家一种印象：流式系统只适合吞吐相对较小的、低延迟不精确的计算；而精确的计算则需要由批处理系统来完成，所以出现了Lambda架构，同时运行两个系统：一个流式，一个批量，用批量计算的精确性来弥补流式计算的不足，但是这个架构存在一个问题就是需要同时维护两套系统，代价比较大。 Spark streaming
Spark streaming采用小批量的方式，提高了吞吐性能。Spark streaming批量读取数据源中的数据，然后把每个batch转化成内部的RDD。Spark streaming以batch为单位进行计算），而不是以record为单位，大大减少了ack所需的开销，显著满足了高吞吐、低延迟的要求，同时也提供exactly once功能。但也因为处理数据的粒度变大，导致Spark streaming的数据延时不如Storm，Spark streaming是秒级返回结果（与设置的batch间隔有关），Storm则是毫秒级。 Flink Flink是一个针对流数据和批数据的分布式处理引擎，主要由Java代码实现。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已。Flink 可以支持本地的快速迭代，以及一些环形的迭代任务，并且可以定制化内存管理。在这点，如果要对比 Flink 和 Spark 的话，Flink 并没有将内存完全交给应用层。这也是为什么 Spark 相对于 Flink，更容易出现 OOM 的原因（out of memory）。就框架本身与应用场景来说，Flink 更相似与 Storm。</description></item><item><title>阿里巴巴流计算引擎Blink</title><link>/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/</guid><description>阿里巴巴是世界上最大的电子商务零售商。 我们在 2015 年的年销售额总计 3940 亿美元，超过 eBay 和亚马逊之和。阿里巴巴搜索（个性化搜索和推荐平台）是客户的关键入口，并承载了大部分在线收入，因此搜索基础架构团队需要不断探索新技术来改进产品。
在电子商务网站应用场景中，什么能造就一个强大的搜索引擎？答案就是尽可能的为每个用户提供实时相关和准确的结果。同样一个不容忽视的问题就是阿里巴巴的规模，当前很难找到能够适合我们的技术。
Apache Flink? 就是一种这样的技术，阿里巴巴正在使用基于 Flink 的系统 Blink 来为搜索基础架构的关键模块提供支持，最终为用户提供相关和准确的搜索结果。在这篇文章中，我将介绍 Flink 在阿里巴巴搜索中的应用，并介绍我们选择在搜索基础架构团队中使用 Flink 的原因。
我还将讨论如何改进 Flink 以满足我们对 Blink 的独特需求，以及我们如何与 data Artisans 和 Flink 社区合作，将这些更改贡献给 Flink 社区。一旦成功地将我们的修改合并到开源项目中，我们会将现有系统从 Blink 转移到 Apache Flink。
Flink 在阿里巴巴搜索中的应用 # 文档创建 # 为用户提供世界级搜索引擎的第一步是创建可供搜索的文档。在阿里巴巴的应用场景中，文档是由数百万个商品列表和相关的商品数据组成。
因为商品数据存储在许多不同的地方，所以搜索文档创建也是一个很大的挑战，搜索基础架构团队将商品相关的所有信息汇总在一起并创建完整的搜索文档。一般来说，整个过程分为 3 个阶段：
将不同来源（例如 MySQL，分布式文件系统）的所有商品数据同步到一个 HBase 集群中。 使用业务逻辑将来自不同表的数据连接在一起，以创建最终的可搜索文档。这是一个 HBase 表，我们称之为’Result’表。 将此 HBase 表导出为文件作为更新集合。 这 3 个阶段实际上是在经典的“lambda 架构”中的 2 个不同的 pipeline 上运行：全量构建 pipeline 和增量构建 pipeline。
在全量构建 pipeline 中，我们需要处理所有数据源，这通常是一个批处理作业。 在增量构建 pipeline 中，我们需要处理在批处理作业完成后发生的更新。例如，卖家可能修改商品价格或商品描述以及库存量的变化。这些信息需要尽可能快的反馈在搜索结果中。增量构建 pipeline 通常是一个流式作业。 搜索算法实时 A/B 测试 # 我们的工程师会定期测试不同的搜索算法，并且需要尽可能快地评估出效果。现在这种评估每天运行一次，因为想实时分析效果，所以我们使用 Blink 构建了一个实时 A/B 测试框架。</description></item><item><title>大数据之流计算之Flink介绍</title><link>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/</guid><description>大数据计算引擎的发展 # 这几年大数据的飞速发展，出现了很多热门的开源社区，其中著名的有 Hadoop、Storm，以及后来的 Spark，他们都有着各自专注的应用场景。Spark 掀开了内存计算的先河，也以内存为赌注，赢得了内存计算的飞速发展。Spark 的火热或多或少的掩盖了其他分布式计算的系统身影。就像 Flink，也就在这个时候默默的发展着。
在国外一些社区，有很多人将大数据的计算引擎分成了 4 代，当然，也有很多人不会认同。我们先姑且这么认为和讨论。
首先第一代的计算引擎，无疑就是 Hadoop 承载的 MapReduce。这里大家应该都不会对 MapReduce 陌生，它将计算分为两个阶段，分别为 Map 和 Reduce。对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。
由于这样的弊端，催生了支持 DAG 框架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来说，大多还是批处理的任务。
接下来就是以 Spark 为代表的第三代的计算引擎。第三代计算引擎的特点主要是 Job 内部的 DAG 支持（不跨越 Job），以及强调的实时计算。在这里，很多人也会认为第三代计算引擎也能够很好的运行批处理的 Job。
随着第三代计算引擎的出现，促进了上层应用快速发展，例如各种迭代计算的性能以及对流计算和 SQL 等的支持。Flink 的诞生就被归在了第四代。这应该主要表现在 Flink 对流计算的支持，以及更一步的实时性上面。当然 Flink 也可以支持 Batch 的任务，以及 DAG 的运算。
或许会有人不同意以上的分类，我觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个，就像 Spark 没有完全取代 Hadoop，当然 Flink 也不可能取代 Spark。本文将致力描述 Flink 的原理以及应用。
Flink 简介 # 很多人可能都是在 2015 年才听到 Flink 这个词，其实早在 2008 年，Flink 的前身已经是柏林理工大学一个研究性项目， 在 2014 被 Apache 孵化器所接受，然后迅速地成为了 ASF（Apache Software Foundation）的顶级项目之一。Flink 的最新版本目前已经更新到了 0.</description></item><item><title>数据仓库数据湖</title><link>/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/</guid><description>前言
随着大数据技术的不断更新和迭代，数据管理工具得到了飞速的发展，相关概念如雨后春笋一般应运而生，如从最初决策支持系统(DSS)到商业智能(BI)、数据仓库、数据湖、数据中台等，这些概念特别容易混淆，本文对这些名词术语及内涵进行系统的解析，便于读者对数据平台相关的概念有全面的认识。
一数据仓库 # 数据仓库平台逐步从BI报表为主到分析为主、到预测为主、再到操作智能为目标。
图1.数据仓库发展阶段划分
**商务智能（BI，Business Intelligence）**是一种以提供决策分析性的运营数据为目的而建立的信息系统。是属于在线分析处理：On Line Analytical Processing(OLAP)，将预先计算完成的汇总数据，储存于魔方数据库(Cube) 之中，针对复杂的分析查询，提供快速的响应。在前10年，BI报表项目比较多，是数据仓库项目的前期预热项目（主要分析为主的阶段，是数据仓库的初级阶段），制作一些可视化报表展现给管理者。
它利用信息科技，将分散于企业内、外部各种数据加以整合并转换成知识，并依据某些特定的主题需求，进行决策分析和运算； 用户则通过报表、图表、多维度分析的方式，寻找解决业务问题所需要的方案； 这些结果将呈报给决策者，以支持策略性的决策和定义组织绩效，或者融入智能知识库自动向客户推送。
1.1 数据仓库基本定义 # **数据仓库(Data Warehouse)**是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化的（Time Variant）数据集合，用于支持管理决策和信息的全局共享。其主要功能是将组织透过资讯系统之联机事务处理(OLTP)经年累月所累积的大量资料，透过数据仓库理论所特有的资料储存架构，作一有系统的分析整理，以利各种分析方法如联机分析处理(OLAP)、数据挖掘(Data Mining)之进行，并进而支持如决策支持系统(DSS)、主管资讯系统(EIS)之创建，帮助决策者能快速有效的自大量资料中，分析出有价值的资讯，以利决策拟定及快速回应外在环境变动，帮助建构商业智能(BI)。[1]：引自全球数据仓库之父 W.H.Inmon。
所谓主题：是指用户使用数据仓库进行决策时所关心的重点方面，如：收入、客户、销售渠道等；所谓面向主题，是指数据仓库内的信息是按主题进行组织的，而不是像业务支撑系统那样是按照业务功能进行组织的。 所谓集成：是指数据仓库中的信息不是从各个业务系统中简单抽取出来的，而是经过一系列加工、整理和汇总的过程，因此数据仓库中的信息是关于整个企业的一致的全局信息。 所谓随时间变化：是指数据仓库内的信息并不只是反映企业当前的状态，而是记录了从过去某一时点到当前各个阶段的信息。通过这些信息，可以对企业的发展历程和未来趋势做出定量分析和预测。
图2.数据仓库逻辑架构
1.2 数据仓库系统作用和定位 # 数据仓库系统的作用能实现跨业务条线、跨系统的数据整合，为管理分析和业务决策提供统一的数据支持。数据仓库能够从根本上帮助你把公司的运营数据转化成为高价值的可以获取的信息（或知识），并且在恰当的时候通过恰当的方式把恰当的信息传递给恰当的人。
图3.数据仓库的作用
是面向企业中、高级管理进行业务分析和绩效考核的数据整合、分析和展现的工具； 是主要用于历史性、综合性和深层次数据分析； 数据来源是ERP（例:SAP）系统或其他业务系统； 能够提供灵活、直观、简洁和易于操作的多维查询分析; 不是日常交易操作系统，不能直接产生交易数据；
数据仓库针对实时数据处理，非结构化数据处理能力较弱，以及在业务在预警预测方面应用相对有限。
1.3数据仓库能提供什么 # 图4.数据仓库提供价值
1.4 数据仓库系统构成 # 数据仓库系统除了包含分析产品本身之外，还包含数据集成、数据存储、数据计算、门户展现、平台管理等其它一系列的产品。
图5.数据仓库产品构成
图6.数据仓库产品构成
二数据湖 # **数据湖(Data Lake)**是Pentaho的CTO James Dixon提出来的(Pentaho作为一家BI公司在理念上是挺先进的)，是一种数据存储理念——即在系统或存储库中以自然格式存储数据的方法。
2.1 维基百科对数据湖的定义 # **数据湖（**Data Lake）**是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。**数据湖是以其自然格式存储的数据的系统或存储库，通常是对象blob或文件。数据湖通常是企业所有数据的单一存储，包括源系统数据的原始副本，以及用于报告、可视化、分析和机器学习等任务的转换数据。数据湖可以包括来自关系数据库（行和列）的结构化数据，半结构化数据（CSV，日志，XML，JSON），非结构化数据（电子邮件，文档，PDF）和二进制数据（图像，音频，视频）。来源：维基百科。
目前，Hadoop是最常用的部署数据湖的技术，所以很多人会觉得数据湖就是Hadoop集群。数据湖是一个概念，而Hadoop是用于实现这个概念的技术。
图7.数据湖的处理架构
图8.数据湖示意图
2.2 数据湖能给企业带来多种能力 # 数据湖能给企业带来多种能力，例如，能实现数据的集中式管理，在此之上，企业能挖掘出很多之前所不具备的能力。另外，数据湖结合先进的数据科学与机器学习技术，能帮助企业构建更多优化后的运营模型，也能为企业提供其他能力，如预测分析、推荐模型等，这些模型能刺激企业能力的后续增长。数据湖能从以下方面帮助到企业：
实现数据治理（data governance）。 通过应用机器学习与人工智能技术实现商业智能。 预测分析，如领域特定的推荐引擎。 信息追踪与一致性保障。 根据对历史的分析生成新的数据维度。 有一个集中式的能存储所有企业数据的数据中心，有利于实现一个针对数据传输优化的数据服务。 帮助组织或企业做出更多灵活的关于企业增长的决策。 2.</description></item><item><title>湖仓一体lake house</title><link>/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/</guid><description>前言 # 本文翻译自大数据技术公司 Databricks 针对数据湖 Delta Lake 系列技术文章。众所周知，Databricks 主导着开源大数据社区 Apache Spark、Delta Lake 以及 ML Flow 等众多热门技术，而 Delta Lake 作为数据湖核心存储引擎方案给企业带来诸多的优势。 此外，阿里云和 Apache Spark 及 Delta Lake 的原厂 Databricks 引擎团队合作，推出了基于阿里云的企业版全托管 Spark 产品——Databricks 数据洞察，该产品原生集成企业版 Delta Engine 引擎，无需额外配置，提供高性能计算能力。有兴趣的同学可以搜索 Databricks 数据洞察或阿里云 Databricks 进入官网，或者直接访问 https://www.aliyun.com/product/bigdata/spark 了解详情。
译者：韩宗泽（棕泽），阿里云计算平台事业部技术专家，负责开源大数据生态企业团队的研发工作。
Delta Lake技术系列 - 湖仓一体（Lakehouse） # ——整合数据湖和数据仓库的最佳优势
目录 # Chapter-01 什么是湖仓一体？
Chapter-02 深入探讨 Lakehouse 和 Delta Lake 的内部工作原理
Chapter-03 探究 Delta Engine
本文介绍内容 # Delta Lake 系列电子书由 Databricks 出版，阿里云计算平台事业部大数据生态企业团队翻译，旨在帮助领导者和实践者了解 Delta Lake 的全部功能以及它所处的场景。在本文中，Delta Lake 系列-湖仓一体（ Lakehouse ），重点介绍湖仓一体。</description></item><item><title>TensorFlow.js简单概念和用法</title><link>/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/</guid><description>简介 # TensorFlow.js 是一个利用 WebGL 来进行加速的机器学习类库，它基于浏览器，提供了高层次的 JavaScript API 接口。它将高性能机器学习构建块带到您的指尖，使您能够在浏览器中训练神经网络或在推理模式下运行预先训练的模型。有关安装/配置 TensorFlow.js 的指南，请参阅 入门指南。
Tensors（张量） # TensorFlow.js 中数据的核心表现形式是 张量 ：一组数值形成的一维或多维的数组。每个Tensor的实例都有 shape 属性来用于定义数组的维度形状——即数组有几个维度，每个维度有几个值。 其中 tensor 最主要的构造函数就是 tf.tensor：
// 2x3 张量 const shape = [2, 3]; // 2 行, 3 列 const a = tf.tensor([1.0, 2.0, 3.0, 10.0, 20.0, 30.0], shape); a.print(); // 输出张量的值 // 输出: [[1 , 2 , 3 ], // [10, 20, 30]] // 张量的维度形状是可以被推测的: const b = tf.tensor([[1.0, 2.0, 3.0], [10.</description></item></channel></rss>