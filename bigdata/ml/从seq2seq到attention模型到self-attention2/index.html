<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.f9552544b4ca62af741c0d24d283b4ddcfa2a026a871cff4112743eeb6c4950a530c923242d9d4d42e93635dd91ebd78601a145b8b205bb363d7065c1fa59ffe.css integrity="sha512-+VUlRLTKYq90HA0k0oO03c+ioCaocc/0ESdD7rbElQpTDJIyQtnU1C6TY13ZHr14YBoUW4sgW7Nj1wZcH6Wf/g==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>从Seq2seq到Attention模型到Self Attention（2） - 万维刀客「w3cdoc」</title><meta name=description content="系列一介绍了Seq2seq和 Attention model。这篇文章将重点摆在Google於2017年发表论文“Attention is all you need”中提出的 “”The transformer模型。”The transformer”模型中主要的概念有2项：1. Self attention 2. Multi-head，此外，模型更解决了传统attention model中无法平行化的缺点，并带来优异的成效。
前言 # 系列一中，我们学到attention model是如何运作的，缺点就是不能平行化，且忽略了输入句中文字间和目标句中文字间的关係。
为了解决此问题，2017年，Self attention诞生了。
Self Attention # Self attention是Google在 “Attention is all you need”论文中提出的”The transformer”模型中主要的概念之一，我们可以把”The transformer”想成是个黑盒子，将输入句输入这个黑盒子，就会產生目标句。
最特别的地方是，”The transformer”完全捨弃了RNN、CNN的架构。
The transformer # “The transformer”和Seq2seq模型皆包含两部分：Encoder和Decoder。比较特别的是，”The transformer”中的Encoder是由6个Encoder堆积而成(paper当中N=6)，Deocder亦然，这和过去的attention model只使用一个encoder/decoder是不同的。
Query, Key, Value # 进入”The transformer”前，我们重新复习attention model，attention model是从输入句&amp;lt;X1,X2,X3…Xm&amp;gt;產生h1,h2,h….hm的hidden state，透过attention score α 乘上input 的序列加权求和得到Context vector c_{i}，有了context vector和hidden state vector，便可计算目标句&amp;lt;y1…yn&amp;gt;。换言之，就是将输入句作为input而目标句作为output。
如果用另一种说法重新詮释：
输入句中的每个文字是由一系列成对的 &amp;lt;地址Key, 元素Value&amp;gt;所构成，而目标中的每个文字是Query，那麼就可以用Key, Value, Query去重新解释如何计算context vector，透过计算Query和各个Key的相似性，得到每个Key对应Value的权重係数，权重係数代表讯息的重要性，亦即attention score；Value则是对应的讯息，再对Value进行加权求和，得到最终的Attention/context vector。
笔者认为这概念非常创新，特别是从attention model到”The transformer”间，鲜少有论文解释这种想法是如何连结的，间接导致”attention is all you need”这篇论文难以入门，有兴趣可以参考key、value的起源论文 Key-Value Memory Networks for Directly Reading Documents。"><link rel=canonical href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="从Seq2seq到Attention模型到Self Attention（2）"><meta property="og:description" content="系列一介绍了Seq2seq和 Attention model。这篇文章将重点摆在Google於2017年发表论文“Attention is all you need”中提出的 “”The transformer模型。”The transformer”模型中主要的概念有2项：1. Self attention 2. Multi-head，此外，模型更解决了传统attention model中无法平行化的缺点，并带来优异的成效。
前言 # 系列一中，我们学到attention model是如何运作的，缺点就是不能平行化，且忽略了输入句中文字间和目标句中文字间的关係。
为了解决此问题，2017年，Self attention诞生了。
Self Attention # Self attention是Google在 “Attention is all you need”论文中提出的”The transformer”模型中主要的概念之一，我们可以把”The transformer”想成是个黑盒子，将输入句输入这个黑盒子，就会產生目标句。
最特别的地方是，”The transformer”完全捨弃了RNN、CNN的架构。
The transformer # “The transformer”和Seq2seq模型皆包含两部分：Encoder和Decoder。比较特别的是，”The transformer”中的Encoder是由6个Encoder堆积而成(paper当中N=6)，Deocder亦然，这和过去的attention model只使用一个encoder/decoder是不同的。
Query, Key, Value # 进入”The transformer”前，我们重新复习attention model，attention model是从输入句<X1,X2,X3…Xm>產生h1,h2,h….hm的hidden state，透过attention score α 乘上input 的序列加权求和得到Context vector c_{i}，有了context vector和hidden state vector，便可计算目标句<y1…yn>。换言之，就是将输入句作为input而目标句作为output。
如果用另一种说法重新詮释：
输入句中的每个文字是由一系列成对的 <地址Key, 元素Value>所构成，而目标中的每个文字是Query，那麼就可以用Key, Value, Query去重新解释如何计算context vector，透过计算Query和各个Key的相似性，得到每个Key对应Value的权重係数，权重係数代表讯息的重要性，亦即attention score；Value则是对应的讯息，再对Value进行加权求和，得到最终的Attention/context vector。
笔者认为这概念非常创新，特别是从attention model到”The transformer”间，鲜少有论文解释这种想法是如何连结的，间接导致”attention is all you need”这篇论文难以入门，有兴趣可以参考key、value的起源论文 Key-Value Memory Networks for Directly Reading Documents。"><meta property="og:url" content="/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/"><meta property="og:site_name" content="万维刀客「w3cdoc」"><meta property="og:image" content="/doks.png"><meta property="og:image:alt" content="万维刀客「w3cdoc」"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@w3cdoc"><meta name=twitter:creator content="@chalecao"><meta name=twitter:title content="从Seq2seq到Attention模型到Self Attention（2）"><meta name=twitter:description content><meta name=twitter:image content="/doks.png"><meta name=twitter:image:alt content="从Seq2seq到Attention模型到Self Attention（2）"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"/#/schema/organization/1","name":"w3cdoc","url":"/","sameAs":["https://twitter.com/w3cdoc"],"logo":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/w3cdoc.png","width":512,"height":512,"caption":"w3cdoc"},"image":{"@id":"/#/schema/image/1"}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"万维刀客「w3cdoc」","description":"互联网同学互相帮助、学习成长的家园","publisher":{"@id":"/#/schema/organization/1"}},{"@type":"WebPage","@id":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/","url":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/","name":"从Seq2seq到Attention模型到Self Attention（2）","description":"","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/organization/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/"]}]},{"@type":"BreadcrumbList","@id":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/bigdataml%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/#/schema/image/2","url":"/doks.png","contentUrl":"/doks.png","caption":"从Seq2seq到Attention模型到Self Attention（2）"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js",{scope:"/"}).then(function(e){console.log("Registration succeeded. Scope is "+e.scope)}).catch(function(e){console.log("Registration failed with "+e)})</script><script src=/js/vendor/autolog.js async></script></head><body class="bigdata single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=万维刀客「w3cdoc」>万维刀客「w3cdoc」</a>
<button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>万维刀客「w3cdoc」</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>初级入门
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/js/basic/javascript%E4%BB%8B%E7%BB%8D/>JS入门</a></li><li><a class=dropdown-item href=/git/basic/introduction/>Git入门</a></li></ul></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>进阶学习
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/fed-regain/html/html%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2/>前端增长</a></li><li><a class=dropdown-item href=/webgl/base/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/>3D图形</a></li><li><a class=dropdown-item href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>数据分析</a></li></ul></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/blog/>博客</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/course/>网课教程</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=false>
计算环境</button><div class=collapse id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=true>
机器学习</button><div class="collapse show" id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded active" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=false>
计算环境</button><div class=collapse id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=true>
机器学习</button><div class="collapse show" id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded active" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"></nav><main class="docs-content col-lg-11 col-xl-9 mx-xl-auto"><h1>从Seq2seq到Attention模型到Self Attention（2）</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"></nav><p>系列一介绍了Seq2seq和 Attention model。这篇文章将重点摆在Google於2017年发表论文“Attention is all you need”中提出的 “”The transformer模型。”The transformer”模型中主要的概念有2项：1. Self attention 2. Multi-head，此外，模型更解决了传统attention model中无法平行化的缺点，并带来优异的成效。</p><h1 id=前言>前言 <a href=#%e5%89%8d%e8%a8%80 class=anchor aria-hidden=true>#</a></h1><p>系列一中，<a href=https://www.w3cdoc.com>我们</a>学到attention model是如何运作的，缺点就是不能平行化，且忽略了输入句中文字间和目标句中文字间的关係。</p><p>为了解决此问题，2017年，Self attention诞生了。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-24-48.png></p><h1 id=self-attention><strong>Self Attention</strong> <a href=#self-attention class=anchor aria-hidden=true>#</a></h1><p>Self attention是Google在 “Attention is all you need”论文中提出的”The transformer”模型中主要的概念之一，<a href=https://www.w3cdoc.com>我们</a>可以把”The transformer”想成是个黑盒子，将输入句输入这个黑盒子，就会產生目标句。</p><p>最特别的地方是，”The transformer”完全捨弃了RNN、CNN的架构。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-24-56.png></p><h1 id=the-transformer><strong>The transformer</strong> <a href=#the-transformer class=anchor aria-hidden=true>#</a></h1><p>“The transformer”和Seq2seq模型皆包含两部分：Encoder和Decoder。比较特别的是，”The transformer”中的Encoder是由6个Encoder堆积而成(paper当中N=6)，Deocder亦然，这和过去的attention model只使用一个encoder/decoder是不同的。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-07.png></p><h1 id=query-key-value><strong>Query, Key, Value</strong> <a href=#query-key-value class=anchor aria-hidden=true>#</a></h1><p>进入”The transformer”前，<a href=https://www.w3cdoc.com>我们</a>重新复习attention model，attention model是从输入句&lt;X1,X2,X3…Xm>產生h1,h2,h….hm的hidden state，透过attention score α 乘上input 的序列加权求和得到Context vector c_{i}，有了context vector和hidden state vector，便可计算目标句&lt;y1…yn>。换言之，就是将输入句作为input而目标句作为output。</p><p>如果用另一种说法重新詮释：</p><p>输入句中的每个文字是由一系列成对的 &lt;地址Key, 元素Value>所构成，而目标中的每个文字是Query，那麼就可以用Key, Value, Query去重新解释如何计算context vector，透过计算Query和各个Key的相似性，得到每个Key对应Value的权重係数，权重係数代表讯息的重要性，亦即attention score；Value则是对应的讯息，再对Value进行加权求和，得到最终的Attention/context vector。</p><p>笔者认为这概念非常创新，特别是从attention model到”The transformer”间，鲜少有论文解释这种想法是如何连结的，间接导致”attention is all you need”这篇论文难以入门，有兴趣可以参考key、value的起源论文 Key-Value Memory Networks for Directly Reading Documents。</p><p>在NLP的领域中，Key, Value通常就是指向同一个文字隐向量(word embedding vector)。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-20.png></p><p>有了Key, Value, Query的概念，<a href=https://www.w3cdoc.com>我们</a>可以将attention model中的Decoder公式重新改写。1. score e_{ij}= Similarity(Query, Key_{i})，上一篇有提到3种计算权重的方式，而<a href=https://www.w3cdoc.com>我们</a>选择用内积。2. 有了Similarity(Query, Key_{i})，便可以透过softmax算出Softmax(sim_{i})=a_{i}，接著就可以透过attention score a_{i}乘上Value_{i}的序列和加总所得 = Attention(Query, Source)，也就是context/attention vector。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-28.png></p><p>在了解Key, Value, Query的概念后，<a href=https://www.w3cdoc.com>我们</a>可以进入”the transformer”的世界了。</p><h1 id=scaled-dot-product-attention><strong>Scaled Dot-Product Attention</strong> <a href=#scaled-dot-product-attention class=anchor aria-hidden=true>#</a></h1><p>如果仔细观察，其实“The transformer”计算 attention score的方法和attention model如出一辙，但”The transformer”还要除上分母=根号d_{k}，目的是避免内积过大时，softmax產出的结果非0即1。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-37.png></p><h1 id=three-kinds-of-attention><strong>Three kinds of Attention</strong> <a href=#three-kinds-of-attention class=anchor aria-hidden=true>#</a></h1><p>“The transformer”在计算attention的方式有三种，1. encoder self attention，存在於encoder间. 2. decoder self attention，存在於decoder间，3. encoder-decoder attention, 这种attention算法和过去的attention model相似。</p><p>接下来<a href=https://www.w3cdoc.com>我们</a>透过encoder和decoder两部份，来分别介绍encoder/decoder self attention。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-47.png></p><h1 id=encoder><strong>Encoder</strong> <a href=#encoder class=anchor aria-hidden=true>#</a></h1><p><a href=https://www.w3cdoc.com>我们</a>将”The transformer”模型分为左右两部分，左边是Encoder，如前述，”Attention is all you need”当中N=6，代表Encoder部分是由6个encoder堆积而成的。其中在计算encoder self attention时，更透过multi-head的方式去学习不同空间的特徵，在后续内容会探讨multi-head的部分。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-25-56.png></p><h1 id=如何计算encoder-self-attention><strong>如何计算encoder self attention?</strong> <a href=#%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97encoder-self-attention class=anchor aria-hidden=true>#</a></h1><p><a href=https://www.w3cdoc.com>我们</a>先用微观的角度来观察Attention(q_{t}, K, V)，也就是输入句中的某个文字，再将所有输入句中的文字一次用矩阵Attention(Q,K,V)来解决。</p><p>第一步是创造三个encoder的输入向量Q,K,V，举例来说，“Are you very big?”中的每一个字的隐向量都有各自的Q,K,V，接著<a href=https://www.w3cdoc.com>我们</a>会乘上一个初始化矩阵，论文中输出维度d_{model}=512。</p><p>第二步是透过内积来计算score &lt;q_{t}, k_{s}>，类似attention model 中的score e_{ij}。假设<a href=https://www.w3cdoc.com>我们</a>在计算第一个字”Are”的self-attention，<a href=https://www.w3cdoc.com>我们</a>可能会将输入句中的每个文字”Are”, ”you”, ‘very’, ‘big’分别和”Are”去做比较，这个分数决定了<a href=https://www.w3cdoc.com>我们</a>在encode某个特定位置的文字时，应该给予多少注意力(attention)。所以当<a href=https://www.w3cdoc.com>我们</a>在计算#位置1的self-attention，第一个分数是q1、k1的内积 (“Are vs Are”)，第二个分数则是q1、k2 (“Are vs you”)，以此类推。</p><p>第三步是将算出的分数除以根号d_{k}，论文当中假定d_{k}=64，接著传递至exponential函数中并乘上1/Z，其实这结果就是attention/softmax score，<a href=https://www.w3cdoc.com>我们</a>可以把1/Z看成是softmax时，所除上的exponential总和，最终的总分数就是attention score，代表<a href=https://www.w3cdoc.com>我们</a>应该放多少注意力在这个位置上，也就是attention model的概念，有趣的是，怎麼算一定都会发现自己位置上的分数永远最高，但有时候可以发现和其他位置的文字是有关联的。</p><p>最后一步就是把attention score再乘上value，然后加总得到attention vector(z_{I})，这就是#位置1的attention vector z1，概念都和以往的attention model类似。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-07.png></p><p>以上就是self-attention的计算，算出来的向量<a href=https://www.w3cdoc.com>我们</a>可以往前传递至feed-forward neural network，实际的运作上，是直接将每个文字同时处理，因此会变成一个矩阵，而非单一词向量，计算后的结果attention vector也会变成attention matrix Z。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-16.png></p><h1 id=multi-head-attention><strong>Multi-head attention</strong> <a href=#multi-head-attention class=anchor aria-hidden=true>#</a></h1><p>有趣的是，如果<a href=https://www.w3cdoc.com>我们</a>只计算一个attention，很难捕捉输入句中所有空间的讯息，为了优化模型，论文当中提出了一个新颖的做法：Multi-head attention，概念是不要只用d_{model}维度的key, value, query们做单一个attention，而是把key, value, query们线性投射到不同空间h次，分别变成维度d_{q}, d_{k} and d_{v}，再各自做attention，其中，d_{k}=d_{v}=d_{model}/h=64，概念就是投射到h个head上。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-26.png></p><p>此外，”The transformer”用了8个attention head，所以<a href=https://www.w3cdoc.com>我们</a>会產生8组encoder/decoder，每一组都代表将输入文字的隐向量投射到不同空间，如果<a href=https://www.w3cdoc.com>我们</a>重复计算刚刚所讲的self-attention，<a href=https://www.w3cdoc.com>我们</a>就会得到8个不同的矩阵Z，可是呢，feed-forward layer期望的是一个矩阵而非8个，所以<a href=https://www.w3cdoc.com>我们</a>要把这8个矩阵併在一起，透过乘上一个权重矩阵，还原成一个矩阵Z。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-34.png></p><h1 id=residual-connections><strong>Residual Connections</strong> <a href=#residual-connections class=anchor aria-hidden=true>#</a></h1><p>Encoder还有一个特别的架构，Multihead-attention完再接到feed-forward layer中间，还有一个sub-layer，会需要经过residual connection和layer normalization。</p><p>Residual connection 就是构建一种新的残差结构，将输出改写成和输入的残差，使得模型在训练时，微小的变化可以被注意到，这种架构很常用在电脑视觉(computer vision)，有兴趣可以参考神人Kaiming He的Deep Residual Learning for Image Recognition。</p><p>Layer normalization则是在深度学习领域中，其中一种正规化方法，最常和batch normalization进行比较，layer normalization的优点在於它是独立计算的，也就是针对单一样本进行正规化，batch normalization则是针对各维度，因此和batch size有所关联，可以参考layer normalization。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-46.png></p><h1 id=position-wise-feed-forward-networks><strong>Position-wise Feed-Forward Networks</strong> <a href=#position-wise-feed-forward-networks class=anchor aria-hidden=true>#</a></h1><p>Encoder/Decoder中的attention sublayers都会接到一层feed-forward networks(FFN)：两层线性转换和一个RELU，论文中是根据各个位置(输入句中的每个文字)分别做FFN，举例来说，如果输入文字是&lt;x1,x2…xm>，代表文字共有m个。</p><p>其中，每个位置进行相同的线性转换，这边使用的是convolution1D，也就是kernel size=1，原因是convolution1D才能保持位置的完整性，可参考CNN，模型的输入/输出维度d_{model}=512，但中间层的维度是2048，目的是为了减少计算量，这部分一样参考神人Kaiming He的Deep Residual Learning for Image Recognition。&lt;</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-26-56.png></p><h1 id=positional-encoding><strong>Positional Encoding</strong> <a href=#positional-encoding class=anchor aria-hidden=true>#</a></h1><p>和RNN不同的是，multi-head attention不能学到输入句中每个文字的位置，举例来说，“Are you very big?” and “Are big very you?”，对multi-head而言，是一样的语句，因此，”The transformer”透过positional encoding，来学习每个文字的相对/绝对位置，最后再和输入句中文字的隐向量相加。</p><p>论文使用了方程式PE(pos, 2i)=sin(pos/10000^{2i/d_{model}})、PE(pos, 2i+1)=cos(pos/10000^{2i/d_{model}})来计算positional encoding，pos代表的是位置，i代表的是维度，偶数位置的文字会透过sin函数进行转换，奇数位置的文字则透过cos函数进行转换，藉由三角函数，可以发现positional encoding 是个有週期性的波长；举例来说，[pos+k]可以写成PE[pos]的线性转换，使得模型可以学到不同位置文字间的相对位置。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-27-06.png></p><p>如下图，假设embedding 的维度为4：</p><p>每列对应的是经过positional encoding后的向量，以第一列而言，就是输入句中第一个文字隐向量和positioncal encoding后的向量和，所以每列维度都是d_{model}，总共有pos列，也就是代表输入句中有几个文字。</p><p>下图为含有20字的输入句，文字向量维度为512，可以发现图层随著位置產生变化。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-27-14.png></p><p>Encoder内容告一段落，接下来让<a href=https://www.w3cdoc.com>我们</a>看Decoder的运作模式。</p><h1 id=decoder><strong>Decoder</strong> <a href=#decoder class=anchor aria-hidden=true>#</a></h1><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-27-22.png></p><p><strong>Masked multi-head attention</strong></p><p>Decoder的运作模式和Encoder大同小异，也都是经过residual connections再到layer normalization。Encoder中的self attention在计算时，key, value, query都是来自encoder前一层的输出，Decoder亦然。</p><p>不同的地方是，为了避免在解码的时后，还在翻译前半段时，就突然翻译到后半段的句子，会在计算self-attention时的softmax前先mask掉未来的位置(设定成-∞)。这个步骤确保在预测位置i的时候只能根据i之前位置的输出，其实这个是因应Encoder-Decoder attention 的特性而做的配套措施，因为Encoder-Decoder attention可以看到encoder的整个句子，</p><h1 id=encoder-decoder-attention>Encoder-Decoder Attention <a href=#encoder-decoder-attention class=anchor aria-hidden=true>#</a></h1><p>“Encoder-Decoder Attention”和Encoder/Decoder self attention不一样，它的Query来自於decoder self-attention，而Key、Value则是encoder的output。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-27-31.png></p><p>至此，<a href=https://www.w3cdoc.com>我们</a>讲完了三种attention，接著看整体运作模式。</p><p>从输入文字的序列给Encoder开始，Encoder的output会变成attention vectors的Key、Value，接著传送至encoder-decoder attention layer，帮助Decoder该将注意力摆在输入文字序列的哪个位置进行解码。</p><h1 id=the-final-linear-and-softmax-layer><strong>The Final Linear and Softmax Layer</strong> <a href=#the-final-linear-and-softmax-layer class=anchor aria-hidden=true>#</a></h1><p>Decoder最后会產出一个向量，传到最后一层linear layer后做softmax。Linear layer只是单纯的全连接层网络，并產生每个文字对应的分数，softmax layer会将分数转成机率值，最高机率的值就是在这个时间顺序时所要產生的文字。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-27-40.png></p><h1 id=why-self-attention><strong>Why self attention?</strong> <a href=#why-self-attention class=anchor aria-hidden=true>#</a></h1><p>过去，Encoder和Decoder的核心架构都是RNN，RNN把输入句的文字序列 (x1…, xn)一个个有序地转成hidden encodings (h1…hn)，接著在產出目标句的文字序列(y1…yn)。然而，RNN的序列性导致模型不可能平行计算，此外，也导致计算复杂度很高，而且，很难捕捉长序列中词语的依赖关係(long-range dependencies)。</p><p>透过 “the transformer”，<a href=https://www.w3cdoc.com>我们</a>可以用multi-head attention来解决平行化和计算复杂度过高的问题，依赖关係也能透过self-attention中词语与词语比较时，长度只有1的方式来克服。</p><h1 id=future><strong>Future</strong> <a href=#future class=anchor aria-hidden=true>#</a></h1><p>在金融业，企业可以透过客户歷程，深入了解客户行为企业，进而提供更好的商品与服务、提升客户满意度，藉此创造价值。然而，和以往的基本特徵不同，从序列化的客户歷程资料去萃取资讯是非常困难的，在有了self-attention的知识后，<a href=https://www.w3cdoc.com>我们</a>可以将这种处理序列资料的概念应用在复杂的客户歷程上，探索客户潜在行为背后无限的商机。</p><p>笔者也推荐有兴趣钻研self-attention概念的读者，可以参考阿里巴巴所提出的论文ATrank，此篇论文将self-attention应用在產品推荐上，并带来更好的成效。</p><h1 id=参考><strong>参考</strong> <a href=#%e5%8f%82%e8%80%83 class=anchor aria-hidden=true>#</a></h1><p>[1] Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translationr. arXiv:1406.1078v3 (2014).</p><p>[2] Sequence to Sequence Learning with Neural Networks. arXiv:1409.3215v3 (2014).</p><p>[3] Neural machine translation by joint learning to align and translate. arXiv:1409.0473v7 (2016).</p><p>[4] Effective Approaches to Attention-based Neural Machine Translation. arXiv:1508.0402v5 (2015).</p><p>[5] Convolutional Sequence to Sequence learning. arXiv:1705.03122v3(2017).</p><p>[6] Attention Is All You Need. arXiv:1706.03762v5 (2017).</p><p>[7] ATRank: An Attention-Based User Behavior Modeling Framework for Recommendation. arXiv:1711.06632v2 (2017).</p><p>[8] Key-Value Memory Networks for Directly Reading Documents. arXiv:1606.03126v2 (2016).</p><p>[9] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv:1502.03044v3 (2016).</p><p>[10] Deep Residual Learning for Image Recognition. arXiv:1512.03385v1 (2015).</p><p>[11] Layer Normalization. arXiv:1607.06450v1 (2016).</p><p>来源：</p><p>https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-1-d332e85e9aad</p><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/><div class="card my-1"><div class="card-body py-2">&larr; 上瘾模型</div></div></a><a class=ms-auto href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/><div class="card my-1"><div class="card-body py-2">湖仓一体lake house &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright © 2022-Present 浙ICP备18052292号-3</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.1117772738b0b01188ff56b000f75758d3ca75fab55d0d7cf813282148e4840455e1fc0a8f1c2951391282de6e64bed66b885160643382a58f67db6d110e9feb.js integrity="sha512-ERd3JziwsBGI/1awAPdXWNPKdfq1XQ18+BMoIUjkhARV4fwKjxwpUTkSgt5uZL7Wa4hRYGQzgqWPZ9ttEQ6f6w==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.35e8488ad0bca08539c53f48a94a3c02f90c10ddc1ecba4a7fd7ec827cef556dec98a33813e3681b33a5750eefd53e65ab606d30febc27e411c293f2290f96a5.js integrity="sha512-NehIitC8oIU5xT9IqUo8AvkMEN3B7LpKf9fsgnzvVW3smKM4E+NoGzOldQ7v1T5lq2BtMP68J+QRwpPyKQ+WpQ==" crossorigin=anonymous defer></script>
<script src=/main.min.5874482df6978e314928bf4ce1c634b6464aa5445dd098e344aa61159e2b10139d82662edd47e835a3ea422c24214134f15eded85f28e55774a70f9c4919b2b3.js integrity="sha512-WHRILfaXjjFJKL9M4cY0tkZKpURd0JjjRKphFZ4rEBOdgmYu3UfoNaPqQiwkIUE08V7e2F8o5Vd0pw+cSRmysw==" crossorigin=anonymous defer></script>
<script src=/index.min.4ae26272486ea46c5bb0bed7a0b434a91b05e8182cfb839a405dd4e647b05ce5d76d401a5103d822d3b1589fc56335cd372b712d97085b8d89aebf244b1b5501.js integrity="sha512-SuJickhupGxbsL7XoLQ0qRsF6Bgs+4OaQF3U5kewXOXXbUAaUQPYItOxWJ/FYzXNNytxLZcIW42Jrr8kSxtVAQ==" crossorigin=anonymous defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
<script type="text/x-mathjax-config;executed=true">
  window.MathJax.Hub.Config({
      showProcessingMessages: false, //关闭js加载过程信息
      messageStyle: "none", //不显示信息
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
          displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
          skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
      },
      "HTML-CSS": {
          availableFonts: ["STIX", "TeX"], //可选字体
          showMathMenu: false //关闭右击菜单显示
      }
  });
  //下面第三个参数可以不写，默认对整个html内的latex进行翻译
  window.MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.getElementsByClassName("ck-content")]);
</script></body></html>