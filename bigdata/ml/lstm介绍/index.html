<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.f9552544b4ca62af741c0d24d283b4ddcfa2a026a871cff4112743eeb6c4950a530c923242d9d4d42e93635dd91ebd78601a145b8b205bb363d7065c1fa59ffe.css integrity="sha512-+VUlRLTKYq90HA0k0oO03c+ioCaocc/0ESdD7rbElQpTDJIyQtnU1C6TY13ZHr14YBoUW4sgW7Nj1wZcH6Wf/g==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>LSTM介绍 - 万维刀客「w3cdoc」</title><meta name=description content="之前一直在做CNN的一些研究，最近刚刚回到实验室，定下来了自己的小组，然后开始了一些LSTM的学习。
将近学习了两天半吧，结构弄得差不多了，Theano上LSTM tutorial 的例程也跑了跑，正在读代码ing。
这篇博客主要是我之后要做的一个小报告的梗概，梳理了一下LSTM的特点和适用性问题。
发在这里权当做开博客压压惊。
希望之后能跟各位朋友多多交流，共同进步。
1. 概念： # Long short-termmemory (LSTM)is a recurrent neuralnetwork (RNN)architecture (an artificialneural network)published[1] in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universalin the sense that given enough network units it can compute anything aconventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program. Unliketraditional RNNs, an LSTM network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknownsize between important events."><link rel=canonical href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="LSTM介绍"><meta property="og:description" content="之前一直在做CNN的一些研究，最近刚刚回到实验室，定下来了自己的小组，然后开始了一些LSTM的学习。
将近学习了两天半吧，结构弄得差不多了，Theano上LSTM tutorial 的例程也跑了跑，正在读代码ing。
这篇博客主要是我之后要做的一个小报告的梗概，梳理了一下LSTM的特点和适用性问题。
发在这里权当做开博客压压惊。
希望之后能跟各位朋友多多交流，共同进步。
1. 概念： # Long short-termmemory (LSTM)is a recurrent neuralnetwork (RNN)architecture (an artificialneural network)published[1] in 1997 by Sepp Hochreiter and Jürgen Schmidhuber. Like most RNNs, an LSTM network is universalin the sense that given enough network units it can compute anything aconventional computer can compute, provided it has the proper weight matrix, which may be viewed as its program. Unliketraditional RNNs, an LSTM network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknownsize between important events."><meta property="og:url" content="/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/"><meta property="og:site_name" content="万维刀客「w3cdoc」"><meta property="og:image" content="/doks.png"><meta property="og:image:alt" content="万维刀客「w3cdoc」"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@w3cdoc"><meta name=twitter:creator content="@chalecao"><meta name=twitter:title content="LSTM介绍"><meta name=twitter:description content><meta name=twitter:image content="/doks.png"><meta name=twitter:image:alt content="LSTM介绍"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"/#/schema/organization/1","name":"w3cdoc","url":"/","sameAs":["https://twitter.com/w3cdoc"],"logo":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/w3cdoc.png","width":512,"height":512,"caption":"w3cdoc"},"image":{"@id":"/#/schema/image/1"}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"万维刀客「w3cdoc」","description":"互联网同学互相帮助、学习成长的家园","publisher":{"@id":"/#/schema/organization/1"}},{"@type":"WebPage","@id":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/","url":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/","name":"LSTM介绍","description":"","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/organization/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/"]}]},{"@type":"BreadcrumbList","@id":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/bigdatamllstm%E4%BB%8B%E7%BB%8D/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/#/schema/image/2","url":"/doks.png","contentUrl":"/doks.png","caption":"LSTM介绍"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js",{scope:"/"}).then(function(e){console.log("Registration succeeded. Scope is "+e.scope)}).catch(function(e){console.log("Registration failed with "+e)})</script><script src=/js/vendor/autolog.js async></script></head><body class="bigdata single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=万维刀客「w3cdoc」>万维刀客「w3cdoc」</a>
<button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>万维刀客「w3cdoc」</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>初级入门
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/js/basic/javascript%E4%BB%8B%E7%BB%8D/>JS入门</a></li><li><a class=dropdown-item href=/git/basic/introduction/>Git入门</a></li></ul></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>进阶学习
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/fed-regain/html/html%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2/>前端增长</a></li><li><a class=dropdown-item href=/webgl/base/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/>3D图形</a></li><li><a class=dropdown-item href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>数据分析</a></li></ul></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/blog/>博客</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/course/>网课教程</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=false>
计算环境</button><div class=collapse id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=true>
机器学习</button><div class="collapse show" id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=false>
计算环境</button><div class=collapse id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=true>
机器学习</button><div class="collapse show" id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded active" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button>
<button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>Book Menu</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#1-概念>1. 概念：</a></li><li><a href=#2类属>2.类属</a></li><li><a href=#3模型的特点与适用性>3.模型的特点与适用性</a></li><li><a href=#31-前馈神经网络vs-反馈神经网络>3.1 前馈神经网络VS 反馈神经网络</a></li><li><a href=#32-cnn-vs-rnn>3.2 CNN vs RNN</a></li><li><a href=#33-lstm-vs-传统rnns>3.3 LSTM vs (传统)RNNs</a></li><li><a href=#4在不同任务上的数据对比>4.在不同任务上的数据对比</a></li><li><a href=#5结论>5.结论</a></li><li><a href=#6案例>6.案例</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#1-概念>1. 概念：</a></li><li><a href=#2类属>2.类属</a></li><li><a href=#3模型的特点与适用性>3.模型的特点与适用性</a></li><li><a href=#31-前馈神经网络vs-反馈神经网络>3.1 前馈神经网络VS 反馈神经网络</a></li><li><a href=#32-cnn-vs-rnn>3.2 CNN vs RNN</a></li><li><a href=#33-lstm-vs-传统rnns>3.3 LSTM vs (传统)RNNs</a></li><li><a href=#4在不同任务上的数据对比>4.在不同任务上的数据对比</a></li><li><a href=#5结论>5.结论</a></li><li><a href=#6案例>6.案例</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9 mx-xl-auto"><h1>LSTM介绍</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button>
<button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>Book Menu</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#1-概念>1. 概念：</a></li><li><a href=#2类属>2.类属</a></li><li><a href=#3模型的特点与适用性>3.模型的特点与适用性</a></li><li><a href=#31-前馈神经网络vs-反馈神经网络>3.1 前馈神经网络VS 反馈神经网络</a></li><li><a href=#32-cnn-vs-rnn>3.2 CNN vs RNN</a></li><li><a href=#33-lstm-vs-传统rnns>3.3 LSTM vs (传统)RNNs</a></li><li><a href=#4在不同任务上的数据对比>4.在不同任务上的数据对比</a></li><li><a href=#5结论>5.结论</a></li><li><a href=#6案例>6.案例</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#1-概念>1. 概念：</a></li><li><a href=#2类属>2.类属</a></li><li><a href=#3模型的特点与适用性>3.模型的特点与适用性</a></li><li><a href=#31-前馈神经网络vs-反馈神经网络>3.1 前馈神经网络VS 反馈神经网络</a></li><li><a href=#32-cnn-vs-rnn>3.2 CNN vs RNN</a></li><li><a href=#33-lstm-vs-传统rnns>3.3 LSTM vs (传统)RNNs</a></li><li><a href=#4在不同任务上的数据对比>4.在不同任务上的数据对比</a></li><li><a href=#5结论>5.结论</a></li><li><a href=#6案例>6.案例</a></li></ul></nav></div></nav><p>之前一直在做CNN的一些研究，最近刚刚回到实验室，定下来了自己的小组，然后开始了一些LSTM的学习。</p><p>将近学习了两天半吧，结构弄得差不多了，Theano上LSTM tutorial 的例程也跑了跑，正在读代码ing。</p><p>这篇博客主要是我之后要做的一个小报告的梗概，梳理了一下LSTM的特点和适用性问题。</p><p>发在这里权当做开博客压压惊。</p><p>希望之后能跟各位朋友多多交流，共同进步。</p><h2 id=1-概念>1. 概念： <a href=#1-%e6%a6%82%e5%bf%b5 class=anchor aria-hidden=true>#</a></h2><p><strong>Long short-termmemory</strong> (<strong>LSTM</strong>)is a <a class=outlink title="Recurrent neural network" target=_blank rel="nofollow noopener noreferrer">recurrent neuralnetwork</a> (RNN)architecture (an <a class=outlink title="Artificial neural network" target=_blank rel="nofollow noopener noreferrer">artificialneural network</a>)published<a class=outlink target=_blank rel="nofollow noopener noreferrer"><sup>[1]</sup></a> in 1997 by <a class=outlink title="Sepp Hochreiter" target=_blank rel="nofollow noopener noreferrer">Sepp Hochreiter</a> and <a class=outlink title="Jürgen Schmidhuber" target=_blank rel="nofollow noopener noreferrer">Jürgen Schmidhuber</a>. Like most RNNs, an LSTM network is universalin the sense that given enough network units it can compute anything aconventional computer can compute, provided it has the proper <a class=outlink title=Weight target=_blank rel="nofollow noopener noreferrer">weight</a> <a class=outlink title="Matrix (mathematics)" target=_blank rel="nofollow noopener noreferrer">matrix</a>, which may be viewed as its program. Unliketraditional RNNs, an LSTM network is well-suited to learn from experience to <a class=outlink title="Classification in machine learning" target=_blank rel="nofollow noopener noreferrer">classify</a>, <a class=outlink title="Computer data processing" target=_blank rel="nofollow noopener noreferrer">process</a> and <a class=outlink title=Predict target=_blank rel="nofollow noopener noreferrer">predict</a> <a class=outlink title="Time series" target=_blank rel="nofollow noopener noreferrer">time series</a> when there are very long time lags of unknownsize between important events. This is one of the main reasons why LSTMoutperforms alternative RNNs and <a class=outlink title="Hidden Markov Models" target=_blank rel="nofollow noopener noreferrer">Hidden Markov Models</a> and other sequence learning methods in numerousapplications.</p><h2 id=2类属>2.类属 <a href=#2%e7%b1%bb%e5%b1%9e class=anchor aria-hidden=true>#</a></h2><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-17-51.png></p><p><strong>LSTM是RNN的一种变种，属于反馈神经网络的范畴。</strong></p><hr><h2 id=3模型的特点与适用性>3.模型的特点与适用性 <a href=#3%e6%a8%a1%e5%9e%8b%e7%9a%84%e7%89%b9%e7%82%b9%e4%b8%8e%e9%80%82%e7%94%a8%e6%80%a7 class=anchor aria-hidden=true>#</a></h2><h2 id=31-前馈神经网络vs-反馈神经网络>3.1 前馈神经网络VS 反馈神经网络 <a href=#31-%e5%89%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9cvs-%e5%8f%8d%e9%a6%88%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c class=anchor aria-hidden=true>#</a></h2><p>在深度学习领域，传统的前馈神经网络（feed-forward neural net，简称FNN）具有出色的表现，取得了许多成功，它曾在许多不同的任务上——包括手写数字识别和目标分类上创造了记录。甚至到了今天，FNN在解决分类任务上始终都比其他方法要略胜一筹。</p><p>尽管如此，大多数专家还是会达成共识：FNN可以实现的功能仍然相当有限。究其原因，人类的大脑有着惊人的计算功能，而“分类”任务仅仅是其中很小的一个组成部分。<a href=https://www.w3cdoc.com>我们</a>不仅能够识别个体案例，更能分析输入信息之间的整体逻辑序列。这些信息序列富含有大量的内容，信息彼此间有着复杂的时间关联性，并且信息长度各种各样。例如视觉、开车、演讲还有理解能力，这些都需要<a href=https://www.w3cdoc.com>我们</a>同时处理高维度的多种输入信息，因为它们时时都在变化，而这是FNN在建模时就极为匮乏的。</p><h2 id=32-cnn-vs-rnn>3.2 CNN vs RNN <a href=#32-cnn-vs-rnn class=anchor aria-hidden=true>#</a></h2><p>《Convolutional Networks for Images, Speech,and Time Series》，by YannLeCun & Yoshua Bengio</p><blockquote><p><a href=http://nuyoo.utm.mx/~jjf/rna/A12%20Convolutional%20networks%20for%20images,%20speech,%20and%20time%20series.pdf>Convolutional Networks for Images</a></p></blockquote><p>尤其是这一段：<br>While characters or short spoken words can besize-normalized and fed to a fixed-size network, more complex objects such aswritten or spoken words and sentences have inherently variable size. One way ofhandling such a composite object is to segment it heuristically into simplerobjects that can be recognized individually, e.g. characters phonemes. However,reliable segmentation heuristics do not exist for speech or cursivehandwriting. A bruteforce solution…..<br>简单的说，CNN并不完全适用于学习时间序列，因此会需要各种辅助性处理，且效果也不一定好。面对对时间序列敏感的问题赫和任务，RNN(如LSTM)通常会比较合适。</p><p>一个例子：</p><p>Task 1 - Sentiment analysis: You’re given some review, and youwant to predict the rating of the review.<br>Task 2 - Machine translation: Translate a sentence from some source language totarget language.</p><p>Now, the basic difference in terms of applicability of conv-net and RNN is thatconv-nets (like most other machine learning algorithm) take a fixed size inputand generate fixed-size outputs. RNN, on the other hand, can handle arbitraryinput/output lengths, but would typically require much more data compared toconv-nets because it is a more complex model.</p><p>Using this insight, we see that task 2 cannot be performed by conv-nets, sinceinputs and outputs are not fixed-length. So RNNs for task 2.</p><p>For task 1, however, you can use RNN if you have a lot of data. But you canalso use conv-nets - fix the length of the input, and adjust the input lengthby truncating or padding the actual input. Note that this will not affect thesentiment of the review much, so this is a reasonable approach. And since it’sa 1D convolution, that is typically used in sequences, it is called temporalconvolution. Conceptually, it is similar to 2D spatial convolution.</p><p>小结：</p><p>RNN回归型网络，用于序列数据，并且有了一定的记忆效应，辅之以lstm。<br>CNN应该侧重空间映射，图像数据尤为贴合此场景。</p><h2 id=33-lstm-vs-传统rnns>3.3 LSTM vs (传统)RNNs <a href=#33-lstm-vs-%e4%bc%a0%e7%bb%9frnns class=anchor aria-hidden=true>#</a></h2><p>两篇文章的描述：</p><p><strong>1.     AlexGraves. 《SupervisedSequence Labelling with Recurrent Neural Networks》. Textbook, Studies inComputational Intelligence, Springer, 2012.</strong></p><p>“Long Short-term Memory (LSTM) is an RNN architecture designed to be better at storing and accessing informationthanstandard RNNs. LSTM has recently given state-of-the-art results in a variety ofsequenceprocessing tasks, including speech andhandwriting recognition .”</p><p><strong>2.    Yann LeCun、Yoshua Bengio和Geoffrey Hinton合作的这篇综述文章《</strong><a class=outlink target=_blank rel="nofollow noopener noreferrer">Deep Learning</a><strong>》</strong></p><p>“RNNs一旦展开（如图5），可以将之视为一个所有层共享同样权值的深度前馈神经网络。虽然它们的目的是学习长期的依赖性，但理论的和经验的证据表明很难学习并长期保存信息。</p><p>为了解决这个问题，一个增大网络存储的想法随之产生。采用了特殊隐式单元的LSTM（long short-termmemory networks）被首先提出，其自然行为便是长期的保存输入。一种称作记忆细胞的特殊单元类似累加器和门控神经元：它在下一个时间步长将拥有一个权值并联接到自身，拷贝自身状态的真实值和累积的外部信号，但这种自联接是由另一个单元学习并决定何时清除记忆内容的乘法门控制的。</p><p>LSTM网络随后被证明比传统的RNNs更加有效，尤其当每一个时间步长内有若干层时，整个语音识别系统能够完全一致的将声学转录为字符序列。目前LSTM网络或者相关的门控单元同样用于编码和解码网络，并且在机器翻译中表现良好。”</p><h2 id=4在不同任务上的数据对比>4.在不同任务上的数据对比 <a href=#4%e5%9c%a8%e4%b8%8d%e5%90%8c%e4%bb%bb%e5%8a%a1%e4%b8%8a%e7%9a%84%e6%95%b0%e6%8d%ae%e5%af%b9%e6%af%94 class=anchor aria-hidden=true>#</a></h2><div align=center><table class="table table-bordered" border=1 width=667 cellspacing=0 cellpadding=0><tr><td valign=bottom><p>Task</p></td><td valign=bottom><p>classification</p></td><td valign=bottom><p>sentiment analysis</p></td><td valign=bottom><p>machine translation</p></td><td valign=bottom><p>dialog</p></td><td valign=bottom><p>language generation</p></td><td valign=bottom><p>QA</p></td><td valign=bottom><p>total</p></td></tr><tr><td colspan=8 valign=bottom width=667>2006年以来，从Google Scholar上的检索数据进行对比</td></tr><tr><td valign=bottom><p>LSTM</p></td><td valign=bottom><p>1900</p></td><td valign=bottom><p>148</p></td><td valign=bottom><p>616</p></td><td valign=bottom><p>373</p></td><td valign=bottom><p>27</p></td><td valign=bottom><p>59</p></td><td valign=bottom><p>3690</p></td></tr><tr><td valign=bottom><p>CNN</p></td><td valign=bottom><p>5060</p></td><td valign=bottom><p>179</p></td><td valign=bottom><p>247</p></td><td valign=bottom><p>304</p></td><td valign=bottom><p>30</p></td><td valign=bottom><p>100</p></td><td valign=bottom><p>5670</p></td></tr><tr><td colspan=8 valign=bottom width=667>从Web of Science数据库上的主题检索进行对比(全时间)</td></tr><tr><td valign=bottom><p>LSTM</p></td><td valign=bottom><p>56</p></td><td valign=bottom></td><td valign=bottom><p>1</p></td><td valign=bottom></td><td valign=bottom><p>6</p></td><td valign=bottom><p>2</p></td><td valign=bottom><p>248</p></td></tr><tr><td valign=bottom><p>CNN</p></td><td valign=bottom><p>373</p></td><td valign=bottom><p>2</p></td><td valign=bottom><p>13</p></td><td valign=bottom></td><td valign=bottom><p>25</p></td><td valign=bottom><p>2</p></td><td valign=bottom><p>1064</p></td></tr></table></div><p>数据尽管在检索上还有一些问题，尤其是 WOS数据库上涵盖的文章可能代表了一部分水平比较高的论文，在数量上并不完全按与研究的力度划等号，但还是可以看出一些端倪。</p><p>CNN大部分的任务都是跟分类相关的，在处理一些较为复杂的任务上的应用暂时还比较匮乏。而LSTM在近年来尤其发展迅猛，在处理序列相关的任务时的应用较为广泛。</p><h2 id=5结论>5.结论 <a href=#5%e7%bb%93%e8%ae%ba class=anchor aria-hidden=true>#</a></h2><p>LSTM是RNN的一个优秀的变种模型，继承了大部分RNN模型的特性，同时解决了梯度反传过程由于逐步缩减而产生的Vanishing Gradient问题。具体到语言处理任务中，LSTM非常适合用于处理与时间序列高度相关的问题，例如机器翻译、对话生成、编码\解码等。</p><p>虽然在分类问题上，至今看来以CNN为代表的前馈网络依然有着性能的优势，但是LSTM在长远的更为复杂的任务上的潜力是CNN无法媲美的。它更真实地表征或模拟了人类行为、逻辑发展和神经组织的认知过程。尤其从2014年以来，LSTM已经成为RNN甚至深度学习框架中非常热点的研究模型，得到大量的关注和研究。</p><h2 id=6案例>6.案例 <a href=#6%e6%a1%88%e4%be%8b class=anchor aria-hidden=true>#</a></h2><ul><li><a href=https://www.zhihu.com/question/34681168>CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)</a></li><li><a href=https://www.altumintelligence.com/articles/a/Time-Series-Prediction-Using-LSTM-Deep-Neural-Networks>案例剖析：利用LSTM深层神经网络进行时间序列预测</a></li></ul><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/><div class="card my-1"><div class="card-body py-2">&larr; 用户体验</div></div></a><a class=ms-auto href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/><div class="card my-1"><div class="card-body py-2">交易模型 &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright © 2022-Present 浙ICP备18052292号-3</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.1117772738b0b01188ff56b000f75758d3ca75fab55d0d7cf813282148e4840455e1fc0a8f1c2951391282de6e64bed66b885160643382a58f67db6d110e9feb.js integrity="sha512-ERd3JziwsBGI/1awAPdXWNPKdfq1XQ18+BMoIUjkhARV4fwKjxwpUTkSgt5uZL7Wa4hRYGQzgqWPZ9ttEQ6f6w==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.35e8488ad0bca08539c53f48a94a3c02f90c10ddc1ecba4a7fd7ec827cef556dec98a33813e3681b33a5750eefd53e65ab606d30febc27e411c293f2290f96a5.js integrity="sha512-NehIitC8oIU5xT9IqUo8AvkMEN3B7LpKf9fsgnzvVW3smKM4E+NoGzOldQ7v1T5lq2BtMP68J+QRwpPyKQ+WpQ==" crossorigin=anonymous defer></script>
<script src=/main.min.5874482df6978e314928bf4ce1c634b6464aa5445dd098e344aa61159e2b10139d82662edd47e835a3ea422c24214134f15eded85f28e55774a70f9c4919b2b3.js integrity="sha512-WHRILfaXjjFJKL9M4cY0tkZKpURd0JjjRKphFZ4rEBOdgmYu3UfoNaPqQiwkIUE08V7e2F8o5Vd0pw+cSRmysw==" crossorigin=anonymous defer></script>
<script src=/index.min.4ae26272486ea46c5bb0bed7a0b434a91b05e8182cfb839a405dd4e647b05ce5d76d401a5103d822d3b1589fc56335cd372b712d97085b8d89aebf244b1b5501.js integrity="sha512-SuJickhupGxbsL7XoLQ0qRsF6Bgs+4OaQF3U5kewXOXXbUAaUQPYItOxWJ/FYzXNNytxLZcIW42Jrr8kSxtVAQ==" crossorigin=anonymous defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
<script type="text/x-mathjax-config;executed=true">
  window.MathJax.Hub.Config({
      showProcessingMessages: false, //关闭js加载过程信息
      messageStyle: "none", //不显示信息
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
          displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
          skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
      },
      "HTML-CSS": {
          availableFonts: ["STIX", "TeX"], //可选字体
          showMathMenu: false //关闭右击菜单显示
      }
  });
  //下面第三个参数可以不写，默认对整个html内的latex进行翻译
  window.MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.getElementsByClassName("ck-content")]);
</script></body></html>