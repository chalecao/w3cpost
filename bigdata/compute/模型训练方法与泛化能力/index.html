<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=/main.f9552544b4ca62af741c0d24d283b4ddcfa2a026a871cff4112743eeb6c4950a530c923242d9d4d42e93635dd91ebd78601a145b8b205bb363d7065c1fa59ffe.css integrity="sha512-+VUlRLTKYq90HA0k0oO03c+ioCaocc/0ESdD7rbElQpTDJIyQtnU1C6TY13ZHr14YBoUW4sgW7Nj1wZcH6Wf/g==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>模型训练与损失和泛化能力 - 万维刀客「w3cdoc」</title><meta name=description content="机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果。
训练与损失 # 简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的取值。
模型训练通用步骤 # 通常学习一个好的函数，分为以下三步：
1、选择一个合适的模型，这通常需要依据实际问题而定，针对不同的问题和任务需要选取恰当的模型，模型就是一组函数的集合。
2、判断一个函数的好坏，这需要确定一个衡量标准，也就是我们通常说的损失函数（Loss Function），损失函数的确定也需要依据具体问题而定，如回归问题一般采用欧式距离，分类问题一般采用交叉熵代价函数。
3、找出“最好”的函数，如何从众多函数中最快的找出“最好”的那一个，这一步是最大的难点，做到又快又准往往不是一件容易的事情。常用的方法有梯度下降算法，最小二乘法等和其他一些技巧（tricks）。学习得到“最好”的函数后，需要在新样本上进行测试，只有在新样本上表现很好，才算是一个“好”的函数。
在监督式学习中，机器学习算法通过以检查多个样本并尝试找出可最大限度地减少损失（cost）的模型，这一过程称为经验风险最小化。
损失是对算法预测数据的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。
其中红色箭头表示损失，蓝色线是算法模型预测的值，黄色圈是实际值。
左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。
损失函数 # 平方损失 # 线性回归模型最常使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：
$$ (y - y)^2 $$
均方误差 (MSE) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
$$ MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2 $$ 其中：(x,y) 指的是样本，其中x 指的是模型进行预测时使用的特征集（例如，温度）。y 指的是样本的标签（例如，知了每分钟的鸣叫次数）。
prediction(x) 指的是权重和偏差与特征集 x 结合的函数。
D 指的是包含多个有标签样本（即 (x,y)）的数据集。N 指的是 D 中的样本数量。
虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。
降低损失 # 为了让我们的模型更加准确，就需要减少损失。下图是机器学习中的一般方法：
通过不断的试错，不断地更新参数，最终可以将损失收敛在要求范围内，那么这个模型就训练完成了。
回想一下我们上节课介绍的温度和知了叫声的模型：
$$ y = b + w_1x_1 $$"><link rel=canonical href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="模型训练与损失和泛化能力"><meta property="og:description" content="机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果。
训练与损失 # 简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的取值。
模型训练通用步骤 # 通常学习一个好的函数，分为以下三步：
1、选择一个合适的模型，这通常需要依据实际问题而定，针对不同的问题和任务需要选取恰当的模型，模型就是一组函数的集合。
2、判断一个函数的好坏，这需要确定一个衡量标准，也就是我们通常说的损失函数（Loss Function），损失函数的确定也需要依据具体问题而定，如回归问题一般采用欧式距离，分类问题一般采用交叉熵代价函数。
3、找出“最好”的函数，如何从众多函数中最快的找出“最好”的那一个，这一步是最大的难点，做到又快又准往往不是一件容易的事情。常用的方法有梯度下降算法，最小二乘法等和其他一些技巧（tricks）。学习得到“最好”的函数后，需要在新样本上进行测试，只有在新样本上表现很好，才算是一个“好”的函数。
在监督式学习中，机器学习算法通过以检查多个样本并尝试找出可最大限度地减少损失（cost）的模型，这一过程称为经验风险最小化。
损失是对算法预测数据的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。
其中红色箭头表示损失，蓝色线是算法模型预测的值，黄色圈是实际值。
左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。
损失函数 # 平方损失 # 线性回归模型最常使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：
$$ (y - y)^2 $$
均方误差 (MSE) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
$$ MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2 $$ 其中：(x,y) 指的是样本，其中x 指的是模型进行预测时使用的特征集（例如，温度）。y 指的是样本的标签（例如，知了每分钟的鸣叫次数）。
prediction(x) 指的是权重和偏差与特征集 x 结合的函数。
D 指的是包含多个有标签样本（即 (x,y)）的数据集。N 指的是 D 中的样本数量。
虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。
降低损失 # 为了让我们的模型更加准确，就需要减少损失。下图是机器学习中的一般方法：
通过不断的试错，不断地更新参数，最终可以将损失收敛在要求范围内，那么这个模型就训练完成了。
回想一下我们上节课介绍的温度和知了叫声的模型：
$$ y = b + w_1x_1 $$"><meta property="og:url" content="/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/"><meta property="og:site_name" content="万维刀客「w3cdoc」"><meta property="og:image" content="/doks.png"><meta property="og:image:alt" content="万维刀客「w3cdoc」"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@w3cdoc"><meta name=twitter:creator content="@chalecao"><meta name=twitter:title content="模型训练与损失和泛化能力"><meta name=twitter:description content><meta name=twitter:image content="/doks.png"><meta name=twitter:image:alt content="模型训练与损失和泛化能力"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"/#/schema/organization/1","name":"w3cdoc","url":"/","sameAs":["https://twitter.com/w3cdoc"],"logo":{"@type":"ImageObject","@id":"/#/schema/image/1","url":"/w3cdoc.png","width":512,"height":512,"caption":"w3cdoc"},"image":{"@id":"/#/schema/image/1"}},{"@type":"WebSite","@id":"/#/schema/website/1","url":"/","name":"万维刀客「w3cdoc」","description":"互联网同学互相帮助、学习成长的家园","publisher":{"@id":"/#/schema/organization/1"}},{"@type":"WebPage","@id":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/","url":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/","name":"模型训练与损失和泛化能力","description":"","isPartOf":{"@id":"/#/schema/website/1"},"about":{"@id":"/#/schema/organization/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/"]}]},{"@type":"BreadcrumbList","@id":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"/","url":"/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@id":"/bigdatacompute%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/#/schema/image/2","url":"/doks.png","contentUrl":"/doks.png","caption":"模型训练与损失和泛化能力"}]}]}</script><meta name=theme-color content="#fff"><link rel=icon href=/favicon.ico sizes=any><link rel=icon type=image/svg+xml href=/favicon.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=/site.webmanifest><script>"serviceWorker"in navigator&&navigator.serviceWorker.register("/sw.js",{scope:"/"}).then(function(e){console.log("Registration succeeded. Scope is "+e.scope)}).catch(function(e){console.log("Registration failed with "+e)})</script><script src=/js/vendor/autolog.js async></script></head><body class="bigdata single"><div class=sticky-top><div class=header-bar></div><header class="navbar navbar-expand-lg navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-lg-nowrap" aria-label="Main navigation"><a class="navbar-brand order-0" href=/ aria-label=万维刀客「w3cdoc」>万维刀客「w3cdoc」</a>
<button class="btn btn-menu order-2 d-block d-lg-none" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-lg-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-lg-none"></div><div class="offcanvas-header d-lg-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>万维刀客「w3cdoc」</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body p-4 p-lg-0"><ul class="nav flex-column flex-lg-row align-items-lg-center mt-2 mt-lg-0 ms-lg-2 me-lg-auto"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>初级入门
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/js/basic/javascript%E4%BB%8B%E7%BB%8D/>JS入门</a></li><li><a class=dropdown-item href=/git/basic/introduction/>Git入门</a></li></ul></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>进阶学习
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/fed-regain/html/html%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2/>前端增长</a></li><li><a class=dropdown-item href=/webgl/base/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/>3D图形</a></li><li><a class=dropdown-item href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>数据分析</a></li></ul></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/blog/>博客</a></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/course/>网课教程</a></li></ul><hr class="text-black-50 my-4 d-lg-none"><form class="doks-search position-relative flex-grow-1 ms-lg-auto me-lg-2"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><hr class="text-black-50 my-4 d-lg-none"><ul class="nav flex-column flex-lg-row"></ul><hr class="text-black-50 my-4 d-lg-none"><button id=mode class="btn btn-link" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></div></div></nav></header></div><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=true>
计算环境</button><div class="collapse show" id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded active" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=false>
机器学习</button><div class=collapse id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-dd4e9c25c56177a32ad30dd10b4f68ff aria-expanded=true>
计算环境</button><div class="collapse show" id=section-dd4e9c25c56177a32ad30dd10b4f68ff><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/>机器学习基础了解</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>机器学习与深度学习介绍</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/>机器学习基础数学知识</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/>机器学习常用术语及含义</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/>机器学习与深度学习常用算法</a></li><li><a class="docs-link rounded active" href=/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/>模型训练与损失和泛化能力</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/>搭建本地python和TensorFlow环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/>在线jupyter环境</a></li><li><a class="docs-link rounded" href=/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/>python-编程基础知识</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-57a4cb7fa1cf829f30045bd59498a1bd aria-expanded=false>
计算框架</button><div class=collapse id=section-57a4cb7fa1cf829f30045bd59498a1bd><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E7%9A%84%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%BF%9B%E5%8C%96%E4%B9%8B%E8%B7%AF/>阿里巴巴的大数据进化之路</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%BA%93mpp-mapreduce/>大数据数据库MPP-MapReduce</a></li><li><a class="docs-link rounded" href=/bigdata/framework/mpp%E6%9E%B6%E6%9E%84/>MPP架构</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/>大数据与流计算概览</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E6%B5%81%E8%AE%A1%E7%AE%97%E5%BC%95%E6%93%8Eblink/>阿里巴巴流计算引擎Blink</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%B5%81%E8%AE%A1%E7%AE%97%E4%B9%8Bflink%E4%BB%8B%E7%BB%8D/>大数据之流计算之Flink介绍</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E6%B9%96/>数据仓库数据湖</a></li><li><a class="docs-link rounded" href=/bigdata/framework/%E6%B9%96%E4%BB%93%E4%B8%80%E4%BD%93lake-house/>湖仓一体lake house</a></li><li><a class="docs-link rounded" href=/bigdata/framework/tensorflow-js%E7%AE%80%E5%8D%95%E6%A6%82%E5%BF%B5%E5%92%8C%E7%94%A8%E6%B3%95/>TensorFlow.js简单概念和用法</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-61bb1751fd355596e307767d1927c855 aria-expanded=false>
机器学习</button><div class=collapse id=section-61bb1751fd355596e307767d1927c855><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/ml/lstm%E4%BB%8B%E7%BB%8D/>LSTM介绍</a></li><li><a class="docs-link rounded" href=/bigdata/ml/lsm%E6%A0%91%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8/>LSM树有什么用</a></li><li><a class="docs-link rounded" href=/bigdata/ml/kimball%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1/>KimBall维度建模</a></li><li><a class="docs-link rounded" href=/bigdata/ml/huffman%E6%A0%91%E5%92%8Chuffman%E7%BC%96%E7%A0%81/>Huffman树和Huffman编码</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87%E4%B8%8E%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/>先验概率与后验概率、贝叶斯区别与联系</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention1/>从Seq2seq到Attention模型到Self Attention（1）</a></li><li><a class="docs-link rounded" href=/bigdata/ml/%E4%BB%8Eseq2seq%E5%88%B0attention%E6%A8%A1%E5%9E%8B%E5%88%B0self-attention2/>从Seq2seq到Attention模型到Self Attention（2）</a></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6450d84327e5a11bef853c4b9d557f42 aria-expanded=false>
数据分析</button><div class=collapse id=section-6450d84327e5a11bef853c4b9d557f42><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-3e958c6d36d4f85f47d730c55380b05f aria-expanded=false>
数据分析模型</button><div class=collapse id=section-3e958c6d36d4f85f47d730c55380b05f><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%BA%A4%E6%98%93%E6%A8%A1%E5%9E%8B/>交易模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F/>商业模式</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/rarra%E5%A2%9E%E9%95%BF%E6%A8%A1%E5%9E%8B/>RARRA增长模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/ttpprc%E5%95%86%E4%B8%9A%E6%A8%A1%E5%9E%8B/>TTPPRC商业模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E6%A0%87%E5%87%86%E7%B4%A2%E6%B4%9B%E6%A8%A1%E5%9E%8B/>标准索洛模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E9%9C%80%E6%B1%82%E4%B8%89%E8%A7%92%E6%A8%A1%E5%9E%8B/>需求三角模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E5%BF%83%E6%B5%81%E6%A8%A1%E5%9E%8B/>心流模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/%E4%B8%8A%E7%98%BE%E6%A8%A1%E5%9E%8B/>上瘾模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/mvp%E6%A8%A1%E5%9E%8B/>MVP模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/pmf%E6%A8%A1%E5%9E%8B/>PMF模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/model/35%E4%B8%AA%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>35个数据分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-34afc8f397ba5a28887ab6dcab63b753 aria-expanded=false>
数据分析方法</button><div class=collapse id=section-34afc8f397ba5a28887ab6dcab63b753><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%82%B9%E5%87%BB%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>点击分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E8%A1%8C%E4%B8%BA%E4%BA%8B%E4%BB%B6%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>行为事件分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E6%BC%8F%E6%96%97%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>漏斗分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/>用户分群画像</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/session%E4%BC%9A%E8%AF%9D%E5%88%86%E6%9E%90/>Session会话分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E5%81%9A%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/>如何做用户行为分析</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%A6%8F%E6%A0%BC%E8%A1%8C%E4%B8%BA%E6%A8%A1%E5%9E%8B/>福格行为模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E9%BA%A6%E8%82%AF%E9%94%A1%E9%80%BB%E8%BE%91%E6%A0%91/>麦肯锡逻辑树</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%94%A8%E6%88%B7%E7%9A%84%E5%BF%83%E6%99%BA%E6%A8%A1%E5%9E%8B/>如何构建用户的心智模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%A2%9E%E9%95%BF%E4%B9%8Baarrr%E6%A8%A1%E5%9E%8B/>用户增长之AARRR模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E7%95%99%E5%AD%98%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户留存分析模型</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%9B%A0%E6%9E%9C%E5%85%B3%E7%B3%BB%E4%B9%8B%E6%A2%AF/>因果关系之梯</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%AE%A2%E6%88%B7%E4%BB%B7%E5%80%BC%E4%B8%BB%E5%BC%A0%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90%E4%BA%A7%E5%93%81%E4%BB%B7%E5%80%BC/>客户价值主张模型分析产品价值</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E6%B3%95/>常见的相关性分析法</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B/>用户行为路径分析模型</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-9a04dfa7d7b348c5eb595a08dac0f926 aria-expanded=false>
数据分析应用</button><div class=collapse id=section-9a04dfa7d7b348c5eb595a08dac0f926><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>用户体验</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C%E6%8C%87%E6%A0%87%E4%B9%8Bcsat-nps-ces/>用户体验指标之CSAT-NPS-CES</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E6%8F%90%E5%8D%87%E6%AF%9B%E5%88%A9%E7%8E%87%E7%9A%84%E4%B8%8D%E5%90%8C%E7%AD%96%E7%95%A5/>提升毛利率的不同策略</a></li><li><a class="docs-link rounded" href=/bigdata/analysis/action/%E8%AE%A1%E5%88%86%E6%B3%95%E9%87%8F%E5%8C%96%E7%94%A8%E6%88%B7%E4%BD%93%E9%AA%8C/>计分法量化用户体验</a></li></ul></div></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button>
<button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>Book Menu</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#训练与损失>训练与损失</a><ul><li><a href=#模型训练通用步骤>模型训练通用步骤</a></li></ul></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#平方损失>平方损失</a></li></ul></li><li><a href=#降低损失>降低损失</a><ul><li><a href=#梯度下降法>梯度下降法</a></li></ul></li><li><a href=#随机梯度下降法>随机梯度下降法</a></li><li><a href=#泛化能力>泛化能力</a><ul><li><a href=#规则化regularize>规则化（Regularize）</a></li></ul></li><li><a href=#l1-范数>L1-范数</a></li><li><a href=#l2-范数>L2-范数</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#训练与损失>训练与损失</a><ul><li><a href=#模型训练通用步骤>模型训练通用步骤</a></li></ul></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#平方损失>平方损失</a></li></ul></li><li><a href=#降低损失>降低损失</a><ul><li><a href=#梯度下降法>梯度下降法</a></li></ul></li><li><a href=#随机梯度下降法>随机梯度下降法</a></li><li><a href=#泛化能力>泛化能力</a><ul><li><a href=#规则化regularize>规则化（Regularize）</a></li></ul></li><li><a href=#l1-范数>L1-范数</a></li><li><a href=#l2-范数>L2-范数</a></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9 mx-xl-auto"><h1>模型训练与损失和泛化能力</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=d-xl-none><button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#onThisPage aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>On this page</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button>
<button class="btn btn-outline-primary btn-sm doks-toc-toggle collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle On this page navigation">
<span>Book Menu</span>
<span><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></span></button><div class=collapse id=onThisPage><div class="card card-body mt-3 py-1"><div class=page-links><nav id=TableOfContents><ul><li><a href=#训练与损失>训练与损失</a><ul><li><a href=#模型训练通用步骤>模型训练通用步骤</a></li></ul></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#平方损失>平方损失</a></li></ul></li><li><a href=#降低损失>降低损失</a><ul><li><a href=#梯度下降法>梯度下降法</a></li></ul></li><li><a href=#随机梯度下降法>随机梯度下降法</a></li><li><a href=#泛化能力>泛化能力</a><ul><li><a href=#规则化regularize>规则化（Regularize）</a></li></ul></li><li><a href=#l1-范数>L1-范数</a></li><li><a href=#l2-范数>L2-范数</a></li></ul></nav></div></div></div></div><div class="page-links d-none d-xl-block"><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#训练与损失>训练与损失</a><ul><li><a href=#模型训练通用步骤>模型训练通用步骤</a></li></ul></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#平方损失>平方损失</a></li></ul></li><li><a href=#降低损失>降低损失</a><ul><li><a href=#梯度下降法>梯度下降法</a></li></ul></li><li><a href=#随机梯度下降法>随机梯度下降法</a></li><li><a href=#泛化能力>泛化能力</a><ul><li><a href=#规则化regularize>规则化（Regularize）</a></li></ul></li><li><a href=#l1-范数>L1-范数</a></li><li><a href=#l2-范数>L2-范数</a></li></ul></nav></div></nav><p>机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果。</p><h2 id=训练与损失>训练与损失 <a href=#%e8%ae%ad%e7%bb%83%e4%b8%8e%e6%8d%9f%e5%a4%b1 class=anchor aria-hidden=true>#</a></h2><p>简单来说，<strong>训练模型</strong>表示通过有标签样本来学习（确定）所有权重和偏差的取值。</p><h3 id=模型训练通用步骤>模型训练通用步骤 <a href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e9%80%9a%e7%94%a8%e6%ad%a5%e9%aa%a4 class=anchor aria-hidden=true>#</a></h3><p>通常学习一个好的函数，分为以下三步：<br>1、选择一个合适的模型，这通常需要依据实际问题而定，针对不同的问题和任务需要选取恰当的模型，模型就是一组函数的集合。<br>2、判断一个函数的好坏，这需要确定一个衡量标准，也就是<a href=https://www.w3cdoc.com>我们</a>通常说的损失函数（Loss Function），损失函数的确定也需要依据具体问题而定，如回归问题一般采用欧式距离，分类问题一般采用交叉熵代价函数。<br>3、找出“最好”的函数，如何从众多函数中最快的找出“最好”的那一个，这一步是最大的难点，做到又快又准往往不是一件容易的事情。常用的方法有梯度下降算法，最小二乘法等和其他一些技巧（tricks）。学习得到“最好”的函数后，需要在新样本上进行测试，只有在新样本上表现很好，才算是一个“好”的函数。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-43-41.png></p><p>在监督式学习中，机器学习算法通过以检查多个样本并尝试找出可最大限度地减少<strong>损失（cost）<strong>的模型，这一过程称为</strong>经验风险最小化</strong>。</p><p><strong>损失</strong>是对算法预测数据的惩罚。也就是说，<strong>损失</strong>是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-43-50.png></p><p>其中红色箭头表示损失，蓝色线是算法模型预测的值，黄色圈是实际值。</p><p>左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。</p><h2 id=损失函数>损失函数 <a href=#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0 class=anchor aria-hidden=true>#</a></h2><h3 id=平方损失>平方损失 <a href=#%e5%b9%b3%e6%96%b9%e6%8d%9f%e5%a4%b1 class=anchor aria-hidden=true>#</a></h3><p>线性回归模型最常使用的是一种称为平方损失（又称为 L<sub>2</sub> 损失）的损失函数。单个样本的平方损失如下：</p><p>$$
(y - y)^2
$$</p><p><strong>均方误差</strong> (<strong>MSE</strong>) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：</p><div style=text-align:center>$$
 MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2
$$</div><p>其中：(x,y) 指的是样本，其中x 指的是模型进行预测时使用的特征集（例如，温度）。y 指的是样本的标签（例如，知了每分钟的鸣叫次数）。</p><ul><li><p>prediction(x) 指的是权重和偏差与特征集 x 结合的函数。</p></li><li><p>D 指的是包含多个有标签样本（即 (x,y)）的数据集。N 指的是 D 中的样本数量。</p></li></ul><p>虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。</p><h2 id=降低损失>降低损失 <a href=#%e9%99%8d%e4%bd%8e%e6%8d%9f%e5%a4%b1 class=anchor aria-hidden=true>#</a></h2><p>为了让<a href=https://www.w3cdoc.com>我们</a>的模型更加准确，就需要减少损失。下图是机器学习中的一般方法：</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-44-23.png></p><p>通过不断的试错，不断地更新参数，最终可以将损失收敛在要求范围内，那么这个模型就训练完成了。</p><p>回想一下<a href=https://www.w3cdoc.com>我们</a>上节课介绍的温度和知了叫声的模型：</p><p>$$
y = b + w_1x_1
$$</p><p>在第一步计算损失的时候<a href=https://www.w3cdoc.com>我们</a>需要一个初始值。对于线性回归问题，理论证明初始值并不重要。<a href=https://www.w3cdoc.com>我们</a>可以随机选择值，不过<a href=https://www.w3cdoc.com>我们</a>还是选择采用以下这些无关紧要的值：w1=0，b=0；将改初始值带入上面的模型计算，假设第一个样本x1=10：</p><pre><code>y' = 0 + 0(10)
   = 0
</code></pre><p>这里y’表示对第一个数据x1=10的预测结果。而实际值为y，假设<a href=https://www.w3cdoc.com>我们</a>选取平方损失 作为损失函数，那么：</p><p>$$
loss = (y - y)^2
$$</p><p>那么问题来了，假设计算出来的损失值很大，<a href=https://www.w3cdoc.com>我们</a>怎么来更新w和b的值，来使得损失减少呢？ 常用的方法是梯度下降法。</p><h3 id=梯度下降法>梯度下降法 <a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95 class=anchor aria-hidden=true>#</a></h3><p>对于回归问题，假设<a href=https://www.w3cdoc.com>我们</a>有时间和计算资源来计算 w1 的所有可能值的损失，所产生的损失与 w1 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-44-35.png></p><p>凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。<a href=https://www.w3cdoc.com>我们</a>来研究一种更好的机制，这种机制在机器学习领域非常热门，称为<strong>梯度下降法</strong>。</p><p>梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是<a href=https://www.w3cdoc.com>我们</a>选择了一个稍大于 0 的起点，然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，<strong>梯度</strong>是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-44-44.png></p><p>请注意，梯度是一个矢量，因此具有以下两个特征：</p><ul><li>方向</li><li>大小</li></ul><p>梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加。然后，梯度下降法会重复此过程，逐渐接近最低点。</p><p>梯度下降法算法用梯度乘以一个称为<strong>学习速率</strong>（有时也称为<strong>步长</strong>）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。学习速率这个参数<a href=https://www.w3cdoc.com>我们</a>也称为<strong>超参数，超参数</strong>是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间。相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-44-53.png></p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-45-02.png></p><p><a href=https://www.w3cdoc.com>我们</a>可以在下面这个模型中做实验。这个是TensorFlow官方提供的栗子，<a href=https://www.w3cdoc.com>我们</a>将这个深度学习的神经网络模型的隐含层设置成0个，其实就是一个线性模型。<a href=https://www.w3cdoc.com>我们</a>通过调节学习速率，可以看到模型最终的收敛速度。</p><h2 id=随机梯度下降法>随机梯度下降法 <a href=#%e9%9a%8f%e6%9c%ba%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d%e6%b3%95 class=anchor aria-hidden=true>#</a></h2><p>上面<a href=https://www.w3cdoc.com>我们</a>介绍了权重和误差的关系，以及如何修改权重是的误差降低。假设<a href=https://www.w3cdoc.com>我们</a>采用的是均方误差，那么对于样本数据集，误差计算规则是：</p><p>$$
 MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2
$$</p><p>在上一节介绍的梯度下降法中<a href=https://www.w3cdoc.com>我们</a>需要计算使得误差减少、权重变化的梯度，根据数据知识，这个梯度就是均方误差对权重的倒数，</p><p>$$
\begin{align}<br>\nabla{E(\mathrm{w})}&=\frac{\partial}{\partial\mathrm{w}}E(\mathrm{w})\<br>&=\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\<br>\end{align}
$$</p><p>可接下来怎么办呢？<a href=https://www.w3cdoc.com>我们</a>知道和的导数等于导数的和，所以<a href=https://www.w3cdoc.com>我们</a>可以先把求和符号里面的导数求出来，然后再把它们加在一起就行了，也就是</p><p>$$
\begin{align}<br>&\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\<br>=&\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\<br>\end{align}
$$</p><p>现在<a href=https://www.w3cdoc.com>我们</a>可以不管高大上的求和函数了，先专心把里面的导数求出来</p><p>$$
\begin{align}<br>&\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\<br>=&\frac{\partial}{\partial\mathrm{w}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\<br>\end{align}
$$</p><p>根据函数求导传递法则：</p><p>$$
\begin{align}<br>&\frac{\partial{E(\mathrm{w})}}{\partial\mathrm{w}}=\frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}\frac{\partial{\bar{y}}}{\partial\mathrm{w}}\end{align}
$$</p><p><a href=https://www.w3cdoc.com>我们</a>分别计算上式等号右边的两个偏导数</p><p>$$
\begin{align}<br>\frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}=<br>&\frac{\partial}{\partial\bar{y}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\<br>=&-2y^{(i)}+2\bar{y}^{(i)}\\<br>\frac{\partial{\bar{y}}}{\partial\mathrm{w}}=<br>&\frac{\partial}{\partial\mathrm{w}}\mathrm{w}^T\mathrm{x}\<br>=&\mathrm{x}<br>\end{align}
$$</p><p>代入，<a href=https://www.w3cdoc.com>我们</a>求得里面的偏导数是</p><p>$$
\begin{align}<br>&\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\<br>=&2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}<br>\end{align}
$$</p><p>最后代入，求得</p><p>$$
\begin{align}<br>\nabla{E(\mathrm{w})}&=\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\<br>&=\frac{1}{2}\sum_{i=1}^{n}2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}\<br>&=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}<br>\end{align}
$$</p><p>那么根据梯度下降法，权重w的更新规则如下</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2023-01-01-18-16-05.png></p><p>如果<a href=https://www.w3cdoc.com>我们</a>根据上面的公式来训练模型，那么<a href=https://www.w3cdoc.com>我们</a>每次更新的迭代，要遍历训练数据中所有的样本进行计算，<a href=https://www.w3cdoc.com>我们</a>称这种算法叫做<strong>批梯度下降(Batch Gradient Descent)</strong>。如果<a href=https://www.w3cdoc.com>我们</a>的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是 <strong>随机梯度下降法 (SGD)</strong> 算法。在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更新数百万次，效率大大提升。</p><p>由于样本的噪音和随机性，每次更新并不一定按照减少的方向。然而，虽然存在一定随机性，实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。</p><p><strong>小批量随机梯度下降法</strong>（<strong>小批量 SGD</strong>）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。</p><h2 id=泛化能力>泛化能力 <a href=#%e6%b3%9b%e5%8c%96%e8%83%bd%e5%8a%9b class=anchor aria-hidden=true>#</a></h2><p>机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。</p><h3 id=规则化regularize>规则化（Regularize） <a href=#%e8%a7%84%e5%88%99%e5%8c%96regularize class=anchor aria-hidden=true>#</a></h3><p>机器学习中，<a href=https://www.w3cdoc.com>我们</a>一直期望学习到一个泛化能力（generalization）强的函数，只有泛化能力强的模型才能很好地适用于整个样本空间，才能在新的样本点上表现良好。但是训练集通常只是整个样本空间很小的一部分，在训练机器学习模型时，稍有不注意，就可能将训练集中样本的特性当作了全体样本的共性，以偏概全，而造成过拟合（overfitting）问题，如何避免过拟合，是训练机器学习模型时最亟待解决的绊脚石。<br>从问题的根源出发，解决过拟合无非两种途径：</p><ol><li>使训练集能够尽可能全面的描述整个样本空间。因此又存在两个解决方向。<br>①减少特征维数，特征维数减少了，样本空间的大小也随之减少了，现有数据集对样本空间的描述也就提高了。<br>②增加训练样本数量，试图直接提升对样本空间的描述能力。</li><li>加入规则化项。<br>第一种方法的人力成本通常很大，所以在实际中，<a href=https://www.w3cdoc.com>我们</a>通常采用第二种方法提升模型的泛化能力。</li></ol><p>注：规则化在有些文档中也称作正则化，在本文中都采用规则化描述。</p><p>首先回顾一下，在寻找模型最优参数时，<a href=https://www.w3cdoc.com>我们</a>通常对损失函数采用梯度下降（gradient descent）算法</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2023-01-01-18-34-31.png></p><p>$$
\frac{\partial L}{\partial w}=\sum_{i=1}^m2(y^{(i)}-(w^Tx^{(i)}+b))(-x^{(i)})
$$</p><p>$$
\frac{\partial L}{\partial b}=\sum_{i=1}^m2(y^{(i)}-(w^Tx^{(i)}+b))
$$</p><p>通过上述公式，<a href=https://www.w3cdoc.com>我们</a>将一步步走到损失函数的最低点（不考虑局部最小值和鞍点情况），这时的ww和bb就是<a href=https://www.w3cdoc.com>我们</a>要找的最优参数。<br>对于回归问题，<a href=https://www.w3cdoc.com>我们</a>还可以直接采用最小二乘法求得解析解。</p><p>$$
L=\frac{1}{2}(y-X\hat{w})^T(y-X\hat{w})\
\hat{w}=(X^TX)^{-1}X^Ty
$$</p><p>可以看到，当前<a href=https://www.w3cdoc.com>我们</a>的损失函数只考虑最小化训练误差，希望找到的最优函数能够尽可能的拟合训练数据。但是正如<a href=https://www.w3cdoc.com>我们</a>所了解的，训练集不能代表整个样本空间，所以训练误差也不能代表在测试误差，训练误差只是经验风险，<a href=https://www.w3cdoc.com>我们</a>不能过分依赖这个值。当<a href=https://www.w3cdoc.com>我们</a>的函数对训练集拟合特别好，训练误差特别小时，<a href=https://www.w3cdoc.com>我们</a>也就走进了一个极端——过拟合。<br>为了解决这个问题，研究人员提出了规则化（regularize）方法。通过给模型参数附加一些规则，也就是约束，防止模型过分拟合训练数据。规则化通过在原有损失函数的基础上加入规则化项实现。<br>此时，最优化的目标函数如下：</p><p>$$
w^*=arg\min_w\sum_iL(y^{(i)},f(x^{(i)};w))+\lambda\Omega[w]
$$</p><p>其中，第一项对应于模型在训练集上的误差，第二项对应于规则化项。为了使得该目标函数最小，<a href=https://www.w3cdoc.com>我们</a>既需要训练误差最小，也需要规则化项最小，因此需要在二者之间做到权衡。<br>那应该选择怎样的表达式作为规则化项呢？以下引用李航博士《统计学习方法》中的一些描述：</p><blockquote><p>规则化是结构风险最小化策略的实现，是在经验风险最小化上加一个规则化项（regularizer）或罚项（penalty term）。规则化项一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。<br>规则化符合奥卡姆剃刀（Occam’s razor）原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。</p></blockquote><p>所以通常<a href=https://www.w3cdoc.com>我们</a>采用L1-范数和L2-范数作为规则化项。</p><h2 id=l1-范数>L1-范数 <a href=#l1-%e8%8c%83%e6%95%b0 class=anchor aria-hidden=true>#</a></h2><p>向量的L1-范数是向量的元素绝对值之和，即</p><p>$$
||x||_1=\sum_i|x_i|
$$</p><p>当采用L1-范数作为规则化项对参数进行约束时，<a href=https://www.w3cdoc.com>我们</a>的优化问题可以写成以下形式：</p><p>$$
\min_w\frac{1}{2}(y-Xw)^2\ s.t.\quad||w||_1\le C
$$</p><p>采用拉格朗日乘子法可以将约束条件合并到最优化函数中，即</p><p>$$
\min_w\frac{1}{2}(y-Xw)^2+\lambda||w||_1
$$</p><p>其中λλ是于CC一一对应的常数，用来权衡误差项和规则化项，λλ越大，约束越强。二维情况下分别将损失函数的等高线图和L1-范数规则化约束画在同一个坐标轴下，</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-46-03.png></p><p>L1-范数约束对应于平面上一个正方形norm ball。等高线与norm ball首次相交的地方就是最优解。可以看到，L1-ball在和每个坐标轴相交的地方都有“角”出现，大部分时候等高线都会与norm ball在角的地方相交。这样部分参数值被置为0，相当于该参数对应的特征将不再发挥作用，实现了特征选择，增加了模型的可解释性。</p><p>关于L1-范数规则化，可以解释如下：训练出来的参数代表权重，反应了特征的重要程度，比如y=20×1+5×2+3y=20×1+5×2+3中，特征x1明显比x2更加重要，因为x1的变动相较于x2的变动，会给y带来更大的变化。在人工选取的特征中，往往会存在一些冗余特征或者无用特征，L1-范数规则化将这些特征的权重置为0，实现了特征选择，同时也简化了模型。<br>L1-范数在x=0处存在拐点，所以不能直接求得解析解，需要用次梯度方法处理不可导的凸函数。</p><h2 id=l2-范数>L2-范数 <a href=#l2-%e8%8c%83%e6%95%b0 class=anchor aria-hidden=true>#</a></h2><p>除了L1-范数，还有一种广泛使用的规则化范数：L2-范数。向量的L2-范数是向量的模长，即</p><p>$$
||x||_2=\sqrt{\sum_ix_i^2}
$$</p><p>当采用L2-范数作为规则化项对参数进行约束时，<a href=https://www.w3cdoc.com>我们</a>的优化问题可以写成以下形式：</p><p>$$
\min_w\frac{1}{2}(y-Xw)^2\ s.t.\quad||w||_2\le C
$$</p><p>同样可以将约束条件合并到最优化函数中，得到如下函数</p><p>$$
\min_w\frac{1}{2}(y-Xw)^2+\lambda||w||_2
$$</p><p>也将损失函数的等高线图和L2-范数规则化约束画在同一个坐标轴下，</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2022-12-31-17-46-14.png></p><p>L2-范数约束对应于平面上一个圆形norm ball。等高线与norm ball首次相交的地方就是最优解。与L1-范数不同，L2-范数使得每一个w都很小，都接近于0，但不会等于0，L2-范数规则化仍然试图使用每一维特征。对于L2-范数规则化可以解释如下：L2-范数规则化项将参数限制在一个较小的范围，参数越小，曲面越光滑，因而不会出现在很小区间内，弯曲度很大的情况，当x一个较大的变化时，y也只会变化一点点，模型因此更加稳定，也就是更加generalization。<br>加入L2-范数规则化项后，目标函数扩展为如下形式：</p><p><img class="img-fluid lazyload blur-up" src=/images/posts/2023-01-01-18-33-52.png></p><p>$$
\frac{\partial L}{\partial w}=\sum_{i=1}^m2[(y^{(i)}-(w^Tx^{(i)}+b))(-x^{(i)})+\lambda w]
$$</p><p>$$
\frac{\partial L}{\partial b}=\sum_{i=1}^m2[(y^{(i)}-(w^Tx^{(i)}+b))(-1)\lambda w]
$$</p><p>同样，如果采用最小二乘法，正规方程的形式需要相应修改，并且对于样本数目少于特征维数的情况时，矩阵(XTX)(XTX)将不满秩，(XTX)(XTX)也就不可逆，确切地说，此时方程组是不定方程组，将会有无穷多解，已有的数据不足以确定一个解，数学上常加入约束项以使得唯一解成为可能，加入L2-范数规则化项正好对应了这种方法，此时解析解如下：</p><p>$$
w^*=(X^TX+\lambda I)^{-1}X^T
$$</p><p>关于L1-范数和L2-范数规则化的解释是个人的总结之词，可能存在不准确，希望<a href=https://www.w3cdoc.com>大家</a>不惜赐教！</p><h1 id=参考文献>参考文献 <a href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae class=anchor aria-hidden=true>#</a></h1><ul><li>李航《统计学习方法》</li><li><a href=https://blog.csdn.net/zouxy09/article/details/24971995>机器学习中的范数规则化之（一）L0、L1与L2范数</a></li><li><a href=http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/>Sparsity and Some Basics of L1 Regularization</a></li><li>https://blog.csdn.net/hohaizx/article/details/80973738</li><li>https://www.zybuluo.com/hanbingtao/note/433855</li></ul><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/bigdata/framework/%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97%E4%BB%8B%E7%BB%8D/><div class="card my-1"><div class="card-body py-2">&larr; 大数据与流计算概览</div></div></a><a class=ms-auto href=/bigdata/analysis/method/%E7%94%A8%E6%88%B7%E5%88%86%E7%BE%A4%E7%94%BB%E5%83%8F/><div class="card my-1"><div class="card-body py-2">用户分群画像 &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Copyright © 2022-Present 浙ICP备18052292号-3</li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.1117772738b0b01188ff56b000f75758d3ca75fab55d0d7cf813282148e4840455e1fc0a8f1c2951391282de6e64bed66b885160643382a58f67db6d110e9feb.js integrity="sha512-ERd3JziwsBGI/1awAPdXWNPKdfq1XQ18+BMoIUjkhARV4fwKjxwpUTkSgt5uZL7Wa4hRYGQzgqWPZ9ttEQ6f6w==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.35e8488ad0bca08539c53f48a94a3c02f90c10ddc1ecba4a7fd7ec827cef556dec98a33813e3681b33a5750eefd53e65ab606d30febc27e411c293f2290f96a5.js integrity="sha512-NehIitC8oIU5xT9IqUo8AvkMEN3B7LpKf9fsgnzvVW3smKM4E+NoGzOldQ7v1T5lq2BtMP68J+QRwpPyKQ+WpQ==" crossorigin=anonymous defer></script>
<script src=/main.min.5874482df6978e314928bf4ce1c634b6464aa5445dd098e344aa61159e2b10139d82662edd47e835a3ea422c24214134f15eded85f28e55774a70f9c4919b2b3.js integrity="sha512-WHRILfaXjjFJKL9M4cY0tkZKpURd0JjjRKphFZ4rEBOdgmYu3UfoNaPqQiwkIUE08V7e2F8o5Vd0pw+cSRmysw==" crossorigin=anonymous defer></script>
<script src=/index.min.4ae26272486ea46c5bb0bed7a0b434a91b05e8182cfb839a405dd4e647b05ce5d76d401a5103d822d3b1589fc56335cd372b712d97085b8d89aebf244b1b5501.js integrity="sha512-SuJickhupGxbsL7XoLQ0qRsF6Bgs+4OaQF3U5kewXOXXbUAaUQPYItOxWJ/FYzXNNytxLZcIW42Jrr8kSxtVAQ==" crossorigin=anonymous defer></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
<script type="text/x-mathjax-config;executed=true">
  window.MathJax.Hub.Config({
      showProcessingMessages: false, //关闭js加载过程信息
      messageStyle: "none", //不显示信息
      jax: ["input/TeX", "output/HTML-CSS"],
      tex2jax: {
          inlineMath: [["$", "$"], ["\\(", "\\)"]], //行内公式选择符
          displayMath: [["$$", "$$"], ["\\[", "\\]"]], //段内公式选择符
          skipTags: ["script", "noscript", "style", "textarea", "pre", "code", "a"] //避开某些标签
      },
      "HTML-CSS": {
          availableFonts: ["STIX", "TeX"], //可选字体
          showMathMenu: false //关闭右击菜单显示
      }
  });
  //下面第三个参数可以不写，默认对整个html内的latex进行翻译
  window.MathJax.Hub.Queue(["Typeset", MathJax.Hub, document.getElementsByClassName("ck-content")]);
</script></body></html>