<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>计算环境 on</title><link>/bigdata/compute/</link><description>Recent content in 计算环境 on</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Tue, 06 Oct 2020 08:49:15 +0000</lastBuildDate><atom:link href="/bigdata/compute/index.xml" rel="self" type="application/rss+xml"/><item><title>机器学习基础了解</title><link>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3/</guid><description>机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？
我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：
这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为“Google大脑”的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是“百度大脑”的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类-深度学习。
Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。
通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。
机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：
什么是机器学习 # 机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？
传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。
下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。
这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。
我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。
对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。
要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。
事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。
依据数据所做的判断跟机器学习的思想根本上是一致的。
刚才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：
这样的图就是一个最简单的机器学习模型，称之为决策树。
当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。
再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。
如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。
如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。
机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。
通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。
下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。
机器学习的定义 # 从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。
让我们具体看一个例子。
拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？
很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。
我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。
对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。
通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：
房价 = 面积 * a + b
上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。
假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。
在求解过程中透露出了两个信息：
1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。
2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。
通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。</description></item><item><title>机器学习与深度学习介绍</title><link>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</guid><description>说起人工智能（AI，Artificial Intelligence）这个概念，你可能想到一部电影，对的，经典的人工智能电影AI（有兴趣推荐看下）。其实类似的电影还有很多，都是有人工智能，机器智能，机器学习的概念。
要说最早，人工智能这个词其实最早来源于雨果的一本著作中，这是第一次出现这个词，而人工智能真正意义上来说，是在 _1956_年 _Dartmouth_学会上提出的，在学会上提出这个概念后，也就是从 _20_世纪 _50_年代中期到 _70_年代中期，众多学者以及研究人员对其展开了深刻的研究，这也是首次对人工智能这个概念进行了一个深入的研究，当然好景不长，我们当时时代的落后，科技的浅显，相关基础理论研究结果的匮乏，以及硬件与软件的落后使得人们对人工智能的这股热潮慢慢冷却。当时研究的最多的是模式识别，也是人工智能的代名词。
人工智能是一个比较宽泛的概念，现在研究比较火热的是机器学习，我们来看下机器是怎么学习的呢?
三个与“学习”高度相关的流行词汇 # 模式识别（Pattern recognition）、机器学习（machine learning）和深度学习（deep learning）代表三种不同的思想流派。模式识别是最古老的（作为一个术语而言，可以说是很过时的）。机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。而深度学习是非常崭新和有影响力的前沿领域，还有未来的后深度学习时代。我们可以看下图所示的谷歌搜索指数趋势图。
1）机器学习就像是一个真正的冠军一样持续昂首而上；
2）模式识别一开始主要是作为机器学习的代名词；
3）模式识别正在慢慢没落和消亡；
4）深度学习是个崭新的和快速攀升的领域。
模式识别：智能程序的诞生 # Putting a human inside a computer is not Artificial Intelligence
模式识别是70年代和80年代非常流行的一个术语。它强调的是如何让一个计算机程序去做一些看起来很“智能”的事情，例如识别“3”这个数字。而且在融入了很多的智慧和直觉后，人们也的确构建了这样的一个程序。例如，区分“3”和“B”或者“3”和“8”。早在以前，大家也不会去关心你是怎么实现的，只要这个机器不是由人躲在盒子里面伪装的就好（图2）。不过，如果你的算法对图像应用了一些像滤波器、边缘检测和形态学处理等等高大上的技术后，模式识别社区肯定就会对它感兴趣。光学字符识别就是从这个社区诞生的。因此，把模式识别称为70年代，80年代和90年代初的“智能”信号处理是合适的。决策树、启发式和二次判别分析等全部诞生于这个时代。而且，在这个时代，模式识别也成为了计算机科学领域的小伙伴搞的东西，而不是电子工程。从这个时代诞生的模式识别领域最著名的书之一是由Duda &amp;amp; Hart执笔的“模式识别（Pattern Classification）”。对基础的研究者来说，仍然是一本不错的入门教材。不过对于里面的一些词汇就不要太纠结了，因为这本书已经有一定的年代了，词汇会有点过时。
一个字符“3”的图像被划分为16个子块。
自定义规则、自定义决策，以及自定义“智能”程序在这个任务上，曾经都风靡一时（更多信息，可以查看这个 [OCR][1]网页）
**小测试：**计算机视觉领域最著名的会议叫CVPR，这个PR就是模式识别。你能猜出第一届CVPR会议是哪年召开的吗？
机器学习：从样本中学习的智能程序 # 在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。因此，我们搜集大量的人脸和非人脸图像，再选择一个算法，然后冲着咖啡、晒着太阳，等着计算机完成对这些图像的学习。这就是机器学习的思想。“机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。相信我，就算计算机完成学习要耗上一天的时间，也会比你邀请你的研究伙伴来到你家然后专门手工得为这个任务设计一些分类规则要好。
图3 典型的机器学习流程（图来源于 Natalia Konstantinova 博士的博客）。
机器学习成为了计算机科学领域一个重要的研究课题，计算机科学家们开始将这些想法应用到更大范围的问题上，不再限于识别字符、识别猫和狗或者识别图像中的某个目标等等这些问题。研究人员开始将机器学习应用到机器人（强化学习，操控，行动规划，抓取）、基因数据的分析和金融市场的预测中。另外，机器学习与图论的联姻也成就了一个新的课题—图模型。每一个机器人专家都“无奈地”成为了机器学习专家，同时，机器学习也迅速成为了众人渴望的必备技能之一。然而，“机器学习”这个概念对底层算法只字未提。我们已经看到凸优化、核方法、支持向量机和Boosting算法等都有各自辉煌的时期。再加上一些人工设计的特征，那在机器学习领域，我们就有了很多的方法，很多不同的思想流派，然而，对于一个新人来说，对特征和算法的选择依然一头雾水，没有清晰的指导原则。但，值得庆幸的是，这一切即将改变……
**延伸阅读：**要了解更多关于计算机视觉特征的知识，可以看看原作者之前的博客文章：“ [从特征描述子到深度学习：计算机视觉的20年][3]”。
深度学习：一统江湖的架构 # 上面这张图可以很清晰的看出机器学习和深度学习的区别，深度学习对人的依赖更少，更智能。 快进到今天，我们看到的是一个夺人眼球的技术—深度学习。而在深度学习的模型中，受宠爱最多的就是被用在大规模图像识别任务中的卷积神经网络（Convolutional Neural Nets，CNN），简称ConvNets。
深度学习强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数通过从数据中学习获得。然而，深度学习也带来了一些其他需要考虑的问题。因为你面对的是一个高维的模型（即庞大的网络），所以你需要大量的数据（大数据）和强大的运算能力（图形处理器，GPU）才能优化这个模型。卷积被广泛用于深度学习（尤其是计算机视觉应用中），而且它的架构往往都是非浅层的。
如果你要学习Deep Learning，那就得先复习下一些线性代数的基本知识，当然了，也得有编程基础。我强烈推荐Andrej Karpathy的博文：“ [神经网络的黑客指南][5]”。另外，作为学习的开端，可以选择一个不用卷积操作的应用问题，然后自己实现基于CPU的反向传播算法。
对于深度学习，还存在很多没有解决的问题。既没有完整的关于深度学习有效性的理论，也没有任何一本能超越机器学习实战经验的指南或者书。另外，深度学习不是万能的，它有足够的理由能日益流行，但始终无法接管整个世界。不过，只要你不断增加你的机器学习技能，你的饭碗无忧。但也不要对深度框架过于崇拜，不要害怕对这些框架进行裁剪和调整，以得到和你的学习算法能协同工作的软件框架。未来的Linux内核也许会在Caffe（一个非常流行的深度学习框架）上运行，然而，伟大的产品总是需要伟大的愿景、领域的专业知识、市场的开发，和最重要的：人类的创造力。
其他相关术语 # 1）大数据（Big-data）：
大数据是个丰富的概念，例如包含大量数据的存储，数据中隐含信息的挖掘等。对企业经营来说，大数据往往可以给出一些决策的建议。对机器学习算法而言，它与大数据的结合在早几年已经出现。研究人员甚至任何一个日常开发人员都可以接触到云计算、GPU、DevOps和PaaS等等这些服务。
2）人工智能（Artificial Intelligence）：
人工智能应该是一个最老的术语了，同时也是最含糊的。它在过去50年里经历了几度兴衰。当你遇到一个说自己是做人工智能的人，你可以有两种选择：要么摆个嘲笑的表情，要么抽出一张纸，记录下他所说的一切。
结论 # 关于机器学习的讨论在此停留（不要单纯的认为它是深度学习、机器学习或者模式识别中的一个，这三者只是强调的东西有所不同），然而，研究会继续，探索会继续。我们会继续构建更智能的软件，我们的算法也将继续学习，但我们只会开始探索那些能真正一统江湖的框架。</description></item><item><title>机器学习基础数学知识</title><link>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/</guid><description>机器学习速成课程中介绍并应用了以下概念和工具。有关详情，请参阅链接的资源。
高等数学 # 变量、系数和函数 大家上高中都学过变量的概率，比如未知数[latex]x[/latex]是一个变量，系数比如[latex]2x[/latex]，其中数字2就是变量的系数，函数就是表达两个变量关系的等式。
线性方程式，例如: [latex]y = b + w_1x_1 + w_2x_2[/latex] 如上面的线性方程的例子，这是个二元一次方程，包含两个自变量x，一个因变量y，表达了这三个变量之间的关系。
对数和对数方程式，例如: [latex]y = ln1+ e^z 如果 [latex]b^y = x[/latex] 那么 [latex]log_b(x) = y[/latex]，是不是很好理解。[latex]log_e(x) = lnx， ln是一种简写形式。
S 型函数又叫sigmod函数，在神经网络中作为激活函数。 Sigmoid函数又分为Log-Sigmoid函数和Tan-Sigmoid函数。
Log-Sigmoid函数: [latex]\sigma(z) = \frac{1}{1+e^{-z}}[/latex]，图形如下：
Tan-Sigmoid函数: [latex]\sigma(z) = \frac{2}{1+e^{-2z}} - 1[/latex]
线性代数 # 张量和张量阶数 # 现代的数学观点定义：张量是多重线性函数， 输入r个向量，输出1个数，r称作张量的阶数。多重线性是指张量对于每个参数都是线性的,对于单个参数就是： ,其中u,v 是任意向量，c 是任意数。
TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.在TensorFlow系统中，张量的维数来被描述为阶.但是张量的阶和矩阵的阶并不是同一个概念.张量的阶（有时是关于如顺序或度数或者是n维）是张量维数的一个数量描述.比如，下面的张量（使用Python中list定义的）就是2阶.
t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] 你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用’t[i, j, k]’来访问其中的任何元素.
张量是所有深度学习框架中最核心的组件，因为后续的所有运算和优化算法都是基于张量进行的。几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。
阶,数学实例,Python例子</description></item><item><title>机器学习常用术语及含义</title><link>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E5%8F%8A%E5%90%AB%E4%B9%89/</guid><description>算法术语 # 其实在 [人工智能与机器学习与深度学习常用算法][1] 这一部分内容中我们已经介绍了相关的理论和算法的术语。
首先我们要明白机器学习的基本的原理，就是把现实世界当中要研究的对象通过抽象其特征值将其数字化，然后让计算机通过这些已有的数据学习“经验”（学习算法模型参数），从而有了判断的能力，这时如果有了新的输入，计算机就能够根据这些经验来做出判断。
机器学习中常用两种算法模型：回归和分类。
回归模型可预测连续值。例如，回归模型做出的预测可回答如下问题： # 加利福尼亚州一栋房产的价值是多少？ 用户点击此广告的概率是多少？ 分类模型可预测离散值。例如，分类模型做出的预测可回答如下问题： # 某个指定电子邮件是垃圾邮件还是非垃圾邮件？ 这是一张狗、猫还是仓鼠图片？ 基础术语 # 为了方便理解，我们还是举例子来说明。
案例1: 西瓜分类 # 比如我们有四个西瓜，前三个是甜的，第四个不知道甜不甜。
西瓜就是我们的样本（sample）数据。其中三个甜西瓜是有标签样本（labeled sample）。 剩下一个不知道甜不甜的西瓜是无标签样本。
我们使用有标签样本来**训练模型。**首先我们需要构建数据模型，需要对西瓜进行特征提取，我们挑选西瓜的属性 比如色泽表示为x1、根蒂表示为x2、敲声表示为x3，作为我们选取西瓜样本的特征。
那么西瓜这个样本经过特征抽取就可以表示成:
{x_1, x_2 ... x_N} 我们称为特征列，也叫特征向量。特征提取的过程也叫做特征工程 (feature engineering)。
这三个西瓜的标签样本的数据组合在一起就构成了我们的数据集（dataset）。
我们在特征抽取的时候需要将特征按照一定的标准映射成一个数值，这是一个抽象提取数据指标的过程，很显然对于西瓜的三个特征都是离散的数据。我们根据西瓜的这些特征，预测西瓜是否成熟，预测的结果也是离散的数据，适合用分类模型来解决。如果我们预测西瓜的成熟度，比如0.9、0.7这样，预测的结果就是连续的值，适合用回归模型来解决。
案例2：温度与知了叫声关系 # 夏天天气越热，知了叫声越大。我们可以统计下夏天温度和每分钟知了叫声的数据如下：
我们可以看到随着温度增加知了叫声越密集，我们可以用一个直线来近似表达他们之间的关系。根据我们学过的数学知识，可以用一个线性方程来表示：
y = mx + b 其中：
y 指的是温度（以摄氏度表示），即我们试图预测的值。 m 指的是直线的斜率。 x 指的是每分钟的鸣叫声次数，即输入特征的值。 b 指的是 y 轴截距。 我们在机器学习中通常用另外一种方式来表示：
y = b + w1 * x1 如果x0=1，那么还可以表示成：
y' = w0 * x0 + w1 * x1 其中：</description></item><item><title>机器学习与深度学习常用算法</title><link>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/</guid><description>要说最早，人工智能这个词其实最早来源于雨果的一本著作中，这是第一次出现这个词，而人工智能真正意义上来说，是在 _1956_年 _Dartmouth_学会上提出的，在学会上提出这个概念后，也就是从 _20_世纪 _50_年代中期到 _70_年代中期，众多学者以及研究人员对其展开了深刻的研究，这也是首次对人工智能这个概念进行了一个深入的研究，当然好景不长，我们当时时代的落后，科技的浅显，相关基础理论研究结果的匮乏，以及硬件与软件的落后使得人们对人工智能的这股热潮慢慢冷却。当时研究的最多的是模式识别，也是人工智能的代名词。
模式识别算法 # 模式识别是上世纪70-80年代的东西了，理论内容也比较纯粹了。模式识别主要可以做三件事情：分类、聚类和预测。其中聚类和分类是根据要解决的问题的类型是监督型数据还是非监督型数据来划分，预测主要是指根据设计的分类或者聚类算法来对下一步的结果做预测。
Classification (分类) # 分类是针对 Supervised Learning (监督学习)的处理方法。因为只有是有监督的（知道能分成几类），才可以用来做分类。
利用分类技术可以从数据集中提取描述数据类的一个函数或模型（也常称为分类器classifier），并把数据集中的每个对象归结到某个已知的对象类中。从机器学习的观点，分类技术是监督学习，即每个训练样本的数据对象已经有类标识，通过学习可以形成表达数据对象与类标识间对应的知识。所谓分类，简单来说，就是根据数据的特征或属性，划分到已有的类别中。
分类作为一种监督学习方法，要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应。但是很多时候上述条件得不到满足，尤其是在处理海量数据的时候，如果通过预处理使得数据满足分类算法的要求，则代价非常大，这时候可以考虑使用聚类算法。
常用的分类算法包括 # 决策树分类法 基于规则的分类器 朴素的贝叶斯分类算法(native Bayesian classifier) 基于支持向量机(SVM)的分类器 神经网络法 k-最近邻法(k-nearest neighbor，kNN) 模糊分类法 举例 # 关于某个关键词的负面新闻检索。 已知类别，正面新闻还是负面新闻。所以是有监督。
全班同学高考主要都去了重点大学。 已知类别是重点大学还是普通大学，或者是211或者985, 分类是明确的，属于有监督。
Clustering(聚类) # 聚类是针对Unsupervised Learning (无监督学习)的，因为无监督的数据集合（不知道能分成几类），适合做聚类分析。
简单地说就是把相似的东西分到一组，聚类的时候，我们并不关心某一类是什么，我们的目标只是把相似的东西聚到一起。聚类分析就是将数据划分成有意义或有用的组（簇）。因此，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此 clustering 通常并不需要使用训练数据进行学习，即unsupervised learning (无监督学习)。聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的，而不同组中的对象是不同的。
什么是一个好的聚类方法? # 一个好的聚类方法要能产生高质量的聚类结果——簇，这些簇要具备以下两个特点： 高的簇内相似性、低的簇间相似性
聚类结果的好坏取决于该聚类方法采用的相似性评估方法以及该方法的具体实现； 聚类方法的好坏还取决于该方法是否能发现某些还是所有的隐含模式；
不同的聚类类型 # 划分聚类（Partitional Clustering）：划分聚类简单地将数据对象集划分成不重叠的子集，使得每个数据对象恰在一个子集。 也正是根据所谓的“启发式算法”，形成了k-means算法及其变体包括k-medoids、k-modes、k-medians、kernel k-means等算法。 层次聚类（Hierarchical Clustering）：层次聚类是嵌套簇的集族，组织成一棵树。k-means算法。改进的算法有BIRCH（Balanced Iterative Reducing and Clustering Using Hierarchies）主要是在数据体量很大的时候使用，而且数据类型是numerical。 互斥聚类（Exclusive Clustering）：每个对象都指派到单个簇。 重叠的（Overlapping）或非互斥的（Non-exclusive）聚类：聚类用来反映一个对象.同时属于多个组（类）这一事实。例如：在大学里，一个人可能既是学生，又是雇员 模糊聚类（Fuzzy Clustering）：每个对象以一个0（绝对不属于）和1（绝对属于）之间的隶属权值属于每个簇。换言之，簇被视为模糊集。 FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。 完全聚类（Complete Clustering）：完全聚类将每个对象指派到一个簇。 部分聚类（Partial Clustering)：部分聚类中数据集某些对象可能不属于明确定义的组。如：一些对象可能是离群点、噪声。 不同的簇类型 # 明显分离的（Well-Separated）：每个点到同簇中任一点的距离比到不同簇中所有点的距离更近。 基于原型的：每个对象到定义该簇的原型的距离比到其他簇的原型的距离更近。对于具有连续属性的数据，簇的原型通常是质心，即簇中所有点的平均值。当质心没有意义时，原型通常是中心点，即簇中最有代表性的点。 基于中心的（Center-Based）的簇：每个点到其簇中心的距离比到任何其他簇中心的距离更近。 ∙∙ 基于图的：如果数据用图表示，其中节点是对象，而边代表对象之间的联系。簇可以定义为连通分支（Connected Component）：互相连通但不与组外对象连通的对象组。 基于近邻的（Contiguity-Based）簇：其中两个对象是相连的，仅当它们的距离在指定的范围内。这意味着，每个对象到该簇某个对象的距离比到不同簇中任意点的距离更近。 ∙∙ 基于密度的（Density-Based）：簇是对象的稠密区域，被低密度的区域环绕。 ∙ 基于网络的方法（Grid-based methods）：这类方法的原理就是将数据空间划分为网格单元，将数据对象集映射到网格单元中，并计算每个单元的密度。根据预设的阈值判断每个网格单元是否为高密度单元，由邻近的稠密单元组形成”类“。STING（STatistical INformation Grid）算法、WAVE-CLUSTER算法和CLIQUE（CLustering In QUEst）是该类方法中的代表性算法。 (共同性质的)概念簇（Conceptual Clusters）：可以把簇定义为有某种共同性质的对象的集合。此情况下，聚类算法都需要非常具体的簇概念来成功检测这些簇，发现这些簇的过程称作概念聚类。DBSCAN（Density-Based Spatial Clustering of Applications with Noise）就是其中的典型.</description></item><item><title>模型训练与损失和泛化能力</title><link>/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%B8%8E%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B/</guid><description>机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果。
训练与损失 # 简单来说，训练模型表示通过有标签样本来学习（确定）所有权重和偏差的取值。
模型训练通用步骤 # 通常学习一个好的函数，分为以下三步：
1、选择一个合适的模型，这通常需要依据实际问题而定，针对不同的问题和任务需要选取恰当的模型，模型就是一组函数的集合。
2、判断一个函数的好坏，这需要确定一个衡量标准，也就是我们通常说的损失函数（Loss Function），损失函数的确定也需要依据具体问题而定，如回归问题一般采用欧式距离，分类问题一般采用交叉熵代价函数。
3、找出“最好”的函数，如何从众多函数中最快的找出“最好”的那一个，这一步是最大的难点，做到又快又准往往不是一件容易的事情。常用的方法有梯度下降算法，最小二乘法等和其他一些技巧（tricks）。学习得到“最好”的函数后，需要在新样本上进行测试，只有在新样本上表现很好，才算是一个“好”的函数。
在监督式学习中，机器学习算法通过以检查多个样本并尝试找出可最大限度地减少损失（cost）的模型，这一过程称为经验风险最小化。
损失是对算法预测数据的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。
其中红色箭头表示损失，蓝色线是算法模型预测的值，黄色圈是实际值。
左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。
损失函数 # 平方损失 # 线性回归模型最常使用的是一种称为平方损失（又称为 L2 损失）的损失函数。单个样本的平方损失如下：
$$ (y - y)^2 $$
均方误差 (MSE) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：
$$ MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2 $$ 其中：(x,y) 指的是样本，其中x 指的是模型进行预测时使用的特征集（例如，温度）。y 指的是样本的标签（例如，知了每分钟的鸣叫次数）。
prediction(x) 指的是权重和偏差与特征集 x 结合的函数。
D 指的是包含多个有标签样本（即 (x,y)）的数据集。N 指的是 D 中的样本数量。
虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。
降低损失 # 为了让我们的模型更加准确，就需要减少损失。下图是机器学习中的一般方法：
通过不断的试错，不断地更新参数，最终可以将损失收敛在要求范围内，那么这个模型就训练完成了。
回想一下我们上节课介绍的温度和知了叫声的模型：
$$ y = b + w_1x_1 $$</description></item><item><title>搭建本地python和TensorFlow环境</title><link>/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0python%E5%92%8Ctensorflow%E7%8E%AF%E5%A2%83/</guid><description>人工智能的时代已经来了，再不学就晚了。说起来有点惭愧，我研究生主修方向是模式识别与人工智能，【捂脸哭】！
好汉不怕远征难，万水千山只等闲。
搭建环境 # 安装python # 如果是MAC系统，那么默认安装了python 2.7.10，你就不要动了。如果是windows，下个python安装包，装一下。
PS：MAC系统自带的python 2.7.10是够用的，不能删除，可以自己装其他版本。不过暂时用不到。
安装pip # pip官网下一个最新的pip，
$ tar zxvf pip-7.1.2.tar.gz $ cd pip-7.1.2 $ sudo python setup.py install # 验证是否安装成功 $ pip freeze PS: 安装需要root权限，所以需要sudo
pip提速 # pip install -i https://pypi.tuna.tsinghua.edu.cn/simple xxxx 安装virtualenv # virtualenv是python的沙箱工具.我们毕竟是在自己机器上做实验,为了不来回修改各种环境变量,我们一般还是弄个沙箱完比较好.测试完直接删除就行,不用再去改各种配置文件.
$pip install --upgrade virtualenv # 安装好后创建一个工作目录,我直接在home里创建了个文件夹. $virtualenv --system-site-packages ~/tensorflow # 进入目录激活沙箱. $ cd ~/tensorflow $ source bin/activate (tensorflow) $ 一般情况是，我们希望创建一个独立的Python运行环境，命名为venv：
Mac:myproject michael$ virtualenv --no-site-packages venv Using base prefix '/usr/local/.</description></item><item><title>在线jupyter环境</title><link>/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/%E5%9C%A8%E7%BA%BFjupyter%E7%8E%AF%E5%A2%83/</guid><description>juputer，想起来丘比特。哈哈
丘比特（拉丁语：Cupid）罗马神话中的小爱神，相当于希腊神话中身为爱与美神阿佛洛狄忒之子的厄洛斯，他的形象大多是即将步入青年的美少年（或一个手拿弓箭的调皮的小男孩），尽管有时他被蒙着眼睛，但没有任何人或神，包括朱庇特在内，能逃避他的恶作剧。他的金箭射入人心会促进爱情走向婚姻，他的铅箭射入人心会使相爱的人产生憎恶，以分手而告终。
安装 # 前面课程已经介绍了，看这里： 本地安装jupyter
在线jupyter # 我踏遍千山万水，找了很多的在线jupyter，最好用的还是这两个：
Colaboratory 如果能翻墙建议使用google的 Colaboratory，可以开启GPU加速，也可以链接本地的jupyter服务端口。和github是通的，可以把写的例子保存到git仓库中。可以看下我的这个示例： Colaboratory使用 。
百度AI studio 不能翻墙的就推荐百度的AI studio，用起来还是很不错的。我这有个 示例项目</description></item><item><title>python-编程基础知识</title><link>/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/bigdata/compute/python-%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid><description>Python 编程 # 基础 Python # Python 教程中介绍了以下 Python 基础知识：
定义和调用函数：使用位置和关键字参数 字典、列表、集合（创建、访问和迭代） for 循环：包含多个迭代器变量的 for 循环（例如 for a, b in [(1,2), (3,4)]） if/else 条件块和条件表达式 字符串格式（例如 '%.2f' % 3.14） 变量、赋值、基本数据类型（int、float、bool、str） pass 语句 中级 Python # Python 教程还介绍了以下更高级的 Python 功能：
列表推导式 Lambda 函数 lambda介绍 第三方 Python 库 # 机器学习速成课程代码示例使用了第三方库提供的以下功能。无需提前熟悉这些库；您可以在需要时查询相关内容。
Matplotlib（适合数据可视化） # pyplot 模块 cm 模块 gridspec 模块 Seaborn（适合热图） # heatmap 函数 Pandas（适合数据处理） # DataFrame 类 NumPy（适合低阶数学运算） # linspace 函数 random 函数 array 函数 arange 函数 scikit-learn（适合评估指标） # metrics 模块 Bash 终端/云端控制台 # 要在本地计算机上或云端控制台中运行编程练习，您应该能熟练使用命令行：</description></item></channel></rss>