---
title: 机器学习与深度学习常用算法
weight: 4

---
要说最早，人工智能这个词其实最早来源于雨果的一本著作中，这是第一次出现这个词，而人工智能真正意义上来说，是在 _1956_年 _Dartmouth_学会上提出的，在学会上提出这个概念后，也就是从 _20_世纪 _50_年代中期到 _70_年代中期，众多学者以及研究人员对其展开了深刻的研究，这也是首次对人工智能这个概念进行了一个深入的研究，当然好景不长，[我们](https://www.w3cdoc.com)当时时代的落后，科技的浅显，相关基础理论研究结果的匮乏，以及硬件与软件的落后使得人们对人工智能的这股热潮慢慢冷却。当时研究的最多的是模式识别，也是人工智能的代名词。

# 模式识别算法

模式识别是上世纪70-80年代的东西了，理论内容也比较纯粹了。模式识别主要可以做三件事情：分类、聚类和预测。其中聚类和分类是根据要解决的问题的类型是监督型数据还是非监督型数据来划分，预测主要是指根据设计的分类或者聚类算法来对下一步的结果做预测。  

![](/images/posts/2022-12-02-21-43-17.png)

## Classification (分类)

![](/images/posts/2022-12-02-21-43-28.png)

分类是针对 Supervised Learning (监督学习)的处理方法。因为只有是有监督的（知道能分成几类），才可以用来做分类。

利用分类技术可以从数据集中提取描述数据类的一个函数或模型（也常称为分类器classifier），并把数据集中的每个对象归结到某个已知的对象类中。从机器学习的观点，分类技术是监督学习，即每个训练样本的数据对象已经有类标识，通过学习可以形成表达数据对象与类标识间对应的知识。所谓分类，简单来说，就是根据数据的特征或属性，划分到已有的类别中。

分类作为一种监督学习方法，要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应。但是很多时候上述条件得不到满足，尤其是在处理海量数据的时候，如果通过预处理使得数据满足分类算法的要求，则代价非常大，这时候可以考虑使用聚类算法。

### 常用的分类算法包括

* 决策树分类法
* 基于规则的分类器
* 朴素的贝叶斯分类算法(native Bayesian classifier)
* 基于支持向量机(SVM)的分类器
* 神经网络法
* k-最近邻法(k-nearest neighbor，kNN)
* 模糊分类法

### 举例

  1. 关于某个关键词的负面新闻检索。

已知类别，正面新闻还是负面新闻。所以是有监督。

  1. 全班同学高考主要都去了重点大学。

已知类别是重点大学还是普通大学，或者是211或者985, 分类是明确的，属于有监督。

## Clustering(聚类)

![](/images/posts/2022-12-02-21-43-43.png)

聚类是针对Unsupervised Learning (无监督学习)的，因为无监督的数据集合（不知道能分成几类），适合做聚类分析。

简单地说就是把相似的东西分到一组，聚类的时候，[我们](https://www.w3cdoc.com)并不关心某一类是什么，[我们](https://www.w3cdoc.com)的目标只是把相似的东西聚到一起。聚类分析就是将数据划分成有意义或有用的组（簇）。因此，一个聚类算法通常只需要知道如何计算相似度就可以开始工作了，因此 clustering 通常并不需要使用训练数据进行学习，即unsupervised learning (无监督学习)。聚类分析仅根据在数据中发现的描述对象及其关系的信息，将数据对象分组。其目标是，组内的对象相互之间是相似的，而不同组中的对象是不同的。

### 什么是一个好的聚类方法?

一个好的聚类方法要能产生高质量的聚类结果——簇，这些簇要具备以下两个特点： 高的簇内相似性、低的簇间相似性

聚类结果的好坏取决于该聚类方法采用的相似性评估方法以及该方法的具体实现； 聚类方法的好坏还取决于该方法是否能发现某些还是所有的隐含模式；

### 不同的聚类类型

* 划分聚类（Partitional Clustering）：划分聚类简单地将数据对象集划分成不重叠的子集，使得每个数据对象恰在一个子集。 也正是根据所谓的“启发式算法”，形成了k-means算法及其变体包括k-medoids、k-modes、k-medians、kernel k-means等算法。
* 层次聚类（Hierarchical Clustering）：层次聚类是嵌套簇的集族，组织成一棵树。k-means算法。改进的算法有BIRCH（Balanced Iterative Reducing and Clustering Using Hierarchies）主要是在数据体量很大的时候使用，而且数据类型是numerical。
* 互斥聚类（Exclusive Clustering）：每个对象都指派到单个簇。
* 重叠的（Overlapping）或非互斥的（Non-exclusive）聚类：聚类用来反映一个对象.同时属于多个组（类）这一事实。例如：在大学里，一个人可能既是学生，又是雇员
* 模糊聚类（Fuzzy Clustering）：每个对象以一个0（绝对不属于）和1（绝对属于）之间的隶属权值属于每个簇。换言之，簇被视为模糊集。 FCM算法是一种以隶属度来确定每个数据点属于某个聚类程度的算法。该聚类算法是传统硬聚类算法的一种改进。
* 完全聚类（Complete Clustering）：完全聚类将每个对象指派到一个簇。
* 部分聚类（Partial Clustering)：部分聚类中数据集某些对象可能不属于明确定义的组。如：一些对象可能是离群点、噪声。

### 不同的簇类型

* 明显分离的（Well-Separated）：每个点到同簇中任一点的距离比到不同簇中所有点的距离更近。
* 基于原型的：每个对象到定义该簇的原型的距离比到其他簇的原型的距离更近。对于具有连续属性的数据，簇的原型通常是质心，即簇中所有点的平均值。当质心没有意义时，原型通常是中心点，即簇中最有代表性的点。
* 基于中心的（Center-Based）的簇：每个点到其簇中心的距离比到任何其他簇中心的距离更近。 ∙∙
* 基于图的：如果数据用图表示，其中节点是对象，而边代表对象之间的联系。簇可以定义为连通分支（Connected Component）：互相连通但不与组外对象连通的对象组。
* 基于近邻的（Contiguity-Based）簇：其中两个对象是相连的，仅当它们的距离在指定的范围内。这意味着，每个对象到该簇某个对象的距离比到不同簇中任意点的距离更近。 ∙∙
* 基于密度的（Density-Based）：簇是对象的稠密区域，被低密度的区域环绕。 ∙
* 基于网络的方法（Grid-based methods）：这类方法的原理就是将数据空间划分为网格单元，将数据对象集映射到网格单元中，并计算每个单元的密度。根据预设的阈值判断每个网格单元是否为高密度单元，由邻近的稠密单元组形成”类“。STING（STatistical INformation Grid）算法、WAVE-CLUSTER算法和CLIQUE（CLustering In QUEst）是该类方法中的代表性算法。
* (共同性质的)概念簇（Conceptual Clusters）：可以把簇定义为有某种共同性质的对象的集合。此情况下，聚类算法都需要非常具体的簇概念来成功检测这些簇，发现这些簇的过程称作概念聚类。DBSCAN（Density-Based Spatial Clustering of Applications with Noise）就是其中的典型.

### 举例

* 自动新闻分组
* 自动邮件归类



# 机器学习算法

机器学习的算法很多。很多时候困惑人们都是，很多算法是一类算法，而有些算法又是从其他算法中延伸出来的。这里，[我们](https://www.w3cdoc.com)从两个方面来给[大家](https://www.w3cdoc.com)介绍：

* 学习的方式
* 算法的分类

先看下这个机器学习图谱：

![](/images/posts/2022-12-02-21-45-07.png)

## 4大主要学习方式

* 监督式学习

在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，如对防垃圾邮件系统中“垃圾邮件”“非垃圾邮件”，对手写数字识别中的“1“，”2“，”3“，”4“等。在建立预测模型的时候，监督式学习建立一个学习过程，将预测结果与“训练数据”的实际结果进行比较，不断的调整预测模型，直到模型的预测结果达到一个预期的准确率。监督式学习的常见应用场景如分类问题和回归问题。常见算法有逻辑回归（Logistic Regression）和反向传递神经网络（Back Propagation Neural Network）。

![](/images/posts/2022-12-02-21-45-17.png)

* 半监督式学习

在此学习方式下，输入数据部分被标识，部分没有被标识，这种学习模型可以用来进行预测，但是模型首先需要学习数据的内在结构以便合理的组织数据来进行预测。应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，这些算法首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM.）等。

![](/images/posts/2022-12-02-21-45-28.png)

* 非监督式学习

在非监督式学习中，数据并不被特别标识，学习模型是为了推断出数据的一些内在结构。常见的应用场景包括关联规则的学习以及聚类等。常见算法包括Apriori算法以及k-Means算法。

![](/images/posts/2022-12-02-21-45-42.png)

* 强化学习

在这种学习模式下，输入数据作为对模型的反馈，不像监督模型那样，输入数据仅仅是作为一个检查模型对错的方式，在强化学习下，输入数据直接反馈到模型，模型必须对此立刻作出调整。常见的应用场景包括动态系统以及机器人控制等。常见算法包括Q-Learning以及时间差学习（Temporal difference learning）。

![](/images/posts/2022-12-02-21-45-51.png)

## 常用算法

根据算法的功能和形式的类似性，[我们](https://www.w3cdoc.com)可以把算法分类，比如说基于树的算法，基于神经网络的算法等等。当然，机器学习的范围非常庞大，有些算法很难明确归类到某一类。

![](/images/posts/2022-12-02-21-46-03.png)

### 举例

  1. 根据卫星云图，预测明天下雨的概率。

很多例子，这里就不一一列举了。因为机器学习主要是强调学习的方法和过程，然后是使用的算法模型。



# 深度学习

深度学习强调的是你使用的模型（例如深度卷积多层神经网络），模型中的参数通过从数据中学习获得。然而，深度学习也带来了一些其他需要考虑的问题。因为你面对的是一个高维的模型（即庞大的网络），所以你需要大量的数据（大数据）和强大的运算能力（图形处理器，GPU）才能优化这个模型。卷积被广泛用于深度学习（尤其是计算机视觉应用中），而且它的架构往往都是非浅层的。

## 常用算法

- 卷积网络（Convolutional Network）
- 循环神经网络（LSTM）（Recurrent Neural Network (LSTM)）
- 受限玻尔兹曼机（Restricted Boltzmann Machine）
- 深度信念网络（Deep Belief Network）
- 作为RBM堆叠的深度自编码器（Deep Autoencoder as stack of RBMs）
- 去噪自编码器（Denoising Autoencoder）
- 堆叠的去噪自编码器（Stacked Denoising Autoencoder）
- 作为去噪自编码器堆叠的深度自编码器（Deep Autoencoder as stack of Denoising Autoencoders）
- 多层感知器（MultiLayer Perceptron）
- Logistic 回归

# 参考

* https://blog.csdn.net/abc200941410128/article/details/78541273
* https://cloud.tencent.com/developer/article/1029070
* https://www.tinymind.cn/articles/632?from=articles_commend
* https://blog.csdn.net/abc200941410128/article/details/79269386
* https://www.jiqizhixin.com/articles/2016-10-11-4