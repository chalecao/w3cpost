---
title: 机器学习基础数学知识
weight: 3

---

机器学习速成课程中介绍并应用了以下概念和工具。有关详情，请参阅链接的资源。

## 高等数学

* 变量、系数和函数

[大家](https://www.w3cdoc.com)上高中都学过变量的概率，比如未知数[latex]x[/latex]是一个变量，系数比如[latex]2x[/latex]，其中数字2就是变量的系数，函数就是表达两个变量关系的等式。

* 线性方程式，例如: [latex]y = b + w\_1x\_1 + w\_2x\_2[/latex]

如上面的线性方程的例子，这是个二元一次方程，包含两个自变量x，一个因变量y，表达了这三个变量之间的关系。

* 对数和对数方程式，例如: [latex]y = ln[1+ e^z](/latex)

如果 [latex]b^y = x[/latex] 那么 [latex]log\_b(x) = y[/latex]，是不是很好理解。[latex]log\_e(x) = ln[x](/latex)， ln是一种简写形式。

* S 型函数又叫sigmod函数，在神经网络中作为激活函数。

Sigmoid函数又分为Log-Sigmoid函数和Tan-Sigmoid函数。

Log-Sigmoid函数: [latex]\sigma(z) = \frac{1}{1+e^{-z}}[/latex]，图形如下：

![](/images/posts/img_5bf6bd51e6f81.webp)

Tan-Sigmoid函数: [latex]\sigma(z) = \frac{2}{1+e^{-2z}} - 1[/latex]

## 线性代数

### 张量和张量阶数

现代的数学观点定义：**张量是多重线性函数**，![](/images/posts/2022-12-02-20-32-48.png)
输入r个向量，输出1个数，r称作张量的阶数。多重线性是指张量对于每个参数都是线性的,对于单个参数就是：
![](/images/posts/2022-12-02-20-32-23.png)

,其中u,v 是任意向量，c 是任意数。

TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.在TensorFlow系统中，张量的维数来被描述为阶.但是张量的阶和矩阵的阶并不是同一个概念.张量的阶（有时是关于如顺序或度数或者是n维）是张量维数的一个数量描述.比如，下面的张量（使用Python中list定义的）就是2阶.
```
t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
```

你可以认为一个二阶张量就是[我们](https://www.w3cdoc.com)平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用&#8217;t[i, j, k]&#8217;来访问其中的任何元素.

张量是所有深度学习框架中最核心的组件，因为后续的所有运算和优化算法都是基于张量进行的。几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，[我们](https://www.w3cdoc.com)可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

阶,数学实例,Python例子  
0,纯量 (只有大小), s = 483  
1,向量(大小和方向), v = [1.1\, 2.2\, 3.3]  
2,矩阵(数据表), m = [[1\, 2\, 3]\, [4\, 5\, 6]\, [7\, 8\, 9]]  
3,3阶张量 (数据立体),t = [[[2]\, [4]\, [6]]\, [[8]\, [10]\, [12]]\, [[14]\, [16]\, [18]]]  
n,n阶 (自己想想看)

举例来说，[我们](https://www.w3cdoc.com)可以将任意一张RGB彩色图片表示成一个三阶张量（三个维度分别是图片的高度、宽度和色彩数据）。如下图所示是一张普通的水果图片，按照RGB三原色表示，其可以拆分为三张红色、绿色和蓝色的灰度图片，如果将这种表示方法用张量的形式写出来，就是图中最下方的那张表格。

![](/images/posts/2022-12-02-20-34-03.png)
![](/images/posts/2022-12-02-20-34-21.png)

图中只显示了前5行、320列的数据，每个方格代表一个像素点，其中的数据[1.0, 1.0, 1.0]即为颜色。假设用[1.0, 0, 0]表示红色，[0, 1.0, 0]表示绿色，[0, 0, 1.0]表示蓝色，那么如图所示，前面5行的数据则全是白色。

将这一定义进行扩展，[我们](https://www.w3cdoc.com)也可以用四阶张量表示一个包含多张图片的数据集，其中的四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

### 矩阵乘法

![](/images/posts/2022-12-02-20-34-35.png)

![](/images/posts/2022-12-02-20-36-04.png)

第一个图给出来常用的矩阵乘积因子对数据做转换。第二个图从行向量和列向量理解矩阵乘法。

## 三角学

* [Tanh][9]（作为[激活函数][10]进行讲解，无需提前掌握相关知识）

<a href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a> 是双曲正切函数，是三角学中普遍使用的 <a href="https://reference.wolfram.com/language/ref/Tan.html">Tan</a> 圆函数的双曲类比. <a href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a>[α] 定义为对应的双曲正弦和双曲余弦函数的比值：

![](/images/posts/2022-12-02-20-36-55.png)

![](/images/posts/2022-12-02-20-37-05.png)

其中 e 是自然对数 <a href="https://reference.wolfram.com/language/ref/Log.html">Log</a> 的底数。常用的激活函数图形如下：

![](/images/posts/2022-12-02-20-37-15.png)

上面的图形不是很清楚，[我们](https://www.w3cdoc.com)可以自己用python绘制一下：

```
import matplotlib.pyplot as plt
import numpy as np
import math

def sigmod(x):
    return 1.0/(1.0+np.exp(-x))

def tanh(x):
    y = np.tanh(x)
    return y

def relu(x):
    y = x.copy()
    y[y<0]=0
    return y

x = np.arange(-50.0,50.0,0.1)
y_relu = relu(x)
y_sigmod = sigmod(x)
y_tanh = tanh(x)

plt.plot(x,y_relu,c='r',label="Relu",linestyle='--')
plt.plot(x,y_sigmod,c='g',label="Sigmod",linestyle='-.')
plt.plot(x,y_tanh,c='b',label="Tanh")
plt.ylim([-1,4])
plt.xlim([-4,4])
plt.legend(loc=2)
plt.savefig('sig_tan_relu.png')
plt.show()
```

![](/images/posts/2022-12-02-20-37-36.png)
## 统计信息

* [均值、中间值、离群值](https://www.khanacademy.org/math/probability/data-distributions-a1/summarizing-center-distributions/v/mean-median-and-mode)和[标准偏差](https://wikipedia.org/wiki/Standard_deviation)

![](/images/posts/2022-12-02-20-37-46.png)

![](/images/posts/2022-12-02-20-37-55.png)

* 能够读懂[直方图](https://wikipedia.org/wiki/Histogram)

## 微积分（可选，适合高级主题）

* [导数](https://wikipedia.org/wiki/Derivative)概念（您不必真正计算导数）

![](/images/posts/2022-12-02-20-38-04.png)

* [梯度](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)或斜率
* [偏导数](https://wikipedia.org/wiki/Partial_derivative)（与梯度紧密相关）
![](/images/posts/2022-12-02-20-38-11.png)

* [链式法则](https://wikipedia.org/wiki/Chain_rule)（带您全面了解用于训练神经网络的[反向传播算法](https://developers.google.com/machine-learning/crash-course/backprop-scroll/?hl=zh-cn)）
![](/images/posts/2022-12-02-20-38-35.png)

![](/images/posts/2022-12-02-20-38-43.png)

## 参考资料

* https://www.zhihu.com/question/20695804/answer/64920043
* https://blog.csdn.net/pandamax/article/details/63684633
* https://www.jianshu.com/p/b2c43c7d1b09
