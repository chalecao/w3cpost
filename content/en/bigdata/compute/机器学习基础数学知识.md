---
title: 机器学习基础数学知识


date: 2018-11-22T01:34:53+00:00
url: /aistack/2826.html
featured_image: https://haomou.oss-cn-beijing.aliyuncs.com/upload/;https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png
fifu_image_url:
  - https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png
fifu_image_alt:
  - 机器学习基础数学知识
views:
  - 1503
like:
  - 1
onesignal_meta_box_present:
  - 1


---
<p id="OpdXeuN">
  <img loading="lazy" class="alignnone wp-image-2832 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png?x-oss-process=image/format,webp" alt="" width="514" height="289" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png?x-oss-process=image/format,webp 800w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_169/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf60809dcb95.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_432/format,webp 768w" sizes="(max-width: 514px) 100vw, 514px" />
</p>

机器学习速成课程中介绍并应用了以下概念和工具。有关详情，请参阅链接的资源。

## 高等数学

  * [变量][1]、[系数][2]和[函数][3]

大家上高中都学过变量的概率，比如未知数[latex]x[/latex]是一个变量，系数比如[latex]2x[/latex]，其中数字2就是变量的系数，函数就是表达两个变量关系的等式。

  * [线性方程式][4]，例如: [latex]y = b + w\_1x\_1 + w\_2x\_2[/latex]

如上面的线性方程的例子，这是个二元一次方程，包含两个自变量x，一个因变量y，表达了这三个变量之间的关系。

  * [对数][5]和对数方程式，例如: [latex]y = ln(1+ e^z)[/latex]

如果 [latex]b^y = x[/latex] 那么 [latex]log\_b(x) = y[/latex]，是不是很好理解。[latex]log\_e(x) = ln(x)[/latex]， ln是一种简写形式。

  * [S 型函数，][6]又叫sigmod函数，在神经网络中作为激活函数。

Sigmoid函数又分为Log-Sigmoid函数和Tan-Sigmoid函数。

Log-Sigmoid函数: [latex]\sigma(z) = \frac{1}{1+e^{-z}}[/latex]，图形如下：

<img loading="lazy" class="alignnone wp-image-2846 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf6bd51e6f81.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf6bd51e6f81.png?x-oss-process=image/format,webp" alt="" width="377" height="251" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf6bd51e6f81.png?x-oss-process=image/format,webp 600w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/11/img_5bf6bd51e6f81.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_200/format,webp 300w" sizes="(max-width: 377px) 100vw, 377px" /> 

Tan-Sigmoid函数: [latex]\sigma(z) = \frac{2}{1+e^{-2z}} &#8211; 1[/latex]

## 线性代数

  * [张量和张量阶数][7]

现代的数学观点定义：**张量是多重线性函数**，<img loading="lazy" width="129" height="23" class="alignnone size-full wp-image-3197 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0928a952417.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0928a952417.png?x-oss-process=image/format,webp" alt="" />

输入r个向量，输出1个数，r称作张量的阶数。多重线性是指张量对于每个参数都是线性的,对于单个参数就是：

<p id="pftveyI">
  <img loading="lazy" width="208" height="23" class="alignnone size-full wp-image-3198 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0928c47027b.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0928c47027b.png?x-oss-process=image/format,webp" alt="" />
</p>

,其中u,v 是任意向量，c 是任意数。

&nbsp;

TensorFlow用张量这种数据结构来表示所有的数据.你可以把一个张量想象成一个n维的数组或列表.一个张量有一个静态类型和动态类型的维数.张量可以在图中的节点之间流通.在TensorFlow系统中，张量的维数来被描述为阶.但是张量的阶和矩阵的阶并不是同一个概念.张量的阶（有时是关于如顺序或度数或者是n维）是张量维数的一个数量描述.比如，下面的张量（使用Python中list定义的）就是2阶.

<pre class="EnlighterJSRAW" data-enlighter-language="null">t = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</pre>

你可以认为一个二阶张量就是我们平常所说的矩阵，一阶张量可以认为是一个向量.对于一个二阶张量你可以用语句t[i, j]来访问其中的任何元素.而对于三阶张量你可以用&#8217;t[i, j, k]&#8217;来访问其中的任何元素.

张量是所有深度学习框架中最核心的组件，因为后续的所有运算和优化算法都是基于张量进行的。几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

[tb colalign=&#8221;left&#8221;]  
阶,数学实例,Python例子  
0,纯量 (只有大小), s = 483  
1,向量(大小和方向), v = [1.1\, 2.2\, 3.3]  
2,矩阵(数据表), m = [[1\, 2\, 3]\, [4\, 5\, 6]\, [7\, 8\, 9]]  
3,3阶张量 (数据立体),t = [[[2]\, [4]\, [6]]\, [[8]\, [10]\, [12]]\, [[14]\, [16]\, [18]]]  
n,n阶 (自己想想看), &#8230;.  
[/tb]

举例来说，我们可以将任意一张RGB彩色图片表示成一个三阶张量（三个维度分别是图片的高度、宽度和色彩数据）。如下图所示是一张普通的水果图片，按照RGB三原色表示，其可以拆分为三张红色、绿色和蓝色的灰度图片，如果将这种表示方法用张量的形式写出来，就是图中最下方的那张表格。

<p id="XVFivzP">
  <img loading="lazy" class="alignnone wp-image-3200 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b5cbc508.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b5cbc508.png?x-oss-process=image/format,webp" alt="" width="293" height="274" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b5cbc508.png?x-oss-process=image/format,webp 635w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b5cbc508.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_281/format,webp 300w" sizes="(max-width: 293px) 100vw, 293px" />
</p>

<p id="igaBguN">
  <img loading="lazy" class="alignnone wp-image-3201 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b6ec10ce.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b6ec10ce.png?x-oss-process=image/format,webp" alt="" width="519" height="260" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b6ec10ce.png?x-oss-process=image/format,webp 675w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c092b6ec10ce.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_150/format,webp 300w" sizes="(max-width: 519px) 100vw, 519px" />
</p>

图中只显示了前5行、320列的数据，每个方格代表一个像素点，其中的数据[1.0, 1.0, 1.0]即为颜色。假设用[1.0, 0, 0]表示红色，[0, 1.0, 0]表示绿色，[0, 0, 1.0]表示蓝色，那么如图所示，前面5行的数据则全是白色。

将这一定义进行扩展，我们也可以用四阶张量表示一个包含多张图片的数据集，其中的四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

  * [矩阵乘法][8]

<p id="yhESDtA">
  <img loading="lazy" width="346" height="221" class="alignnone size-full wp-image-3204 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09302144e8a.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09302144e8a.png?x-oss-process=image/format,webp" alt="" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09302144e8a.png?x-oss-process=image/format,webp 346w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09302144e8a.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_192/format,webp 300w" sizes="(max-width: 346px) 100vw, 346px" />
</p>

<img loading="lazy" width="293" height="172" class="alignnone size-full wp-image-3205 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0930374dacb.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0930374dacb.png?x-oss-process=image/format,webp" alt="" /> 

第一个图给出来常用的矩阵乘积因子对数据做转换。第二个图从行向量和列向量理解矩阵乘法。

## 三角学

  * [Tanh][9]（作为[激活函数][10]进行讲解，无需提前掌握相关知识）

<span class="IF"><a href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a></span> 是双曲正切函数，是三角学中普遍使用的 <span class="IF"><a href="https://reference.wolfram.com/language/ref/Tan.html">Tan</a></span> 圆函数的双曲类比. <span class="IF"><a href="https://reference.wolfram.com/language/ref/Tanh.html">Tanh</a><span class="openbracket">[</span><span class="TR"><span class="special-character Alpha">α</span></span><span class="closebracket">]</span></span> 定义为对应的双曲正弦和双曲余弦函数的比值：

<p id="IZBGWip">
  <img loading="lazy" width="182" height="46" class="alignnone size-full wp-image-3207 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09313692e2d.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09313692e2d.png?x-oss-process=image/format,webp" alt="" />
</p>

<p id="NZvWLeZ">
  <img loading="lazy" width="178" height="52" class="alignnone size-full wp-image-3208 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09314452160.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09314452160.png?x-oss-process=image/format,webp" alt="" />
</p>

其中 e <span class="IM">是自然对数 <span class="IF"><a href="https://reference.wolfram.com/language/ref/Log.html">Log</a></span> 的底数。常用的激活函数图形如下：</span>

<p id="VEveKKy">
  <img loading="lazy" class="alignnone wp-image-3209 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09329dc0768.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09329dc0768.png?x-oss-process=image/format,webp" alt="" width="550" height="339" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09329dc0768.png?x-oss-process=image/format,webp 740w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09329dc0768.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_185/format,webp 300w" sizes="(max-width: 550px) 100vw, 550px" />
</p>

上面的图形不是很清楚，我们可以自己用python绘制一下：

<pre class="EnlighterJSRAW" data-enlighter-language="python">import matplotlib.pyplot as plt
import numpy as np
import math

def sigmod(x):
    return 1.0/(1.0+np.exp(-x))

def tanh(x):
    y = np.tanh(x)
    return y

def relu(x):
    y = x.copy()
    y[y&lt;0]=0
    return y

x = np.arange(-50.0,50.0,0.1)
y_relu = relu(x)
y_sigmod = sigmod(x)
y_tanh = tanh(x)

plt.plot(x,y_relu,c='r',label="Relu",linestyle='--')
plt.plot(x,y_sigmod,c='g',label="Sigmod",linestyle='-.')
plt.plot(x,y_tanh,c='b',label="Tanh")
plt.ylim([-1,4])
plt.xlim([-4,4])
plt.legend(loc=2)
plt.savefig('sig_tan_relu.png')
plt.show()</pre>

<p id="dTEfjrA">
  <img loading="lazy" class="alignnone wp-image-3210 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0936e0ae542.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0936e0ae542.png?x-oss-process=image/format,webp" alt="" width="453" height="340" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0936e0ae542.png?x-oss-process=image/format,webp 640w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0936e0ae542.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_225/format,webp 300w" sizes="(max-width: 453px) 100vw, 453px" />
</p>

## 统计信息

  * [均值、中间值、离群值][11]和[标准偏差][12]

<p id="LPGJeUm">
  <img loading="lazy" class="alignnone wp-image-3211 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/format,webp" alt="" width="655" height="382" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/format,webp 954w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_175/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_448/format,webp 768w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093773926ff.png?x-oss-process=image/quality,q_50/resize,m_fill,w_800,h_466/format,webp 800w" sizes="(max-width: 655px) 100vw, 655px" />
</p>

<p id="QZzqioi">
  <img loading="lazy" class="alignnone wp-image-3212 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/format,webp" alt="" width="439" height="46" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/format,webp 1030w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_31/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_81/format,webp 768w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0937917d39b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_800,h_84/format,webp 800w" sizes="(max-width: 439px) 100vw, 439px" />
</p>

  * 能够读懂[直方图][13]

## 微积分（可选，适合高级主题）

  * [导数][14]概念（您不必真正计算导数）

<p id="LNYNXCU">
  <img loading="lazy" class="alignnone wp-image-3213 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093821e9dca.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093821e9dca.png?x-oss-process=image/format,webp" alt="" width="446" height="345" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093821e9dca.png?x-oss-process=image/format,webp 682w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093821e9dca.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_232/format,webp 300w" sizes="(max-width: 446px) 100vw, 446px" />
</p>

  * [梯度][15]或斜率
  * [偏导数][16]（与梯度紧密相关）

<p id="osTTplo">
  <img loading="lazy" class="alignnone wp-image-3214 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093879313eb.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093879313eb.png?x-oss-process=image/format,webp" alt="" width="488" height="366" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093879313eb.png?x-oss-process=image/format,webp 755w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c093879313eb.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_225/format,webp 300w" sizes="(max-width: 488px) 100vw, 488px" />
</p>

  * [链式法则][17]（带您全面了解用于训练神经网络的[反向传播算法][18]）

<p id="jjxVnkM">
  <img loading="lazy" class="alignnone wp-image-3215 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09394ae25bb.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09394ae25bb.png?x-oss-process=image/format,webp" alt="" width="334" height="258" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09394ae25bb.png?x-oss-process=image/format,webp 567w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09394ae25bb.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_232/format,webp 300w" sizes="(max-width: 334px) 100vw, 334px" />
</p>

<p id="qyNvpeV">
  <img loading="lazy" width="500" height="260" class="alignnone size-full wp-image-3216 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09395eef74e.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09395eef74e.png?x-oss-process=image/format,webp" alt="" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09395eef74e.png?x-oss-process=image/format,webp 500w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c09395eef74e.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_156/format,webp 300w" sizes="(max-width: 500px) 100vw, 500px" />
</p>

## 参考资料

  * https://www.zhihu.com/question/20695804/answer/64920043
  * https://blog.csdn.net/pandamax/article/details/63684633
  * https://www.jianshu.com/p/b2c43c7d1b09

 [1]: https://www.khanacademy.org/math/algebra/introduction-to-algebra/alg1-intro-to-variables/v/what-is-a-variable
 [2]: https://www.khanacademy.org/math/cc-sixth-grade-math/cc-6th-equivalent-exp/cc-6th-parts-of-expressions/v/expression-terms-factors-and-coefficients
 [3]: https://www.khanacademy.org/math/algebra/algebra-functions
 [4]: https://wikipedia.org/wiki/Linear_equation
 [5]: https://wikipedia.org/wiki/Logarithm
 [6]: https://wikipedia.org/wiki/Sigmoid_function
 [7]: https://www.tensorflow.org/programmers_guide/tensors?hl=zh-cn
 [8]: https://wikipedia.org/wiki/Matrix_multiplication
 [9]: https://reference.wolfram.com/language/ref/Tanh.html
 [10]: https://developers.google.com/machine-learning/glossary?hl=zh-cn#activation_function
 [11]: https://www.khanacademy.org/math/probability/data-distributions-a1/summarizing-center-distributions/v/mean-median-and-mode
 [12]: https://wikipedia.org/wiki/Standard_deviation
 [13]: https://wikipedia.org/wiki/Histogram
 [14]: https://wikipedia.org/wiki/Derivative
 [15]: https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient
 [16]: https://wikipedia.org/wiki/Partial_derivative
 [17]: https://wikipedia.org/wiki/Chain_rule
 [18]: https://developers.google.com/machine-learning/crash-course/backprop-scroll/?hl=zh-cn