---
title: 模型训练与损失和泛化能力
weight: 5
---
机器学习是一类算法的总称，这些算法企图从大量历史数据中挖掘出其中隐含的规律，并用于预测或者分类，更具体的说，机器学习可以看作是寻找一个函数，输入是样本数据，输出是期望的结果。

# 训练与损失

简单来说，**训练模型**表示通过有标签样本来学习（确定）所有权重和偏差的取值。

## 模型训练通用步骤

通常学习一个好的函数，分为以下三步：  
1、选择一个合适的模型，这通常需要依据实际问题而定，针对不同的问题和任务需要选取恰当的模型，模型就是一组函数的集合。  
2、判断一个函数的好坏，这需要确定一个衡量标准，也就是我们通常说的损失函数（Loss Function），损失函数的确定也需要依据具体问题而定，如回归问题一般采用欧式距离，分类问题一般采用交叉熵代价函数。  
3、找出“最好”的函数，如何从众多函数中最快的找出“最好”的那一个，这一步是最大的难点，做到又快又准往往不是一件容易的事情。常用的方法有梯度下降算法，最小二乘法等和其他一些技巧（tricks）。学习得到“最好”的函数后，需要在新样本上进行测试，只有在新样本上表现很好，才算是一个“好”的函数。

<p id="SyxRGKl">
  <img loading="lazy" class="alignnone wp-image-3187 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/format,webp" alt="" width="648" height="370" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/format,webp 908w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_171/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_439/format,webp 768w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0801965083b.png?x-oss-process=image/quality,q_50/resize,m_fill,w_800,h_457/format,webp 800w" sizes="(max-width: 648px) 100vw, 648px" />
</p>

在监督式学习中，机器学习算法通过以检查多个样本并尝试找出可最大限度地减少**损失（cost）**的模型，这一过程称为**经验风险最小化**。

**损失**是对算法预测数据的惩罚。也就是说，**损失**是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，下图左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。

<img loading="lazy" class="alignnone wp-image-3278 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/format,webp" alt="" width="661" height="220" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/format,webp 1308w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_100/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_256/format,webp 768w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c0d1230ec87c.png?x-oss-process=image/quality,q_50/resize,m_fill,w_800,h_267/format,webp 800w" sizes="(max-width: 661px) 100vw, 661px" />

其中红色箭头表示损失，蓝色线是算法模型预测的值，黄色圈是实际值。

左侧曲线图中的红色箭头比右侧曲线图中的对应红色箭头长得多。显然，相较于左侧曲线图中的蓝线，右侧曲线图中的蓝线代表的是预测效果更好的模型。您可能想知道自己能否创建一个数学函数（损失函数），以有意义的方式汇总各个损失。

## 损失函数

### 平方损失 {.hide-from-toc}

<p class="hide-from-toc">
  线性回归模型最常使用的是一种称为<strong>平方损失</strong>（又称为 <strong>L<sub>2</sub> 损失</strong>）的损失函数。单个样本的平方损失如下：
</p>

<p style="text-align: center;">
  [latex](y&#8217; &#8211; y)^2[/latex]
</p>

**均方误差** (**MSE**) 指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：

<div style="text-align: center;">
  [latex] MSE = \frac{1}{N} \sum_{(x,y)\in D} (y &#8211; prediction(x))^2 [/latex]
</div>

<div>
  <p>
    其中：(x,y) 指的是样本，其中x 指的是模型进行预测时使用的特征集（例如，温度）。y 指的是样本的标签（例如，知了每分钟的鸣叫次数）。
  </p>
  
  <ul>
    <li>
      prediction(x) 指的是权重和偏差与特征集 x 结合的函数。
    </li>
    <li>
      D 指的是包含多个有标签样本（即 (x,y)）的数据集。N 指的是 D 中的样本数量。
    </li>
  </ul>
  
  <p>
    虽然 MSE 常用于机器学习，但它既不是唯一实用的损失函数，也不是适用于所有情形的最佳损失函数。
  </p>
</div>

## 降低损失

为了让我们的模型更加准确，就需要减少损失。下图是机器学习中的一般方法：

&nbsp;

<p id="EAUPbcB">
  <img loading="lazy" class="alignnone wp-image-3316 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/format,webp" alt="" width="727" height="327" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/format,webp 1402w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_135/format,webp 300w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/quality,q_50/resize,m_fill,w_768,h_345/format,webp 768w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128a5545980.png?x-oss-process=image/quality,q_50/resize,m_fill,w_800,h_359/format,webp 800w" sizes="(max-width: 727px) 100vw, 727px" />
</p>

通过不断的试错，不断地更新参数，最终可以将损失收敛在要求范围内，那么这个模型就训练完成了。

回想一下我们上节课介绍的温度和知了叫声的模型：

<p style="text-align: center;">
  [latex]y&#8217; = b + w_1x_1[/latex]
</p>

在第一步计算损失的时候我们需要一个初始值。对于线性回归问题，理论证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：w1=0，b=0；将改初始值带入上面的模型计算，假设第一个样本x1=10：

<pre class="EnlighterJSRAW" data-enlighter-language="null">y' = 0 + 0(10)
   = 0</pre>

这里y’表示对第一个数据x1=10的预测结果。而实际值为y，假设我们选取平方损失 作为损失函数，那么：

<p style="text-align: center;">
  [latex]loss = (y&#8217; &#8211; y)^2[/latex]
</p>

那么问题来了，假设计算出来的损失值很大，我们怎么来更新w和b的值，来使得损失减少呢？ 常用的方法是梯度下降法。

### 梯度下降法

对于回归问题，假设我们有时间和计算资源来计算 w1 的所有可能值的损失，所产生的损失与 w1 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：

<p id="OrppIHq">
  <img loading="lazy" class="alignnone wp-image-3317 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128d9da893c.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128d9da893c.png?x-oss-process=image/format,webp" alt="" width="415" height="268" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128d9da893c.png?x-oss-process=image/format,webp 582w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c128d9da893c.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_194/format,webp 300w" sizes="(max-width: 415px) 100vw, 415px" />
</p>

&nbsp;

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点，然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

<p id="IDlOjdR">
  <img loading="lazy" class="alignnone wp-image-3378 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1cf7874cf3d.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1cf7874cf3d.png?x-oss-process=image/format,webp" alt="" width="378" height="244" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1cf7874cf3d.png?x-oss-process=image/format,webp 582w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1cf7874cf3d.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_194/format,webp 300w" sizes="(max-width: 378px) 100vw, 378px" />
</p>

请注意，梯度是一个矢量，因此具有以下两个特征：

* 方向
* 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加。然后，梯度下降法会重复此过程，逐渐接近最低点。

梯度下降法算法用梯度乘以一个称为**学习速率**（有时也称为**步长**）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。学习速率这个参数我们也称为**超参数，超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果您选择的学习速率过小，就会花费太长的学习时间。相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：

<img loading="lazy" class="alignnone wp-image-3383 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/format,webp" alt="" width="507" height="211" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/format,webp 718w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_125/format,webp 300w" sizes="(max-width: 507px) 100vw, 507px" />

&nbsp;

<p id="bcuXGzN">
  <img loading="lazy" class="alignnone wp-image-3383 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/format,webp" alt="" width="533" height="222" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/format,webp 718w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2018/12/img_5c1d04be28a8f.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_125/format,webp 300w" sizes="(max-width: 533px) 100vw, 533px" />
</p>

我们可以在下面这个模型中做实验。这个是TensorFlow官方提供的栗子，我们将这个深度学习的神经网络模型的隐含层设置成0个，其实就是一个线性模型。我们通过调节学习速率，可以看到模型最终的收敛速度。

## 随机梯度下降法

上面我们介绍了权重和误差的关系，以及如何修改权重是的误差降低。假设我们采用的是均方误差，那么对于样本数据集，误差计算规则是：

<p style="text-align: center;">
  [latex] MSE = \frac{1}{N} \sum_{(x,y)\in D} (y &#8211; prediction(x))^2 [/latex]
</p>

在上一节介绍的梯度下降法中我们需要计算使得误差减少、权重变化的梯度，根据数据知识，这个梯度就是均方误差对权重的倒数，

<p style="text-align: center;">
  [latex]\begin{align}<br /> \nabla{E(\mathrm{w})}&=\frac{\partial}{\partial\mathrm{w}}E(\mathrm{w})\\<br /> &=\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> \end{align}[/latex]
</p>

可接下来怎么办呢？我们知道和的导数等于导数的和，所以我们可以先把求和符号<span id="MathJax-Element-117-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>里面的导数求出来，然后再把它们加在一起就行了，也就是

<p style="text-align: center;">
  [latex]\begin{align}<br /> &\frac{\partial}{\partial\mathrm{w}}\frac{1}{2}\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> =&\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> \end{align}[/latex]
</p>

现在我们可以不管高大上的求和函数了，先专心把里面的导数求出来

<p style="text-align: center;">
  [latex]\begin{align}<br /> &\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> =&\frac{\partial}{\partial\mathrm{w}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\<br /> \end{align}[/latex]
</p>

根据函数求导传递法则：

<p style="text-align: center;">
  [latex]\begin{align}<br /> &\frac{\partial{E(\mathrm{w})}}{\partial\mathrm{w}}=\frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}\frac{\partial{\bar{y}}}{\partial\mathrm{w}}\end{align}[/latex]
</p>

我们分别计算上式等号右边的两个偏导数

<p style="text-align: center;">
  [latex]\begin{align}<br /> \frac{\partial{E(\mathrm{w})}}{\partial\bar{y}}=<br /> &\frac{\partial}{\partial\bar{y}}(y^{(i)2}-2\bar{y}^{(i)}y^{(i)}+\bar{y}^{(i)2})\\<br /> =&-2y^{(i)}+2\bar{y}^{(i)}\\\\<br /> \frac{\partial{\bar{y}}}{\partial\mathrm{w}}=<br /> &\frac{\partial}{\partial\mathrm{w}}\mathrm{w}^T\mathrm{x}\\<br /> =&\mathrm{x}<br /> \end{align}[/latex]
</p>

代入，我们求得<span id="MathJax-Element-126-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>里面的偏导数是

<p style="text-align: center;">
  [latex]\begin{align}<br /> &\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> =&2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}<br /> \end{align}[/latex]
</p>

最后代入<span id="MathJax-Element-128-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>，求得

<p style="text-align: center;">
  [latex]\begin{align}<br /> \nabla{E(\mathrm{w})}&=\frac{1}{2}\sum_{i=1}^{n}\frac{\partial}{\partial\mathrm{w}}(y^{(i)}-\bar{y}^{(i)})^2\\<br /> &=\frac{1}{2}\sum_{i=1}^{n}2(-y^{(i)}+\bar{y}^{(i)})\mathrm{x}\\<br /> &=-\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}<br /> \end{align}[/latex]
</p>

那么根据梯度下降法，权重w的更新规则如下

<p style="text-align: center;">
  [latex]\mathrm{w}_{new}=\mathrm{w}_{old}+\eta\sum_{i=1}^{n}(y^{(i)}-\bar{y}^{(i)})\mathrm{x}^{(i)}\qquad\qquad[/latex]
</p>

如果我们根据上面的公式来训练模型，那么我们每次更新<span id="MathJax-Element-130-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做**批梯度下降(Batch Gradient Descent)**。如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是 **随机梯度下降法 (SGD)** 算法。在SGD算法中，每次更新<span id="MathJax-Element-131-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对<span id="MathJax-Element-132-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>更新数百万次，效率大大提升。

由于样本的噪音和随机性，每次更新<span id="MathJax-Element-133-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>并不一定按照减少<span id="MathJax-Element-134-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>的方向。然而，虽然存在一定随机性，实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。大量的更新总体上沿着减少<span id="MathJax-Element-135-Frame" class="MathJax_SVG" role="textbox" aria-readonly="true"></span>的方向前进的，因此最后也能收敛到最小值附近。

**小批量随机梯度下降法**（**小批量 SGD**）是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。

&nbsp;

# 泛化能力

机器学习的目标是使学到的函数很好地适用于“新样本”，而不仅仅是在训练样本上表现很好。学到的函数适用于新样本的能力，称为泛化（Generalization）能力。

## 规则化（Regularize）

机器学习中，我们一直期望学习到一个泛化能力（generalization）强的函数，只有泛化能力强的模型才能很好地适用于整个样本空间，才能在新的样本点上表现良好。但是训练集通常只是整个样本空间很小的一部分，在训练机器学习模型时，稍有不注意，就可能将训练集中样本的特性当作了全体样本的共性，以偏概全，而造成过拟合（overfitting）问题，如何避免过拟合，是训练机器学习模型时最亟待解决的绊脚石。  
从问题的根源出发，解决过拟合无非两种途径：

  1. 使训练集能够尽可能全面的描述整个样本空间。因此又存在两个解决方向。  
    ①减少特征维数，特征维数减少了，样本空间的大小也随之减少了，现有数据集对样本空间的描述也就提高了。  
    ②增加训练样本数量，试图直接提升对样本空间的描述能力。
  2. 加入规则化项。  
    第一种方法的人力成本通常很大，所以在实际中，我们通常采用第二种方法提升模型的泛化能力。

注：规则化在有些文档中也称作正则化，在本文中都采用规则化描述。

首先回顾一下，在寻找模型最优参数时，我们通常对损失函数采用梯度下降（gradient descent）算法

<p style="text-align: center;">
  [latex]w^*,b^*=arg\min_{w,b}\sum_{i=1}^m(y^{(i)}-(w^Tx^{(i)}+b))^2\\ \frac{\partial L}{\partial w}=\sum_{i=1}^m2(y^{(i)}-(w^Tx^{(i)}+b))(-x^{(i)})\\ \frac{\partial L}{\partial b}=\sum_{i=1}^m2(y^{(i)}-(w^Tx^{(i)}+b))[-1](/latex)
</p>

通过上述公式，我们将一步步走到损失函数的最低点（不考虑局部最小值和鞍点情况），这时的ww和bb就是我们要找的最优参数。  
对于回归问题，我们还可以直接采用最小二乘法求得解析解。

<p style="text-align: center;">
  [latex]L=\frac{1}{2}(y-X\hat{w})^T(y-X\hat{w})\\\hat{w}=(X^TX)^{-1}X^Ty[/latex]
</p>

可以看到，当前我们的损失函数只考虑最小化训练误差，希望找到的最优函数能够尽可能的拟合训练数据。但是正如我们所了解的，训练集不能代表整个样本空间，所以训练误差也不能代表在测试误差，训练误差只是经验风险，我们不能过分依赖这个值。当我们的函数对训练集拟合特别好，训练误差特别小时，我们也就走进了一个极端——过拟合。  
为了解决这个问题，研究人员提出了规则化（regularize）方法。通过给模型参数附加一些规则，也就是约束，防止模型过分拟合训练数据。规则化通过在原有损失函数的基础上加入规则化项实现。  
此时，最优化的目标函数如下：

<p style="text-align: center;">
  [latex]w^*=arg\min_w\sum_iL(y^{(i)},f(x^{(i)};w))+\lambda\Omega[w](/latex)
</p>

其中，第一项对应于模型在训练集上的误差，第二项对应于规则化项。为了使得该目标函数最小，我们既需要训练误差最小，也需要规则化项最小，因此需要在二者之间做到权衡。  
那应该选择怎样的表达式作为规则化项呢？以下引用李航博士《统计学习方法》中的一些描述：

> 规则化是结构风险最小化策略的实现，是在经验风险最小化上加一个规则化项（regularizer）或罚项（penalty term）。规则化项一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。  
> 规则化符合奥卡姆剃刀（Occam’s razor）原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。

所以通常我们采用L1-范数和L2-范数作为规则化项。

## L1-范数

向量的L1-范数是向量的元素绝对值之和，即

<p style="text-align: center;">
  [latex]||x||_1=\sum_i|x_i|[/latex]
</p>

当采用L1-范数作为规则化项对参数进行约束时，我们的优化问题可以写成以下形式：

<p style="text-align: center;">
  [latex]\min_w\frac{1}{2}(y-Xw)^2\\ s.t.\quad||w||_1\le C[/latex]
</p>

采用拉格朗日乘子法可以将约束条件合并到最优化函数中，即

<p style="text-align: center;">
  [latex]\min_w\frac{1}{2}(y-Xw)^2+\lambda||w||_1[/latex]
</p>

其中λλ是于CC一一对应的常数，用来权衡误差项和规则化项，λλ越大，约束越强。二维情况下分别将损失函数的等高线图和L1-范数规则化约束画在同一个坐标轴下，

<p id="cGWoToD">
  <img loading="lazy" width="304" height="299" class="alignnone size-full wp-image-4201 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacaa7b9ba04.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacaa7b9ba04.png?x-oss-process=image/format,webp" alt="" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacaa7b9ba04.png?x-oss-process=image/format,webp 304w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacaa7b9ba04.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_295/format,webp 300w" sizes="(max-width: 304px) 100vw, 304px" />
</p>

L1-范数约束对应于平面上一个正方形norm ball。等高线与norm ball首次相交的地方就是最优解。可以看到，L1-ball在和每个坐标轴相交的地方都有“角”出现，大部分时候等高线都会与norm ball在角的地方相交。这样部分参数值被置为0，相当于该参数对应的特征将不再发挥作用，实现了特征选择，增加了模型的可解释性。

关于L1-范数规则化，可以解释如下：训练出来的参数代表权重，反应了特征的重要程度，比如y=20&#215;1+5&#215;2+3y=20&#215;1+5&#215;2+3中，特征x1明显比x2更加重要，因为x1的变动相较于x2的变动，会给y带来更大的变化。在人工选取的特征中，往往会存在一些冗余特征或者无用特征，L1-范数规则化将这些特征的权重置为0，实现了特征选择，同时也简化了模型。  
L1-范数在x=0处存在拐点，所以不能直接求得解析解，需要用次梯度方法处理不可导的凸函数。

## L2-范数

除了L1-范数，还有一种广泛使用的规则化范数：L2-范数。向量的L2-范数是向量的模长，即

<p style="text-align: center;">
  [latex]||x||_2=\sqrt{\sum_ix_i^2}[/latex]
</p>

当采用L2-范数作为规则化项对参数进行约束时，我们的优化问题可以写成以下形式：

<p style="text-align: center;">
  [latex]\min_w\frac{1}{2}(y-Xw)^2\\ s.t.\quad||w||_2\le C[/latex]
</p>

同样可以将约束条件合并到最优化函数中，得到如下函数

<p style="text-align: center;">
  [latex]\min_w\frac{1}{2}(y-Xw)^2+\lambda||w||_2[/latex]
</p>

也将损失函数的等高线图和L2-范数规则化约束画在同一个坐标轴下，

<p id="sBKvSsA">
  <img loading="lazy" width="306" height="298" class="alignnone size-full wp-image-4202 shadow" src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacab29ba61a.png?x-oss-process=image/quality,q_10/resize,m_lfit,w_200" data-src="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacab29ba61a.png?x-oss-process=image/format,webp" alt="" srcset="https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacab29ba61a.png?x-oss-process=image/format,webp 306w, https://haomou.oss-cn-beijing.aliyuncs.com/upload/2019/04/img_5cacab29ba61a.png?x-oss-process=image/quality,q_50/resize,m_fill,w_300,h_292/format,webp 300w" sizes="(max-width: 306px) 100vw, 306px" />
</p>

L2-范数约束对应于平面上一个圆形norm ball。等高线与norm ball首次相交的地方就是最优解。与L1-范数不同，L2-范数使得每一个w都很小，都接近于0，但不会等于0，L2-范数规则化仍然试图使用每一维特征。对于L2-范数规则化可以解释如下：L2-范数规则化项将参数限制在一个较小的范围，参数越小，曲面越光滑，因而不会出现在很小区间内，弯曲度很大的情况，当x一个较大的变化时，y也只会变化一点点，模型因此更加稳定，也就是更加generalization。  
加入L2-范数规则化项后，目标函数扩展为如下形式：

<p style="text-align: center;">
  [latex]w^*,b^*=arg\min_{w,b}\sum_{i=1}^m(y^{(i)}-(w^Tx^{(i)}+b))^2+\lambda\sum_{j=1}^nw_j^2\\ \frac{\partial L}{\partial w}=\sum_{i=1}^m2[(y^{(i)}-(w^Tx^{(i)}+b))(-x^{(i)})+\lambda w]\\ \frac{\partial L}{\partial b}=\sum_{i=1}^m2[(y^{(i)}-(w^Tx^{(i)}+b))(-1)\lambda w][/latex]
</p>

同样，如果采用最小二乘法，正规方程的形式需要相应修改，并且对于样本数目少于特征维数的情况时，矩阵(XTX)(XTX)将不满秩，(XTX)(XTX)也就不可逆，确切地说，此时方程组是不定方程组，将会有无穷多解，已有的数据不足以确定一个解，数学上常加入约束项以使得唯一解成为可能，加入L2-范数规则化项正好对应了这种方法，此时解析解如下：

<p style="text-align: center;">
  [latex]w^*=(X^TX+\lambda I)^{-1}X^T[/latex]
</p>

关于L1-范数和L2-范数规则化的解释是个人的总结之词，可能存在不准确，希望大家不惜赐教！

# 参考文献

* 李航《统计学习方法》
* [机器学习中的范数规则化之（一）L0、L1与L2范数][1]
* [Sparsity and Some Basics of L1 Regularization][2]
* https://blog.csdn.net/hohaizx/article/details/80973738
* https://www.zybuluo.com/hanbingtao/note/433855

<audio style="display: none;" controls="controls"></audio>

<audio style="display: none;" controls="controls"></audio>

 [1]: https://blog.csdn.net/zouxy09/article/details/24971995
 [2]: http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/
